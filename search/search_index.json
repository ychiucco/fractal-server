{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fractal Analytics Framework","text":"<p>Fractal is a framework to process high content imaging data at scale and prepare it for interactive visualization.</p> <p>This is the server component of the fractal analytics platform (find more information about Fractal in general and the other repositories at the Fractal home page). The source code is available on the fractal-server GitHub repository.</p>"},{"location":"#licence-and-copyright","title":"Licence and Copyright","text":"<p>Fractal was conceived in the Liberali Lab at the Friedrich Miescher Institute for Biomedical Research and in the Pelkmans Lab at the University of Zurich by @jluethi and @gusqgm. The Fractal project is now developed at the BioVisionCenter at the University of Zurich and the project lead is with @jluethi. The core development is done under contract by eXact lab S.r.l..</p> <p>Unless otherwise stated in each individual module, all Fractal components are released according to a BSD 3-Clause License, and Copyright is with Friedrich Miescher Institute for Biomedical Research and University of Zurich.</p> <p>The SLURM compatibility layer is based on clusterfutures, by @sampsyo and collaborators, and it is released under the terms of the MIT license.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>Note: Numbers like (#123) point to closed Pull Requests on the fractal-server repository.</p>"},{"location":"changelog/#1410","title":"1.4.10","text":"<p>WARNING: Starting from this version, the dependencies for the <code>slurm</code> extra are required; commands like <code>pip install fractal-server[slurm,postgres]</code> must be replaced by <code>pip install fractal-server[postgres]</code>.</p> <ul> <li>Dependencies:<ul> <li>Make <code>clusterfutures</code> and <code>cloudpickle</code> required dependencies (#1255).</li> <li>Remove <code>slurm</code> extra from package (#1255).</li> </ul> </li> <li>API:<ul> <li>Handle invalid history file in <code>GET /project/{project_id}/dataset/{dataset_id}/status/</code> (#1259).</li> </ul> </li> <li>Runner:<ul> <li>Add custom <code>_jobs_finished</code> function to check the job status and to avoid squeue errors (#1266)</li> </ul> </li> </ul>"},{"location":"changelog/#149","title":"1.4.9","text":"<p>This release is a follow-up of 1.4.7 and 1.4.8, to mitigate the risk of job folders becoming very large.</p> <ul> <li>Runner:<ul> <li>Exclude <code>history</code> from <code>TaskParameters</code> object for parallel tasks, so that it does not end up in input pickle files (#1247).</li> </ul> </li> </ul>"},{"location":"changelog/#148","title":"1.4.8","text":"<p>This release is a follow-up of 1.4.7, to mitigate the risk of job folders becoming very large.</p> <ul> <li>Runner:<ul> <li>Exclude <code>metadata[\"image\"]</code> from <code>TaskParameters</code> object for parallel tasks, so that it does not end up in input pickle files (#1245).</li> <li>Exclude components list from <code>workflow.log</code> logs (#1245).</li> </ul> </li> <li>Database:<ul> <li>Remove spurious logging of <code>fractal_server.app.db</code> string (#1245).</li> </ul> </li> </ul>"},{"location":"changelog/#147","title":"1.4.7","text":"<p>This release provides a bugfix (PR 1239) and a workaround (PR 1238) for the SLURM runner, which became relevant for the use case of processing a large dataset (300 wells with 25 cycles each).</p> <ul> <li>Runner:<ul> <li>Do not include <code>metadata[\"image\"]</code> in JSON file with task arguments (#1238).</li> <li>Add <code>FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE</code> configuration variable, to define exceptions where tasks still require <code>metadata[\"image\"]</code> (#1238).</li> <li>Fix bug in globbing patterns, when copying files from user-side to server-side job folder in SLURM executor (#1239).</li> </ul> </li> <li>API:<ul> <li>Fix error message for rate limits in apply-workflow endpoint (#1231).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add more scenarios, as per issue #1184 (#1232).</li> </ul> </li> </ul>"},{"location":"changelog/#146","title":"1.4.6","text":"<ul> <li>API:<ul> <li>Add <code>GET /admin/job/{job_id}</code> (#1230).</li> <li>Handle <code>FileNotFound</code> in <code>GET /project/{project_id}/job/{job_id}/</code> (#1230).</li> </ul> </li> </ul>"},{"location":"changelog/#145","title":"1.4.5","text":"<ul> <li>Remove CORS middleware (#1228).</li> <li>Testing:<ul> <li>Fix <code>migrations.yml</code> GitHub action (#1225).</li> </ul> </li> </ul>"},{"location":"changelog/#144","title":"1.4.4","text":"<ul> <li>API:<ul> <li>Add rate limiting to <code>POST /{project_id}/workflow/{workflow_id}/apply/</code> (#1199).</li> <li>Allow users to read the logs of ongoing jobs with <code>GET /project/{project_id}/job/{job_id}/</code>, using <code>show_tmp_logs</code> query parameter (#1216).</li> <li>Add <code>log</code> query parameter in <code>GET {/api/v1/job/,/api/v1/{project.id}/job/,/admin/job/}</code>, to trim response body (#1218).</li> <li>Add <code>args_schema</code> query parameter in <code>GET /api/v1/task/</code> to trim response body (#1218).</li> <li>Add <code>history</code> query parameter in <code>GET {/api/v1/dataset/,/api/v1/project/{project.id}/dataset/}</code> to trim response body (#1219).</li> <li>Remove <code>task_list</code> from <code>job.workflow_dump</code> creation in <code>/api/v1/{project_id}/workflow/{workflow_id}/apply/</code>(#1219)</li> <li>Remove <code>task_list</code> from <code>WorkflowDump</code> Pydantic schema (#1219)</li> </ul> </li> <li>Dependencies:<ul> <li>Update fastapi to <code>^0.109.0</code> (#1222).</li> <li>Update gunicorn to <code>^21.2.0</code> (#1222).</li> <li>Update aiosqlite to <code>^0.19.0</code> (#1222).</li> <li>Update uvicorn to <code>^0.27.0</code> (#1222).</li> </ul> </li> </ul>"},{"location":"changelog/#143","title":"1.4.3","text":"<p>WARNING:</p> <p>This update requires running a fix-db script, via <code>fractalctl update-db-data</code>.</p> <ul> <li>API:<ul> <li>Improve validation of <code>UserCreate.slurm_accounts</code> (#1162).</li> <li>Add <code>timestamp_created</code> to <code>WorkflowRead</code>, <code>WorkflowDump</code>, <code>DatasetRead</code> and <code>DatasetDump</code> (#1152).</li> <li>Make all dumps in <code>ApplyWorkflowRead</code> non optional (#1175).</li> <li>Ensure that timestamps in <code>Read</code> schemas are timezone-aware, regardless of <code>DB_ENGINE</code> (#1186).</li> <li>Add timezone-aware timestamp query parameters to all <code>/admin</code> endpoints (#1186).</li> </ul> </li> <li>API (internal):<ul> <li>Change the class method <code>Workflow.insert_task</code> into the auxiliary function <code>_workflow_insert_task</code> (#1149).</li> </ul> </li> <li>Database:<ul> <li>Make <code>WorkflowTask.workflow_id</code> and <code>WorfklowTask.task_id</code> not nullable (#1137).</li> <li>Add <code>Workflow.timestamp_created</code> and <code>Dataset.timestamp_created</code> columns (#1152).</li> <li>Start a new <code>current.py</code> fix-db script (#1152, #1195).</li> <li>Add to <code>migrations.yml</code> a new script (<code>validate_db_data_with_read_schemas.py</code>) that validates test-DB data with Read schemas (#1187).</li> <li>Expose <code>fix-db</code> scripts via command-line option <code>fractalctl update-db-data</code> (#1197).</li> </ul> </li> <li>App (internal):<ul> <li>Check in <code>Settings</code> that <code>psycopg2</code>, <code>asyngpg</code> and <code>cfut</code>, if required, are installed (#1167).</li> <li>Split <code>DB.set_db</code> into sync/async methods (#1165).</li> <li>Rename <code>DB.get_db</code> into <code>DB.get_async_db</code> (#1183).</li> <li>Normalize names of task packages (#1188).</li> </ul> </li> <li>Testing:<ul> <li>Update <code>clean_db_fractal_1.4.1.sql</code> to <code>clean_db_fractal_1.4.2.sql</code>, and change <code>migrations.yml</code> target version (#1152).</li> <li>Reorganise the test directory into subdirectories, named according to the order in which we want the CI to execute them (#1166).</li> <li>Split the CI into two independent jobs, <code>Core</code> and <code>Runner</code>, to save time through parallelisation (#1204).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>python-dotenv</code> to version 0.21.0 (#1172).</li> </ul> </li> <li>Runner:<ul> <li>Remove <code>JobStatusType.RUNNING</code>, incorporating it into <code>JobStatusType.SUBMITTED</code> (#1179).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add <code>fractal_client.py</code> and <code>populate_script_v2.py</code> for creating different database status scenarios (#1178).</li> <li>Add a custom benchmark suite in <code>api_bench.py</code>.</li> <li>Remove locust.</li> </ul> </li> <li>Documentation:<ul> <li>Add the minimum set of environment variables required to set the database and start the server (#1198).</li> </ul> </li> </ul>"},{"location":"changelog/#142","title":"1.4.2","text":"<p>WARNINGs:</p> <ol> <li>This update requires running a fix-db script, available at https://raw.githubusercontent.com/fractal-analytics-platform/fractal-server/1.4.2/scripts/fix_db/current.py.</li> <li>Starting from this version, non-verified users have limited access to <code>/api/v1/</code> endpoints. Before the upgrade, all existing users must be manually set to verified.</li> </ol> <ul> <li>API:<ul> <li>Prevent access to <code>GET/PATCH</code> task endpoints for non-verified users (#1114).</li> <li>Prevent access to task-collection and workflow-apply endpoints for non-verified users (#1099).</li> <li>Make first-admin-user verified (#1110).</li> <li>Add the automatic setting of <code>ApplyWorkflow.end_timestamp</code> when patching <code>ApplyWorkflow.status</code> via <code>PATCH /admin/job/{job_id}</code> (#1121).</li> <li>Change <code>ProjectDump.timestamp_created</code> type from <code>datetime</code> to <code>str</code> (#1120).</li> <li>Change <code>_DatasetHistoryItem.workflowtask</code> type into <code>WorkflowTaskDump</code> (#1139).</li> <li>Change status code of stop-job endpoints to 202 (#1151).</li> </ul> </li> <li>API (internal):<ul> <li>Implement cascade operations explicitly, in <code>DELETE</code> endpoints for datasets, workflows and projects (#1130).</li> <li>Update <code>GET /project/{project_id}/workflow/{workflow_id}/job/</code> to avoid using <code>Workflow.job_list</code> (#1130).</li> <li>Remove obsolete sync-database dependency from apply-workflow endpoint (#1144).</li> </ul> </li> <li>Database:<ul> <li>Add <code>ApplyWorkflow.project_dump</code> column (#1070).</li> <li>Provide more meaningful names to fix-db scripts (#1107).</li> <li>Add <code>Project.timestamp_created</code> column, with timezone-aware default (#1102, #1131).</li> <li>Remove <code>Dataset.list_jobs_input</code> and <code>Dataset.list_jobs_output</code> relationships (#1130).</li> <li>Remove <code>Workflow.job_list</code> (#1130).</li> </ul> </li> <li>Runner:<ul> <li>In SLURM backend, use <code>slurm_account</code> (as received from apply-workflow endpoint) with top priority (#1145).</li> <li>Forbid setting of SLURM account from <code>WorkflowTask.meta</code> or as part of <code>worker_init</code> variable (#1145).</li> <li>Include more info in error message upon <code>sbatch</code> failure (#1142).</li> <li>Replace <code>sbatch</code> <code>--chdir</code> option with <code>-D</code>, to support also slurm versions before 17.11 (#1159).</li> </ul> </li> <li>Testing:<ul> <li>Extended systematic testing of database models (#1078).</li> <li>Review <code>MockCurrentUser</code> fixture, to handle different kinds of users (#1099).</li> <li>Remove <code>persist</code> from <code>MockCurrentUser</code> (#1098).</li> <li>Update <code>migrations.yml</code> GitHub Action to use up-to-date database and also test fix-db script (#1101).</li> <li>Add more schema-based validation to fix-db current script (#1107).</li> <li>Update <code>.dict()</code> to <code>.model_dump()</code> for <code>SQLModel</code> objects, to fix some <code>DeprecationWarnings</code>(##1133).</li> <li>Small improvement in schema coverage (#1125).</li> <li>Add unit test for <code>security</code> module (#1036).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>sqlmodel</code> to version 0.0.14 (#1124).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add automatic benchmark system for API's performances (#1123)</li> </ul> </li> <li>App (internal):<ul> <li>Move <code>_create_first_user</code> from <code>main</code> to <code>security</code> module, and allow it to create multiple regular users (#1036).</li> </ul> </li> </ul>"},{"location":"changelog/#141","title":"1.4.1","text":"<ul> <li>API:<ul> <li>Add <code>GET /admin/job/{job_id}/stop/</code> and <code>GET /admin/job/{job_id}/download/</code> endpoints (#1059).</li> <li>Use <code>DatasetDump</code> and <code>WorkflowDump</code> models for \"dump\" attributes of <code>ApplyWorkflowRead</code> (#1049, #1082).</li> <li>Add <code>slurm_accounts</code> to <code>User</code> schemas and add <code>slurm_account</code> to <code>ApplyWorkflow</code> schemas (#1067).</li> <li>Prevent providing a <code>package_version</code> for task collection from a <code>.whl</code> local package (#1069).</li> <li>Add <code>DatasetRead.project</code> and <code>WorkflowRead.project</code> attributes (#1082).</li> </ul> </li> <li>Database:<ul> <li>Make <code>ApplyWorkflow.workflow_dump</code> column non-nullable (#1049).</li> <li>Add <code>UserOAuth.slurm_accounts</code> and <code>ApplyWorkflow.slurm_account</code> columns (#1067).</li> <li>Add script for adding <code>ApplyWorkflow.user_email</code> (#1058).</li> <li>Add <code>Dataset.project</code> and <code>Workflow.project</code> relationships (#1082).</li> <li>Avoid using <code>Project</code> relationships <code>dataset_list</code> or <code>workflow_list</code> within some <code>GET</code> endpoints (#1082).</li> <li>Fully remove <code>Project</code> relationships <code>dataset_list</code>, <code>workflow_list</code> and <code>job_list</code> (#1091).</li> </ul> </li> <li>Testing:<ul> <li>Only use ubuntu-22.04 in GitHub actions (#1061).</li> <li>Improve unit testing of database models (#1082).</li> </ul> </li> <li>Dependencies:<ul> <li>Pin <code>bcrypt</code> to 4.0.1 to avoid warning in passlib (#1060).</li> </ul> </li> <li>Runner:<ul> <li>Set SLURM-job working directory to <code>job.working_dir_user</code> through <code>--chdir</code> option (#1064).</li> </ul> </li> </ul>"},{"location":"changelog/#140","title":"1.4.0","text":"<ul> <li>API:<ul> <li>Major endpoint changes:<ul> <li>Add trailing slash to all endpoints' paths (#1003).</li> <li>Add new admin-area endpoints restricted to superusers at <code>/admin</code> (#947, #1009, #1032).</li> <li>Add new <code>GET</code> endpoints <code>api/v1/job/</code> and <code>api/v1/project/{project_id}/workflow/{workflow_id}/job/</code> (#969, #1003).</li> <li>Add new <code>GET</code> endpoints <code>api/v1/dataset/</code> and <code>api/v1/workflow/</code> (#988, #1003).</li> <li>Add new <code>GET</code> endpoint <code>api/v1/project/{project_id}/dataset/</code> (#993).</li> <li>Add <code>PATCH /admin/job/{job_id}/</code> endpoint (#1030, #1053).</li> <li>Move <code>GET /auth/whoami/</code> to <code>GET /auth/current-user/</code> (#1013).</li> <li>Move <code>PATCH /auth/users/me/</code> to <code>PATCH /auth/current-user/</code> (#1013, #1035).</li> <li>Remove <code>DELETE /auth/users/{id}/</code> endpoint (#994).</li> <li>Remove <code>GET /auth/users/me/</code> (#1013).</li> <li>Remove <code>POST</code> <code>/auth/forgot-password/</code>, <code>/auth/reset-password/</code>, <code>/auth/request-verify-token/</code>, <code>/auth/verify/</code> (#1033).</li> <li>Move <code>GET /auth/userlist/</code> to <code>GET /auth/users/</code> (#1033).</li> </ul> </li> <li>New behaviors or responses of existing endpoints:<ul> <li>Change response of <code>/api/v1/project/{project_id}/job/{job_id}/stop/</code> endpoint to 204 no-content (#967).</li> <li>Remove <code>dataset_list</code> attribute from <code>ProjectRead</code>, which affects all <code>GET</code> endpoints that return some project (#993).</li> <li>Make it possible to delete a <code>Dataset</code>, <code>Workflow</code> or <code>Project</code>, even when it is in relationship to an <code>ApplyWorkflow</code> - provided that the <code>ApplyWorkflow</code> is not pending or running (#927, #973).</li> <li>Align <code>ApplyWorkflowRead</code> with new <code>ApplyWorkflow</code>, which has optional foreign keys <code>project_id</code>, <code>workflow_id</code>, <code>input_dataset_id</code>, and <code>output_dataset_id</code> (#984).</li> <li>Define types for <code>ApplyWorkflowRead</code> \"dump\" attributes (#990). WARNING: reverted with #999.</li> </ul> </li> <li>Internal changes:<ul> <li>Move all routes definitions into <code>fractal_server/app/routes</code> (#976).</li> <li>Fix construction of <code>ApplyWorkflow.workflow_dump</code>, within apply endpoint (#968).</li> <li>Fix construction of <code>ApplyWorkflow</code> attributes <code>input_dataset_dump</code> and <code>output_dataset_dump</code>, within apply endpoint (#990).</li> <li>Remove <code>asyncio.gather</code>, in view of SQLAlchemy2 update (#1004).</li> </ul> </li> </ul> </li> <li>Database:<ul> <li>Make foreign-keys of <code>ApplyWorkflow</code> (<code>project_id</code>, <code>workflow_id</code>, <code>input_dataset_id</code>, <code>output_dataset_id</code>) optional (#927).</li> <li>Add columns <code>input_dataset_dump</code>, <code>output_dataset_dump</code> and <code>user_email</code> to <code>ApplyWorkflow</code> (#927).</li> <li>Add relations <code>Dataset.list_jobs_input</code> and <code>Dataset.list_jobs_output</code> (#927).</li> <li>Make <code>ApplyWorkflow.start_timestamp</code> non-nullable (#927).</li> <li>Remove <code>\"cascade\": \"all, delete-orphan\"</code> from <code>Project.job_list</code> (#927).</li> <li>Add <code>Workflow.job_list</code> relation (#927).</li> <li>Do not use <code>Enum</code>s as column types (e.g. for <code>ApplyWorkflow.status</code>), but only for (de-)serialization (#974).</li> <li>Set <code>pool_pre_ping</code> option to <code>True</code>, for asyncpg driver (#1037).</li> <li>Add script for updating DB from 1.4.0 to 1.4.1 (#1010)</li> <li>Fix missing try/except in sync session (#1020).</li> </ul> </li> <li>App:<ul> <li>Skip creation of first-superuser when one superuser already exists (#1006).</li> </ul> </li> <li>Dependencies:<ul> <li>Update sqlalchemy to version <code>&gt;=2.0.23,&lt;2.1</code> (#1044).</li> <li>Update sqlmodel to version 0.0.12 (#1044).</li> <li>Upgrade asyncpg to version 0.29.0 (#1036).</li> </ul> </li> <li>Runner:<ul> <li>Refresh DB objects within <code>submit_workflow</code> (#927).</li> </ul> </li> <li>Testing:<ul> <li>Add <code>await db_engine.dispose()</code> in <code>db_create_tables</code> fixture (#1047).</li> <li>Set <code>debug=False</code> in <code>event_loop</code> fixture (#1044).</li> <li>Improve <code>test_full_workflow.py</code> (#971).</li> <li>Update <code>pytest-asyncio</code> to v0.21 (#1008).</li> <li>Fix CI issue related to event loop and asyncpg (#1012).</li> <li>Add GitHub Action testing database migrations (#1010).</li> <li>Use greenlet v3 in <code>poetry.lock</code> (#1044).</li> </ul> </li> <li>Documentation:<ul> <li>Add OAuth2 example endpoints to Web API page (#1034, #1038).</li> </ul> </li> <li>Development:<ul> <li>Use poetry 1.7.1 (#1043).</li> </ul> </li> </ul>"},{"location":"changelog/#1314-do-not-use","title":"1.3.14 (do not use!)","text":"<p>WARNING: This version introduces a change that is then reverted in 1.4.0, namely it sets the <code>ApplyWorkflow.status</code> type to <code>Enum</code>, when used with PostgreSQL. It is recommended to not use it, and upgrade to 1.4.0 directly.</p> <ul> <li>Make <code>Dataset.resource_list</code> an <code>ordering_list</code>, ordered by <code>Resource.id</code> (#951).</li> <li>Expose <code>redirect_url</code> for OAuth clients (#953).</li> <li>Expose JSON Schema for the <code>ManifestV1</code> Pydantic model (#942).</li> <li>Improve delete-resource endpoint (#943).</li> <li>Dependencies:<ul> <li>Upgrade sqlmodel to 0.0.11 (#949).</li> </ul> </li> <li>Testing:<ul> <li>Fix bug in local tests with Docker/SLURM (#948).</li> </ul> </li> </ul>"},{"location":"changelog/#1313","title":"1.3.13","text":"<ul> <li>Configure sqlite WAL to avoid \"database is locked\" errors (#860).</li> <li>Dependencies:<ul> <li>Add <code>sqlalchemy[asyncio]</code> extra, and do not directly require <code>greenlet</code> (#895).</li> <li>Fix <code>cloudpickle</code>-version definition in <code>pyproject.toml</code> (#937).</li> <li>Remove obsolete <code>sqlalchemy_utils</code> dependency (#939).</li> </ul> </li> <li>Testing:<ul> <li>Use ubuntu-22 for GitHub CI (#909).</li> <li>Run GitHub CI both with SQLite and Postgres (#915).</li> <li>Disable <code>postgres</code> service in GitHub action when running tests with SQLite (#931).</li> <li>Make <code>test_commands.py</code> tests stateless, also when running with Postgres (#917).</li> </ul> </li> <li>Documentation:<ul> <li>Add information about minimal supported SQLite version (#916).</li> </ul> </li> </ul>"},{"location":"changelog/#1312","title":"1.3.12","text":"<ul> <li>Project creation:<ul> <li>Do not automatically create a dataset upon project creation (#897).</li> <li>Remove <code>ProjectCreate.default_dataset_name</code> attribute (#897).</li> </ul> </li> <li>Dataset history:<ul> <li>Create a new (non-nullable) history column in <code>Dataset</code> table (#898, #901).</li> <li>Deprecate history handling in <code>/project/{project_id}/job/{job_id}</code> endpoint (#898).</li> <li>Deprecate <code>HISTORY_LEGACY</code> (#898).</li> </ul> </li> <li>Testing:<ul> <li>Remove obsolete fixture <code>slurm_config</code> (#903).</li> </ul> </li> </ul>"},{"location":"changelog/#1311","title":"1.3.11","text":"<p>This is mainly a bugfix release for the <code>PermissionError</code> issue.</p> <ul> <li>Fix <code>PermissionError</code>s in parallel-task metadata aggregation for the SLURM backend (#893).</li> <li>Documentation:<ul> <li>Bump <code>mkdocs-render-swagger-plugin</code> to 0.1.0 (#889).</li> </ul> </li> <li>Testing:<ul> <li>Fix <code>poetry install</code> command and <code>poetry</code> version in GitHub CI (#889).</li> </ul> </li> </ul>"},{"location":"changelog/#1310","title":"1.3.10","text":"<p>Warning: updating to this version requires changes to the configuration variable</p> <ul> <li>Updates to SLURM interface:<ul> <li>Remove <code>sudo</code>-requiring <code>ls</code> calls from <code>FractalFileWaitThread.check</code> (#885);</li> <li>Change default of <code>FRACTAL_SLURM_POLL_INTERVAL</code> to 5 seconds (#885);</li> <li>Rename <code>FRACTAL_SLURM_OUTPUT_FILE_GRACE_TIME</code> configuration variables into <code>FRACTAL_SLURM_ERROR_HANDLING_INTERVAL</code> (#885);</li> <li>Remove <code>FRACTAL_SLURM_KILLWAIT_INTERVAL</code> variable and corresponding logic (#885);</li> <li>Remove <code>_multiple_paths_exist_as_user</code> helper function (#885);</li> <li>Review type hints and default values of SLURM-related configuration variables (#885).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>fastapi</code> to version <code>^0.103.0</code> (#877);</li> <li>Update <code>fastapi-users</code> to version <code>^12.1.0</code> (#877).</li> </ul> </li> </ul>"},{"location":"changelog/#139","title":"1.3.9","text":"<ul> <li>Make updated-metadata collection robust for metadiff files consisting of a single <code>null</code> value (#879).</li> <li>Automate procedure for publishing package to PyPI (#881).</li> </ul>"},{"location":"changelog/#138","title":"1.3.8","text":"<ul> <li>Backend runner:<ul> <li>Add aggregation logic for parallel-task updated metadata (#852);</li> <li>Make updated-metadata collection robust for missing files (#852, #863).</li> </ul> </li> <li>Database interface:</li> <li>API:<ul> <li>Prevent user from bypassing workflow-name constraint via the PATCH endpoint (#867).</li> <li>Handle error upon task collection, when tasks exist in the database but not on-disk (#874).</li> <li>Add <code>_check_project_exists</code> helper function (#872).</li> </ul> </li> <li>Configuration variables:<ul> <li>Remove <code>DEPLOYMENT_TYPE</code> variable and update <code>alive</code> endpoint (#875);</li> <li>Introduce <code>Settings.check_db</code> method, and call it during inline/offline migrations (#855);</li> <li>Introduce <code>Settings.check_runner</code> method (#875);</li> <li>Fail if <code>FRACTAL_BACKEND_RUNNER</code> is <code>\"local\"</code> and <code>FRACTAL_LOCAL_CONFIG_FILE</code> is set but missing on-disk (#875);</li> <li>Clean up <code>Settings.check</code> method and improve its coverage (#875);</li> </ul> </li> <li>Package, repository, documentation:<ul> <li>Change <code>fractal_server.common</code> from being a git-submodule to being a regular folder (#859).</li> <li>Pin documentation dependencies (#865).</li> <li>Split <code>app/models/project.py</code> into two modules for dataset and project (#871).</li> <li>Revamp documentation on database interface and on the corresponding configuration variables (#855).</li> </ul> </li> </ul>"},{"location":"changelog/#137","title":"1.3.7","text":"<ul> <li>Oauth2-related updates (#822):<ul> <li>Update configuration of OAuth2 clients, to support OIDC/GitHub/Google;</li> <li>Merge <code>SQLModelBaseOAuthAccount</code> and <code>OAuthAccount</code> models;</li> <li>Update <code>UserOAuth.oauth_accounts</code> relationship and fix <code>list_users</code> endpoint accordingly;</li> <li>Introduce dummy <code>UserManager.on_after_login</code> method;</li> <li>Rename <code>OAuthClient</code> into <code>OAuthClientConfig</code>;</li> <li>Revamp users-related parts of documentation.</li> </ul> </li> </ul>"},{"location":"changelog/#136","title":"1.3.6","text":"<ul> <li>Update <code>output_dataset.meta</code> also when workflow execution fails (#843).</li> <li>Improve error message for unknown errors in job execution (#843).</li> <li>Fix log message incorrectly marked as \"error\" (#846).</li> </ul>"},{"location":"changelog/#135","title":"1.3.5","text":"<ul> <li>Review structure of dataset history (#803):<ul> <li>Re-define structure for <code>history</code> property of <code>Dataset.meta</code>;</li> <li>Introduce <code>\"api/v1/project/{project_id}/dataset/{dataset_id}/status/\"</code> endpoint;</li> <li>Introduce <code>\"api/v1/project/{project_id}/dataset/{dataset_id}/export_history/\"</code> endpoint;</li> <li>Move legacy history to <code>Dataset.meta[\"HISTORY_LEGACY\"]</code>.</li> </ul> </li> <li>Make <code>first_task_index</code> and <code>last_task_index</code> properties of <code>ApplyWorkflow</code> required (#803).</li> <li>Add <code>docs_info</code> and <code>docs_link</code> to Task model (#814)</li> <li>Accept <code>TaskUpdate.version=None</code> in task-patch endpoint (#818).</li> <li>Store a copy of the <code>Workflow</code> into the optional column <code>ApplyWorkflow.workflow_dump</code> at the time of submission (#804, #834).</li> <li>Prevent execution of multiple jobs with the same output dataset (#801).</li> <li>Transform non-absolute <code>FRACTAL_TASKS_DIR</code> into absolute paths, relative to the current working directory (#825).</li> <li>Error handling:<ul> <li>Raise an appropriate error if a task command is not executable (#800).</li> <li>Improve handling of errors raised in <code>get_slurm_config</code> (#800).</li> </ul> </li> <li>Documentation:<ul> <li>Clarify documentation about <code>SlurmConfig</code> (#798).</li> <li>Update documentation configuration and GitHub actions (#811).</li> </ul> </li> <li>Tests:<ul> <li>Move <code>tests/test_common.py</code> into <code>fractal-common</code> repository (#808).</li> <li>Switch to <code>docker compose</code> v2 and unpin <code>pyyaml</code> version (#816).</li> </ul> </li> </ul>"},{"location":"changelog/#134","title":"1.3.4","text":"<ul> <li>Support execution of a workflow subset (#784).</li> <li>Fix internal server error for invalid <code>task_id</code> in <code>create_workflowtask</code> endpoint (#782).</li> <li>Improve logging in background task collection (#776).</li> <li>Handle failures in <code>submit_workflow</code> without raising errors (#787).</li> <li>Simplify internal function for execution of a list of task (#780).</li> <li>Exclude <code>common/tests</code> and other git-related files from build (#795).</li> <li>Remove development dependencies <code>Pillow</code> and <code>pytest-mock</code> (#795).</li> <li>Remove obsolete folders from <code>tests/data</code> folder (#795).</li> </ul>"},{"location":"changelog/#133","title":"1.3.3","text":"<ul> <li>Pin Pydantic to v1 (#779).</li> </ul>"},{"location":"changelog/#132","title":"1.3.2","text":"<ul> <li>Add sqlalchemy naming convention for DB constraints, and add <code>render_as_batch=True</code> to <code>do_run_migrations</code> (#757).</li> <li>Fix bug in job-stop endpoint, due to missing default for <code>FractalSlurmExecutor.wait_thread.shutdown_file</code> (#768, #769).</li> <li>Fix bug upon inserting a task with <code>meta=None</code> into a Workflow (#772).</li> </ul>"},{"location":"changelog/#131","title":"1.3.1","text":"<ul> <li>Fix return value of stop-job endpoint (#764).</li> <li>Expose new GET <code>WorkflowTask</code> endpoint (#762).</li> <li>Clean up API modules (#762):<ul> <li>Split workflow/workflowtask modules;</li> <li>Split tasks/task-collection modules.</li> </ul> </li> </ul>"},{"location":"changelog/#130","title":"1.3.0","text":"<ul> <li>Refactor user model:<ul> <li>Switch from UUID4 to int for IDs (#660, #684).</li> <li>Fix many-to-many relationship between users and project (#660).</li> <li>Rename <code>Project.user_member_list</code> into <code>Project.user_list</code> (#660).</li> <li>Add <code>username</code> column (#704).</li> </ul> </li> <li>Update endpoints (see also 1.2-&gt;1.3 upgrade info in the documentation):<ul> <li>Review endpoint URLs (#669).</li> <li>Remove foreign keys from payloads (#669).</li> </ul> </li> <li>Update <code>Task</code> models, task collection and task-related endpoints:<ul> <li>Add <code>version</code> and <code>owner</code> columns to <code>Task</code> model (#704).</li> <li>Set <code>Task.version</code> during task collection (#719).</li> <li>Set <code>Task.owner</code> as part of create-task endpoint (#704).</li> <li>For custom tasks, prepend <code>owner</code> to user-provided <code>source</code> (#725).</li> <li>Remove <code>default_args</code> from <code>Tasks</code> model and from manifest tasks (#707).</li> <li>Add <code>args_schema</code> and <code>args_schema_version</code> to <code>Task</code> model (#707).</li> <li>Expose <code>args_schema</code> and <code>args_schema_version</code> in task POST/PATCH endpoints (#749).</li> <li>Make <code>Task.source</code> task-specific rather than package-specific (#719).</li> <li>Make <code>Task.source</code> unique (#725).</li> <li>Update <code>_TaskCollectPip</code> methods, attributes and properties (#719).</li> <li>Remove private/public options for task collection (#704).</li> <li>Improve error message for missing package manifest (#704).</li> <li>Improve behavior when task-collection folder already exists (#704).</li> <li>Expose <code>pinned_package_version</code> for tasks collection (#744).</li> <li>Restrict Task editing to superusers and task owners (#733).</li> <li>Implement <code>delete_task</code> endpoint (#745).</li> </ul> </li> <li>Update <code>Workflow</code> and <code>WorkflowTask</code> endpoints:<ul> <li>Always merge new <code>WorkflowTask.args</code> with defaults from <code>Task.args_schema</code>, in <code>update_workflowtask</code> endpoint (#759).</li> <li>Remove <code>WorkflowTask.overridden_meta</code> property and on-the-fly overriding of <code>meta</code> (#752).</li> <li>Add warning when exporting workflows which include custom tasks (#728).</li> <li>When importing a workflow, only use tasks' <code>source</code> values, instead of <code>(source,name)</code> pairs (#719).</li> </ul> </li> <li>Job execution:<ul> <li>Add <code>FractalSlurmExecutor.shutdown</code> and corresponding endpoint (#631, #691, #696).</li> <li>In <code>FractalSlurmExecutor</code>, make <code>working_dir*</code> attributes required (#679).</li> <li>Remove <code>ApplyWorkflow.overwrite_input</code> column (#684, #694).</li> <li>Make <code>output_dataset_id</code> a required argument of apply-workflow endpoint (#681).</li> <li>Improve error message related to out-of-space disk (#699).</li> <li>Include timestamp in job working directory, to avoid name clashes (#756).</li> </ul> </li> <li>Other updates to endpoints and database:<ul> <li>Add <code>ApplyWorkflow.end_timestamp</code> column (#687, #684).</li> <li>Prevent deletion of a <code>Workflow</code>/<code>Dataset</code> in relationship with existing <code>ApplyWorkflow</code> (#703).</li> <li>Add project-name uniqueness constraint in project-edit endpoint (#689).</li> </ul> </li> <li>Other updates to internal logic:<ul> <li>Drop <code>WorkflowTask.arguments</code> property and <code>WorkflowTask.assemble_args</code> method (#742).</li> <li>Add test for collection of tasks packages with tasks in a subpackage (#743).</li> <li>Expose <code>FRACTAL_CORS_ALLOW_ORIGIN</code> environment variable (#688).</li> <li>Expose <code>FRACTAL_DEFAULT_ADMIN_USERNAME</code> environment variable (#751).</li> </ul> </li> <li>Package and repository:<ul> <li>Remove <code>fastapi-users-db-sqlmodel</code> dependency (#660).</li> <li>Make coverage measure more accurate (#676) and improve coverage (#678).</li> <li>Require pydantic version to be <code>&gt;=1.10.8</code> (#711, #713).</li> <li>Include multiple <code>fractal-common</code> updates (#705, #719).</li> <li>Add test equivalent to <code>alembic check</code> (#722).</li> <li>Update <code>poetry.lock</code> to address security alerts (#723).</li> <li>Remove <code>sqlmodel</code> from <code>fractal-common</code>, and declare database models with multiple inheritance (#710).</li> <li>Make email generation more robust in <code>MockCurrentUser</code> (#730).</li> <li>Update <code>poetry.lock</code> to <code>cryptography=41</code>, to address security alert (#739).</li> <li>Add <code>greenlet</code> as a direct dependency (#748).</li> <li>Removed tests for <code>IntegrityError</code> (#754).</li> </ul> </li> </ul>"},{"location":"changelog/#125","title":"1.2.5","text":"<ul> <li>Fix bug in task collection when using sqlite (#664, #673).</li> <li>Fix bug in task collection from local package, where package extras were not considered (#671).</li> <li>Improve error handling in workflow-apply endpoint (#665).</li> <li>Fix a bug upon project removal in the presence of project-related jobs (#666). Note: this removes the <code>ApplyWorkflow.Project</code> attribute.</li> </ul>"},{"location":"changelog/#124","title":"1.2.4","text":"<ul> <li>Review setup for database URLs, especially to allow using UNIX-socket connections for postgresl (#657).</li> </ul>"},{"location":"changelog/#123","title":"1.2.3","text":"<ul> <li>Fix bug that was keeping multiple database conection open (#649).</li> </ul>"},{"location":"changelog/#122","title":"1.2.2","text":"<ul> <li>Fix bug related to <code>user_local_exports</code> in SLURM-backend configuration (#642).</li> </ul>"},{"location":"changelog/#121","title":"1.2.1","text":"<ul> <li>Fix bug upon creation of first user when using multiple workers (#632).</li> <li>Allow both ports 5173 and 4173 as CORS origins (#637).</li> </ul>"},{"location":"changelog/#120","title":"1.2.0","text":"<ul> <li>Drop <code>project.project_dir</code> and replace it with <code>user.cache_dir</code> (#601).</li> <li>Update SLURM backend (#582, #612, #614); this includes (1) combining several tasks in a single SLURM job, and (2) offering more granular sources for SLURM configuration options.</li> <li>Expose local user exports in SLURM configuration file (#625).</li> <li>Make local backend rely on custom <code>FractalThreadPoolExecutor</code>, where <code>parallel_tasks_per_job</code> can affect parallelism (#626).</li> <li>Review logging configuration (#619, #623).</li> <li>Update to fastapi <code>0.95</code> (#587).</li> <li>Minor improvements in dataset-edit endpoint (#593) and tests (#589).</li> <li>Include test of non-python task (#594).</li> <li>Move dummy tasks from package to tests (#601).</li> <li>Remove deprecated parsl backend (#607).</li> <li>Improve error handling in workflow-import endpoint (#595).</li> <li>Also show logs for successful workflow execution (#635).</li> </ul>"},{"location":"changelog/#111","title":"1.1.1","text":"<ul> <li>Include <code>reordered_workflowtask_ids</code> in workflow-edit endpoint payload, to reorder the task list of a workflow (#585).</li> </ul>"},{"location":"changelog/#110","title":"1.1.0","text":"<ul> <li>Align with new tasks interface in <code>fractal-tasks-core&gt;=0.8.0</code>, and remove <code>glob_pattern</code> column from <code>resource</code> database table (#544).</li> <li>Drop python 3.8 support (#527).</li> <li>Improve validation of API request payloads (#545).</li> <li>Improve request validation in project-creation endpoint (#537).</li> <li>Update the endpoint to patch a <code>Task</code> (#526).</li> <li>Add new project-update endpoint, and relax constraints on <code>project_dir</code> in new-project endpoint (#563).</li> <li>Update <code>DatasetUpdate</code> schema (#558 and #565).</li> <li>Fix redundant task-error logs in slurm backend (#552).</li> <li>Improve handling of task-collection errors (#559).</li> <li>If <code>FRACTAL_BACKEND_RUNNER=slurm</code>, include some configuration checks at server startup (#529).</li> <li>Fail if <code>FRACTAL_SLURM_WORKER_PYTHON</code> has different versions of <code>fractal-server</code> or <code>cloudpickle</code> (#533).</li> </ul>"},{"location":"changelog/#108","title":"1.0.8","text":"<ul> <li>Fix handling of parallel-tasks errors in <code>FractalSlurmExecutor</code> (#497).</li> <li>Add test for custom tasks (#500).</li> <li>Improve formatting of job logs (#503).</li> <li>Improve error handling in workflow-execution server endpoint (#515).</li> <li>Update <code>_TaskBase</code> schema from fractal-common (#517).</li> </ul>"},{"location":"changelog/#107","title":"1.0.7","text":"<ul> <li>Update endpoints to import/export a workflow (#495).</li> </ul>"},{"location":"changelog/#106","title":"1.0.6","text":"<ul> <li>Add new endpoints to import/export a workflow (#490).</li> </ul>"},{"location":"changelog/#105","title":"1.0.5","text":"<ul> <li>Separate workflow-execution folder into two (server- and user-owned) folders, to avoid permission issues (#475).</li> <li>Explicitly pin sqlalchemy to v1 (#480).</li> </ul>"},{"location":"changelog/#104","title":"1.0.4","text":"<ul> <li>Add new POST endpoint to create new Task (#486).</li> </ul>"},{"location":"changelog/#103","title":"1.0.3","text":"<p>Missing due to releasing error.</p>"},{"location":"changelog/#102","title":"1.0.2","text":"<ul> <li>Add <code>FRACTAL_RUNNER_MAX_TASKS_PER_WORKFLOW</code> configuration variable (#469).</li> </ul>"},{"location":"changelog/#101","title":"1.0.1","text":"<ul> <li>Fix bug with environment variable names (#468).</li> </ul>"},{"location":"changelog/#100","title":"1.0.0","text":"<ul> <li>First release listed in CHANGELOG.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>To configure the Fractal Server one must define some environment variables. Some of them are required, and the server will not start unless they are set. Some are optional and sensible defaults are provided.</p> <p>The required variables are the following</p> <pre><code>JWT_SECRET_KEY\nFRACTAL_TASKS_DIR\nFRACTAL_RUNNER_WORKING_BASE_DIR\n</code></pre> <p>together with the database-specific variables: <code>SQLITE_PATH</code> for SQLite, or <code>DB_ENGINE=postgres</code> and <code>POSTGRES_DB</code> for Postgres.</p>"},{"location":"configuration/#fractal_server.config.OAuthClientConfig","title":"<code>OAuthClientConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>OAuth Client Config Model</p> <p>This model wraps the variables that define a client against an Identity Provider. As some providers are supported by the libraries used within the server, some attributes are optional.</p> <p>Attributes:</p> Name Type Description <code>CLIENT_NAME</code> <code>str</code> <p>The name of the client</p> <code>CLIENT_ID</code> <code>str</code> <p>ID of client</p> <code>CLIENT_SECRET</code> <code>str</code> <p>Secret to authorise against the identity provider</p> <code>OIDC_CONFIGURATION_ENDPOINT</code> <code>Optional[str]</code> <p>OpenID configuration endpoint, allowing to discover the required endpoints automatically</p> <code>REDIRECT_URL</code> <code>Optional[str]</code> <p>String to be used as <code>redirect_url</code> argument for <code>fastapi_users.get_oauth_router</code>, and then in <code>httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback</code>.</p> Source code in <code>fractal_server/config.py</code> <pre><code>class OAuthClientConfig(BaseModel):\n    \"\"\"\n    OAuth Client Config Model\n\n    This model wraps the variables that define a client against an Identity\n    Provider. As some providers are supported by the libraries used within the\n    server, some attributes are optional.\n\n    Attributes:\n        CLIENT_NAME:\n            The name of the client\n        CLIENT_ID:\n            ID of client\n        CLIENT_SECRET:\n            Secret to authorise against the identity provider\n        OIDC_CONFIGURATION_ENDPOINT:\n            OpenID configuration endpoint,\n            allowing to discover the required endpoints automatically\n        REDIRECT_URL:\n            String to be used as `redirect_url` argument for\n            `fastapi_users.get_oauth_router`, and then in\n            `httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback`.\n    \"\"\"\n\n    CLIENT_NAME: str\n    CLIENT_ID: str\n    CLIENT_SECRET: str\n    OIDC_CONFIGURATION_ENDPOINT: Optional[str]\n    REDIRECT_URL: Optional[str] = None\n\n    @root_validator\n    def check_configuration(cls, values):\n        if values.get(\"CLIENT_NAME\") not in [\"GOOGLE\", \"GITHUB\"]:\n            if not values.get(\"OIDC_CONFIGURATION_ENDPOINT\"):\n                raise FractalConfigurationError(\n                    f\"Missing OAUTH_{values.get('CLIENT_NAME')}\"\n                    \"_OIDC_CONFIGURATION_ENDPOINT\"\n                )\n        return values\n</code></pre>"},{"location":"configuration/#fractal_server.config.Settings","title":"<code>Settings</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Contains all the configuration variables for Fractal Server</p> <p>The attributes of this class are set from the environtment.</p> Source code in <code>fractal_server/config.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"\n    Contains all the configuration variables for Fractal Server\n\n    The attributes of this class are set from the environtment.\n    \"\"\"\n\n    class Config:\n        case_sensitive = True\n\n    PROJECT_NAME: str = \"Fractal Server\"\n    PROJECT_VERSION: str = fractal_server.__VERSION__\n\n    ###########################################################################\n    # AUTH\n    ###########################################################################\n\n    OAUTH_CLIENTS_CONFIG: list[OAuthClientConfig] = Field(default_factory=list)\n\n    # JWT TOKEN\n    JWT_EXPIRE_SECONDS: int = 180\n    \"\"\"\n    JWT token lifetime, in seconds.\n    \"\"\"\n\n    JWT_SECRET_KEY: Optional[str]\n    \"\"\"\n    JWT secret\n\n    \u26a0\ufe0f **IMPORTANT**: set this variable to a secure string, and do not disclose\n    it.\n    \"\"\"\n\n    # COOKIE TOKEN\n    COOKIE_EXPIRE_SECONDS: int = 86400\n    \"\"\"\n    Cookie token lifetime, in seconds.\n    \"\"\"\n\n    @root_validator(pre=True)\n    def collect_oauth_clients(cls, values):\n        \"\"\"\n        Automatic collection of OAuth Clients\n\n        This method collects the environment variables relative to a single\n        OAuth client and saves them within the `Settings` object in the form\n        of an `OAuthClientConfig` instance.\n\n        Fractal can support an arbitrary number of OAuth providers, which are\n        automatically detected by parsing the environment variable names. In\n        particular, to set the provider `FOO`, one must specify the variables\n\n            OAUTH_FOO_CLIENT_ID\n            OAUTH_FOO_CLIENT_SECRET\n            ...\n\n        etc (cf. OAuthClientConfig).\n        \"\"\"\n        oauth_env_variable_keys = [\n            key for key in environ.keys() if key.startswith(\"OAUTH_\")\n        ]\n        clients_available = {\n            var.split(\"_\")[1] for var in oauth_env_variable_keys\n        }\n\n        values[\"OAUTH_CLIENTS_CONFIG\"] = []\n        for client in clients_available:\n            prefix = f\"OAUTH_{client}\"\n            oauth_client_config = OAuthClientConfig(\n                CLIENT_NAME=client,\n                CLIENT_ID=getenv(f\"{prefix}_CLIENT_ID\", None),\n                CLIENT_SECRET=getenv(f\"{prefix}_CLIENT_SECRET\", None),\n                OIDC_CONFIGURATION_ENDPOINT=getenv(\n                    f\"{prefix}_OIDC_CONFIGURATION_ENDPOINT\", None\n                ),\n                REDIRECT_URL=getenv(f\"{prefix}_REDIRECT_URL\", None),\n            )\n            values[\"OAUTH_CLIENTS_CONFIG\"].append(oauth_client_config)\n        return values\n\n    ###########################################################################\n    # DATABASE\n    ###########################################################################\n    DB_ENGINE: Literal[\"sqlite\", \"postgres\"] = \"sqlite\"\n    \"\"\"\n    Select which database engine to use (supported: `sqlite` and `postgres`).\n    \"\"\"\n    DB_ECHO: bool = False\n    \"\"\"\n    If `True`, make database operations verbose.\n    \"\"\"\n    POSTGRES_USER: Optional[str]\n    \"\"\"\n    User to use when connecting to the PostgreSQL database.\n    \"\"\"\n    POSTGRES_PASSWORD: Optional[str]\n    \"\"\"\n    Password to use when connecting to the PostgreSQL database.\n    \"\"\"\n    POSTGRES_HOST: Optional[str] = \"localhost\"\n    \"\"\"\n    URL to the PostgreSQL server or path to a UNIX domain socket.\n    \"\"\"\n    POSTGRES_PORT: Optional[str] = \"5432\"\n    \"\"\"\n    Port number to use when connecting to the PostgreSQL server.\n    \"\"\"\n    POSTGRES_DB: Optional[str]\n    \"\"\"\n    Name of the PostgreSQL database to connect to.\n    \"\"\"\n\n    SQLITE_PATH: Optional[str]\n    \"\"\"\n    File path where the SQLite database is located (or will be located).\n    \"\"\"\n\n    @property\n    def DATABASE_URL(self) -&gt; URL:\n        if self.DB_ENGINE == \"sqlite\":\n            if not self.SQLITE_PATH:\n                raise FractalConfigurationError(\n                    \"SQLITE_PATH path cannot be None\"\n                )\n            sqlite_path = abspath(self.SQLITE_PATH)\n            url = URL.create(\n                drivername=\"sqlite+aiosqlite\",\n                database=sqlite_path,\n            )\n            return url\n        elif \"postgres\":\n            url = URL.create(\n                drivername=\"postgresql+asyncpg\",\n                username=self.POSTGRES_USER,\n                password=self.POSTGRES_PASSWORD,\n                host=self.POSTGRES_HOST,\n                port=self.POSTGRES_PORT,\n                database=self.POSTGRES_DB,\n            )\n            return url\n\n    @property\n    def DATABASE_SYNC_URL(self):\n        if self.DB_ENGINE == \"sqlite\":\n            if not self.SQLITE_PATH:\n                raise FractalConfigurationError(\n                    \"SQLITE_PATH path cannot be None\"\n                )\n            return self.DATABASE_URL.set(drivername=\"sqlite\")\n        elif self.DB_ENGINE == \"postgres\":\n            return self.DATABASE_URL.set(drivername=\"postgresql+psycopg2\")\n\n    ###########################################################################\n    # FRACTAL SPECIFIC\n    ###########################################################################\n\n    FRACTAL_DEFAULT_ADMIN_EMAIL: str = \"admin@fractal.xy\"\n    \"\"\"\n    Admin default email, used upon creation of the first superuser during\n    server startup.\n\n    \u26a0\ufe0f  **IMPORTANT**: After the server startup, you should always edit the\n    default admin credentials.\n    \"\"\"\n\n    FRACTAL_DEFAULT_ADMIN_PASSWORD: str = \"1234\"\n    \"\"\"\n    Admin default password, used upon creation of the first superuser during\n    server startup.\n\n    \u26a0\ufe0f **IMPORTANT**: After the server startup, you should always edit the\n    default admin credentials.\n    \"\"\"\n\n    FRACTAL_DEFAULT_ADMIN_USERNAME: str = \"admin\"\n    \"\"\"\n    Admin default username, used upon creation of the first superuser during\n    server startup.\n\n    \u26a0\ufe0f **IMPORTANT**: After the server startup, you should always edit the\n    default admin credentials.\n    \"\"\"\n\n    FRACTAL_TASKS_DIR: Optional[Path]\n    \"\"\"\n    Directory under which all the tasks will be saved (either an absolute path\n    or a path relative to current working directory).\n    \"\"\"\n\n    @validator(\"FRACTAL_TASKS_DIR\", always=True)\n    def make_FRACTAL_TASKS_DIR_absolute(cls, v):\n        \"\"\"\n        If `FRACTAL_TASKS_DIR` is a non-absolute path, make it absolute (based\n        on the current working directory).\n        \"\"\"\n        if v is None:\n            return None\n        FRACTAL_TASKS_DIR_path = Path(v)\n        if not FRACTAL_TASKS_DIR_path.is_absolute():\n            FRACTAL_TASKS_DIR_path = FRACTAL_TASKS_DIR_path.resolve()\n            logging.warning(\n                f'FRACTAL_TASKS_DIR=\"{v}\" is not an absolute path; '\n                f'converting it to \"{str(FRACTAL_TASKS_DIR_path)}\"'\n            )\n        return FRACTAL_TASKS_DIR_path\n\n    FRACTAL_RUNNER_BACKEND: Literal[\"local\", \"slurm\"] = \"local\"\n    \"\"\"\n    Select which runner backend to use.\n    \"\"\"\n\n    FRACTAL_RUNNER_WORKING_BASE_DIR: Optional[Path]\n    \"\"\"\n    Base directory for running jobs / workflows. All artifacts required to set\n    up, run and tear down jobs are placed in subdirs of this directory.\n    \"\"\"\n\n    FRACTAL_LOGGING_LEVEL: int = logging.INFO\n    \"\"\"\n    Logging-level threshold for logging\n\n    Only logs of with this level (or higher) will appear in the console logs;\n    see details [here](../internals/logs/).\n    \"\"\"\n\n    FRACTAL_LOCAL_CONFIG_FILE: Optional[Path]\n    \"\"\"\n    Path of JSON file with configuration for the local backend.\n    \"\"\"\n\n    FRACTAL_SLURM_CONFIG_FILE: Optional[Path]\n    \"\"\"\n    Path of JSON file with configuration for the SLURM backend.\n    \"\"\"\n\n    FRACTAL_SLURM_WORKER_PYTHON: Optional[str] = None\n    \"\"\"\n    Path to Python interpreter that will run the jobs on the SLURM nodes. If\n    not specified, the same interpreter that runs the server is used.\n    \"\"\"\n\n    FRACTAL_SLURM_POLL_INTERVAL: int = 5\n    \"\"\"\n    Interval to wait (in seconds) before checking whether unfinished job are\n    still running on SLURM (see `SlurmWaitThread` in\n    [`clusterfutures`](https://github.com/sampsyo/clusterfutures/blob/master/cfut/__init__.py)).\n    \"\"\"\n\n    FRACTAL_SLURM_ERROR_HANDLING_INTERVAL: int = 5\n    \"\"\"\n    Interval to wait (in seconds) when the SLURM backend does not find an\n    output pickle file - which could be due to several reasons (e.g. the SLURM\n    job was cancelled or failed, or writing the file is taking long). If the\n    file is still missing after this time interval, this leads to a\n    `JobExecutionError`.\n    \"\"\"\n\n    FRACTAL_API_SUBMIT_RATE_LIMIT: int = 2\n    \"\"\"\n    Interval to wait (in seconds) to be allowed to call again\n    `POST api/v1/{project_id}/workflow/{workflow_id}/apply/`\n    with the same path and query parameters.\n    \"\"\"\n\n    FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE: str = (\n        \"Copy OME-Zarr structure;Convert Metadata Components from 2D to 3D\"\n    )\n    \"\"\"\n    `;`-separated list of names for task that require the `metadata[\"image\"]`\n    attribute in their input-arguments JSON file.\n    \"\"\"\n\n    ###########################################################################\n    # BUSINESS LOGIC\n    ###########################################################################\n    def check_db(self) -&gt; None:\n        \"\"\"\n        Checks that db environment variables are properly set.\n        \"\"\"\n        if self.DB_ENGINE == \"postgres\":\n            if not self.POSTGRES_DB:\n                raise FractalConfigurationError(\n                    \"POSTGRES_DB cannot be None when DB_ENGINE=postgres.\"\n                )\n            try:\n                import psycopg2  # noqa: F401\n                import asyncpg  # noqa: F401\n            except ModuleNotFoundError:\n                raise FractalConfigurationError(\n                    \"DB engine is `postgres` but `psycopg2` or `asyncpg` \"\n                    \"are not available\"\n                )\n        else:\n            if not self.SQLITE_PATH:\n                raise FractalConfigurationError(\n                    \"SQLITE_PATH cannot be None when DB_ENGINE=sqlite.\"\n                )\n\n    def check_runner(self) -&gt; None:\n\n        if not self.FRACTAL_RUNNER_WORKING_BASE_DIR:\n            raise FractalConfigurationError(\n                \"FRACTAL_RUNNER_WORKING_BASE_DIR cannot be None.\"\n            )\n\n        info = f\"FRACTAL_RUNNER_BACKEND={self.FRACTAL_RUNNER_BACKEND}\"\n        if self.FRACTAL_RUNNER_BACKEND == \"slurm\":\n\n            from fractal_server.app.runner.executors.slurm._slurm_config import (  # noqa: E501\n                load_slurm_config_file,\n            )\n\n            if not self.FRACTAL_SLURM_CONFIG_FILE:\n                raise FractalConfigurationError(\n                    f\"Must set FRACTAL_SLURM_CONFIG_FILE when {info}\"\n                )\n            else:\n                if not self.FRACTAL_SLURM_CONFIG_FILE.exists():\n                    raise FractalConfigurationError(\n                        f\"{info} but FRACTAL_SLURM_CONFIG_FILE=\"\n                        f\"{self.FRACTAL_SLURM_CONFIG_FILE} not found.\"\n                    )\n\n                load_slurm_config_file(self.FRACTAL_SLURM_CONFIG_FILE)\n                if not shutil.which(\"sbatch\"):\n                    raise FractalConfigurationError(\n                        f\"{info} but `sbatch` command not found.\"\n                    )\n                if not shutil.which(\"squeue\"):\n                    raise FractalConfigurationError(\n                        f\"{info} but `squeue` command not found.\"\n                    )\n        else:  # i.e. self.FRACTAL_RUNNER_BACKEND == \"local\"\n            if self.FRACTAL_LOCAL_CONFIG_FILE:\n                if not self.FRACTAL_LOCAL_CONFIG_FILE.exists():\n                    raise FractalConfigurationError(\n                        f\"{info} but FRACTAL_LOCAL_CONFIG_FILE=\"\n                        f\"{self.FRACTAL_LOCAL_CONFIG_FILE} not found.\"\n                    )\n\n    def check(self):\n        \"\"\"\n        Make sure that required variables are set\n\n        This method must be called before the server starts\n        \"\"\"\n\n        if not self.JWT_SECRET_KEY:\n            raise FractalConfigurationError(\"JWT_SECRET_KEY cannot be None\")\n\n        if not self.FRACTAL_TASKS_DIR:\n            raise FractalConfigurationError(\"FRACTAL_TASKS_DIR cannot be None\")\n\n        self.check_db()\n        self.check_runner()\n</code></pre>"},{"location":"configuration/#fractal_server.config.Settings.COOKIE_EXPIRE_SECONDS","title":"<code>COOKIE_EXPIRE_SECONDS: int = 86400</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Cookie token lifetime, in seconds.</p>"},{"location":"configuration/#fractal_server.config.Settings.DB_ECHO","title":"<code>DB_ECHO: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>True</code>, make database operations verbose.</p>"},{"location":"configuration/#fractal_server.config.Settings.DB_ENGINE","title":"<code>DB_ENGINE: Literal['sqlite', 'postgres'] = 'sqlite'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Select which database engine to use (supported: <code>sqlite</code> and <code>postgres</code>).</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_API_SUBMIT_RATE_LIMIT","title":"<code>FRACTAL_API_SUBMIT_RATE_LIMIT: int = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Interval to wait (in seconds) to be allowed to call again <code>POST api/v1/{project_id}/workflow/{workflow_id}/apply/</code> with the same path and query parameters.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_DEFAULT_ADMIN_EMAIL","title":"<code>FRACTAL_DEFAULT_ADMIN_EMAIL: str = 'admin@fractal.xy'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Admin default email, used upon creation of the first superuser during server startup.</p> <p>\u26a0\ufe0f  IMPORTANT: After the server startup, you should always edit the default admin credentials.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_DEFAULT_ADMIN_PASSWORD","title":"<code>FRACTAL_DEFAULT_ADMIN_PASSWORD: str = '1234'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Admin default password, used upon creation of the first superuser during server startup.</p> <p>\u26a0\ufe0f IMPORTANT: After the server startup, you should always edit the default admin credentials.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_DEFAULT_ADMIN_USERNAME","title":"<code>FRACTAL_DEFAULT_ADMIN_USERNAME: str = 'admin'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Admin default username, used upon creation of the first superuser during server startup.</p> <p>\u26a0\ufe0f IMPORTANT: After the server startup, you should always edit the default admin credentials.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_LOCAL_CONFIG_FILE","title":"<code>FRACTAL_LOCAL_CONFIG_FILE: Optional[Path]</code>  <code>instance-attribute</code>","text":"<p>Path of JSON file with configuration for the local backend.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_LOGGING_LEVEL","title":"<code>FRACTAL_LOGGING_LEVEL: int = logging.INFO</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Logging-level threshold for logging</p> <p>Only logs of with this level (or higher) will appear in the console logs; see details here.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_RUNNER_BACKEND","title":"<code>FRACTAL_RUNNER_BACKEND: Literal['local', 'slurm'] = 'local'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Select which runner backend to use.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE","title":"<code>FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE: str = 'Copy OME-Zarr structure;Convert Metadata Components from 2D to 3D'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p><code>;</code>-separated list of names for task that require the <code>metadata[\"image\"]</code> attribute in their input-arguments JSON file.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_RUNNER_WORKING_BASE_DIR","title":"<code>FRACTAL_RUNNER_WORKING_BASE_DIR: Optional[Path]</code>  <code>instance-attribute</code>","text":"<p>Base directory for running jobs / workflows. All artifacts required to set up, run and tear down jobs are placed in subdirs of this directory.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_SLURM_CONFIG_FILE","title":"<code>FRACTAL_SLURM_CONFIG_FILE: Optional[Path]</code>  <code>instance-attribute</code>","text":"<p>Path of JSON file with configuration for the SLURM backend.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_SLURM_ERROR_HANDLING_INTERVAL","title":"<code>FRACTAL_SLURM_ERROR_HANDLING_INTERVAL: int = 5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Interval to wait (in seconds) when the SLURM backend does not find an output pickle file - which could be due to several reasons (e.g. the SLURM job was cancelled or failed, or writing the file is taking long). If the file is still missing after this time interval, this leads to a <code>JobExecutionError</code>.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_SLURM_POLL_INTERVAL","title":"<code>FRACTAL_SLURM_POLL_INTERVAL: int = 5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Interval to wait (in seconds) before checking whether unfinished job are still running on SLURM (see <code>SlurmWaitThread</code> in <code>clusterfutures</code>).</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_SLURM_WORKER_PYTHON","title":"<code>FRACTAL_SLURM_WORKER_PYTHON: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to Python interpreter that will run the jobs on the SLURM nodes. If not specified, the same interpreter that runs the server is used.</p>"},{"location":"configuration/#fractal_server.config.Settings.FRACTAL_TASKS_DIR","title":"<code>FRACTAL_TASKS_DIR: Optional[Path]</code>  <code>instance-attribute</code>","text":"<p>Directory under which all the tasks will be saved (either an absolute path or a path relative to current working directory).</p>"},{"location":"configuration/#fractal_server.config.Settings.JWT_EXPIRE_SECONDS","title":"<code>JWT_EXPIRE_SECONDS: int = 180</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>JWT token lifetime, in seconds.</p>"},{"location":"configuration/#fractal_server.config.Settings.JWT_SECRET_KEY","title":"<code>JWT_SECRET_KEY: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>JWT secret</p> <p>\u26a0\ufe0f IMPORTANT: set this variable to a secure string, and do not disclose it.</p>"},{"location":"configuration/#fractal_server.config.Settings.POSTGRES_DB","title":"<code>POSTGRES_DB: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>Name of the PostgreSQL database to connect to.</p>"},{"location":"configuration/#fractal_server.config.Settings.POSTGRES_HOST","title":"<code>POSTGRES_HOST: Optional[str] = 'localhost'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>URL to the PostgreSQL server or path to a UNIX domain socket.</p>"},{"location":"configuration/#fractal_server.config.Settings.POSTGRES_PASSWORD","title":"<code>POSTGRES_PASSWORD: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>Password to use when connecting to the PostgreSQL database.</p>"},{"location":"configuration/#fractal_server.config.Settings.POSTGRES_PORT","title":"<code>POSTGRES_PORT: Optional[str] = '5432'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Port number to use when connecting to the PostgreSQL server.</p>"},{"location":"configuration/#fractal_server.config.Settings.POSTGRES_USER","title":"<code>POSTGRES_USER: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>User to use when connecting to the PostgreSQL database.</p>"},{"location":"configuration/#fractal_server.config.Settings.SQLITE_PATH","title":"<code>SQLITE_PATH: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>File path where the SQLite database is located (or will be located).</p>"},{"location":"configuration/#fractal_server.config.Settings.check","title":"<code>check()</code>","text":"<p>Make sure that required variables are set</p> <p>This method must be called before the server starts</p> Source code in <code>fractal_server/config.py</code> <pre><code>def check(self):\n    \"\"\"\n    Make sure that required variables are set\n\n    This method must be called before the server starts\n    \"\"\"\n\n    if not self.JWT_SECRET_KEY:\n        raise FractalConfigurationError(\"JWT_SECRET_KEY cannot be None\")\n\n    if not self.FRACTAL_TASKS_DIR:\n        raise FractalConfigurationError(\"FRACTAL_TASKS_DIR cannot be None\")\n\n    self.check_db()\n    self.check_runner()\n</code></pre>"},{"location":"configuration/#fractal_server.config.Settings.check_db","title":"<code>check_db()</code>","text":"<p>Checks that db environment variables are properly set.</p> Source code in <code>fractal_server/config.py</code> <pre><code>def check_db(self) -&gt; None:\n    \"\"\"\n    Checks that db environment variables are properly set.\n    \"\"\"\n    if self.DB_ENGINE == \"postgres\":\n        if not self.POSTGRES_DB:\n            raise FractalConfigurationError(\n                \"POSTGRES_DB cannot be None when DB_ENGINE=postgres.\"\n            )\n        try:\n            import psycopg2  # noqa: F401\n            import asyncpg  # noqa: F401\n        except ModuleNotFoundError:\n            raise FractalConfigurationError(\n                \"DB engine is `postgres` but `psycopg2` or `asyncpg` \"\n                \"are not available\"\n            )\n    else:\n        if not self.SQLITE_PATH:\n            raise FractalConfigurationError(\n                \"SQLITE_PATH cannot be None when DB_ENGINE=sqlite.\"\n            )\n</code></pre>"},{"location":"configuration/#fractal_server.config.Settings.collect_oauth_clients","title":"<code>collect_oauth_clients(values)</code>","text":"<p>Automatic collection of OAuth Clients</p> <p>This method collects the environment variables relative to a single OAuth client and saves them within the <code>Settings</code> object in the form of an <code>OAuthClientConfig</code> instance.</p> <p>Fractal can support an arbitrary number of OAuth providers, which are automatically detected by parsing the environment variable names. In particular, to set the provider <code>FOO</code>, one must specify the variables</p> <pre><code>OAUTH_FOO_CLIENT_ID\nOAUTH_FOO_CLIENT_SECRET\n...\n</code></pre> <p>etc (cf. OAuthClientConfig).</p> Source code in <code>fractal_server/config.py</code> <pre><code>@root_validator(pre=True)\ndef collect_oauth_clients(cls, values):\n    \"\"\"\n    Automatic collection of OAuth Clients\n\n    This method collects the environment variables relative to a single\n    OAuth client and saves them within the `Settings` object in the form\n    of an `OAuthClientConfig` instance.\n\n    Fractal can support an arbitrary number of OAuth providers, which are\n    automatically detected by parsing the environment variable names. In\n    particular, to set the provider `FOO`, one must specify the variables\n\n        OAUTH_FOO_CLIENT_ID\n        OAUTH_FOO_CLIENT_SECRET\n        ...\n\n    etc (cf. OAuthClientConfig).\n    \"\"\"\n    oauth_env_variable_keys = [\n        key for key in environ.keys() if key.startswith(\"OAUTH_\")\n    ]\n    clients_available = {\n        var.split(\"_\")[1] for var in oauth_env_variable_keys\n    }\n\n    values[\"OAUTH_CLIENTS_CONFIG\"] = []\n    for client in clients_available:\n        prefix = f\"OAUTH_{client}\"\n        oauth_client_config = OAuthClientConfig(\n            CLIENT_NAME=client,\n            CLIENT_ID=getenv(f\"{prefix}_CLIENT_ID\", None),\n            CLIENT_SECRET=getenv(f\"{prefix}_CLIENT_SECRET\", None),\n            OIDC_CONFIGURATION_ENDPOINT=getenv(\n                f\"{prefix}_OIDC_CONFIGURATION_ENDPOINT\", None\n            ),\n            REDIRECT_URL=getenv(f\"{prefix}_REDIRECT_URL\", None),\n        )\n        values[\"OAUTH_CLIENTS_CONFIG\"].append(oauth_client_config)\n    return values\n</code></pre>"},{"location":"configuration/#fractal_server.config.Settings.make_FRACTAL_TASKS_DIR_absolute","title":"<code>make_FRACTAL_TASKS_DIR_absolute(v)</code>","text":"<p>If <code>FRACTAL_TASKS_DIR</code> is a non-absolute path, make it absolute (based on the current working directory).</p> Source code in <code>fractal_server/config.py</code> <pre><code>@validator(\"FRACTAL_TASKS_DIR\", always=True)\ndef make_FRACTAL_TASKS_DIR_absolute(cls, v):\n    \"\"\"\n    If `FRACTAL_TASKS_DIR` is a non-absolute path, make it absolute (based\n    on the current working directory).\n    \"\"\"\n    if v is None:\n        return None\n    FRACTAL_TASKS_DIR_path = Path(v)\n    if not FRACTAL_TASKS_DIR_path.is_absolute():\n        FRACTAL_TASKS_DIR_path = FRACTAL_TASKS_DIR_path.resolve()\n        logging.warning(\n            f'FRACTAL_TASKS_DIR=\"{v}\" is not an absolute path; '\n            f'converting it to \"{str(FRACTAL_TASKS_DIR_path)}\"'\n        )\n    return FRACTAL_TASKS_DIR_path\n</code></pre>"},{"location":"development/","title":"Development","text":"<p>The development of Fractal Server takes place on the fractal-server Github repository.  To ask questions or to inform us of a bug or unexpected behavior, please feel free to open an issue.</p> <p>To contribute code, please fork the repository and submit a pull request.</p>"},{"location":"development/#set-up-the-development-environment","title":"Set up the development environment","text":""},{"location":"development/#install-poetry","title":"Install poetry","text":"<p>Fractal uses poetry to manage the development environment and dependencies, and to streamline the build and release operations; at least version 1.3 is recommended.</p> <p>A simple way to install <code>poetry</code> is <pre><code>pipx install poetry==1.7.1`\n</code></pre> while other options are described here.</p>"},{"location":"development/#clone-repository","title":"Clone repository","text":"<p>You can clone the <code>fractal-server</code> repository via <pre><code>git clone https://github.com/fractal-analytics-platform/fractal-server.git\n</code></pre></p>"},{"location":"development/#install-package","title":"Install package","text":"<p>Running <pre><code>poetry install\n</code></pre> will initialise a Python virtual environment and install Fractal Server and all its dependencies, including development dependencies. Note that to run commands from within this environment you should prepend them with <code>poetry run</code>, as in <code>poetry run fractalctl start</code>.</p> <p>To install Fractal Server with some additional extras, use the <code>-E</code> option, as in <pre><code>poetry install -E postgres\npoetry install --all-extras\n</code></pre></p> <p>It may sometimes be useful to use a different Python interpreter from the one installed in your system. To this end we suggest using pyenv. In the project folder, invoking <pre><code>pyenv local &lt;version&gt;\npoetry env use &lt;version&gt;\n</code></pre> will install Fractal in a development environment using an interpreter pinned at the version provided instead of the system interpreter.</p>"},{"location":"development/#update-database-schema","title":"Update database schema","text":"<p>Whenever the models are modified (either in <code>app/models</code> or in <code>app/schemas</code>), you should update them via a migration. To check whether this is needed, run <pre><code>poetry run alembic check\n</code></pre></p> <p>If needed, the simplest procedure is to use <code>alembic --autogenerate</code> to create an incremental migration script, as in <pre><code>$ export SQLITE_PATH=some-test.db\n$ rm some-test.db\n$ poetry run fractalctl set-db\n$ poetry run alembic revision --autogenerate -m \"Some migration message\"\n\n# UserWarning: SQLite is partially supported but discouraged in production environment.SQLite offers partial support for ForeignKey constraints. As such, consistency of the database cannot be guaranteed.\n# INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n# INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n# INFO  [alembic.autogenerate.compare] Detected added column 'task.x'\n#   Generating /some/path/fractal_server/migrations/versions/155de544c342_.py ...  done\n</code></pre></p>"},{"location":"development/#release","title":"Release","text":"<ol> <li>Checkout to branch <code>main</code>.</li> <li>Check that the current HEAD of the <code>main</code> branch passes all the tests (note: make sure that you are using the poetry-installed local package).</li> <li>Update the <code>CHANGELOG.md</code> file (e.g. remove <code>(unreleased)</code> from the upcoming version).</li> <li>If you have modified the models, then you must also create a new migration script (note: in principle the CI will fail if you forget this step).</li> <li> <p>Use: <pre><code>poetry run bumpver update --[tag-num|minor|patch|set-version] --tag-commit --commit --dry\n</code></pre> to test updating the version bump.</p> </li> <li> <p>If the previous step looks good, remove <code>--dry</code> and re-run to actually bump the version, commit and push the changes.</p> </li> <li>Approve (or have approved) the new version at Publish package to PyPI.</li> <li>After the release: If the release was a stable one (e.g. <code>X.Y.Z</code>, not <code>X.Y.Za1</code> or <code>X.Y.Zrc2</code>), move <code>fractal_server/data_migrations/X_Y_Z.py</code> to <code>fractal_server/data_migrations/old</code>.</li> </ol>"},{"location":"development/#run-tests","title":"Run tests","text":"<p>Unit and integration testing of Fractal Server uses the pytest testing framework.</p> <p>To test the SLURM backend, we use a custom version of a Docker local SLURM cluster. The pytest plugin pytest-docker is then used to spin up the Docker containers for the duration of the tests.</p> <p>Important: this requires docker being installed on the development system, and the current user being in the <code>docker</code> group. A simple check for this requirement is to run a command like <code>docker ps</code>, and verify that it does not raise any permission-related error. Note that also <code>docker-compose</code> must be available, but this package is installed as a dependency of <code>pytest-docker</code> (when it is installed with the extra <code>docker-compose-v1</code>, as in the case of Fractal Server).</p> <p>If you installed the development dependencies, you may run the test suite by invoking <pre><code>poetry run pytest\n</code></pre> from the main directory of the <code>fractal-server</code> repository. It is sometimes useful to specify additional arguments, e.g. <pre><code>poetry run pytest -s -vvv --log-cli-level info --full-trace\n</code></pre></p> <p>Tests are also run as part of GitHub Actions Continuous Integration for the <code>fractal-server</code> repository.</p>"},{"location":"development/#documentation","title":"Documentation","text":"<p>The documentations is built with mkdocs and the Material theme.  Whenever possible, docstrings should be formatted as in the Google Python Style Guide.</p> <p>To build the documentation</p> <ol> <li>Setup a python environment and install the requirements from    <code>docs/doc-requirements.txt</code>.</li> <li>Run <pre><code>poetry run mkdocs serve --config-file mkdocs.yml\n</code></pre> and browse the documentation at <code>http://127.0.0.1:8000</code>.</li> </ol>"},{"location":"install_and_deploy/","title":"Install and deploy","text":""},{"location":"install_and_deploy/#preliminary-requirements","title":"Preliminary requirements","text":"<p>Running Fractal Server assumes that</p> <ol> <li>It has access to a shared filesystem on which it can read and write.</li> <li>It has access to a database (currently supported: <code>postgres</code> (recommended)    and <code>sqlite</code>).</li> <li>It has access to one of the supported computational backends.</li> </ol> <p>These requirements are sufficient to use the local backend, while the following additional requirements are needed to use the SLURM backend:</p> <ol> <li> Fractal Server is installed on a SLURM client node, configured to submit and manage jobs. </li> <li> The user who runs Fractal Server has appropriate `sudo` privileges, e.g. to run `sbatch` for other users. </li> <li> The machine where Fractal Server runs exposes a port (possibly only visible from a private network) for communicating with the Fractal client. </li> </ol>"},{"location":"install_and_deploy/#how-to-install","title":"How to install","text":"<p>\u26a0\ufe0f  The minimum supported Python version for fractal-server is 3.9.</p> <p>Fractal Server is hosted on the PyPI index, and it can be installed with <code>pip</code> via <pre><code>pip install fractal-server\n</code></pre></p> <p>Some additional features must be installed as extras, e.g. via one of the following <pre><code>pip install fractal-server[postgres]\npip install fractal-server[gunicorn]\npip install fractal-server[gunicorn,postgres]\n</code></pre></p> <p>When including the <code>postgres</code> extra, the following additional packages should be available on the system (e.g. through an <code>apt</code>-like package manager): <code>postgresql, postgresql-contrib, libpq-dev, gcc</code>.</p> <p>For details on how to install Fractal Server in a development environment, see the Contribute page.</p>"},{"location":"install_and_deploy/#how-to-deploy","title":"How to deploy","text":""},{"location":"install_and_deploy/#basic-procedure","title":"Basic procedure","text":"<p>The basic procedure for running Fractal Server consists in setting up some configuration variables, setting up the database, and then starting the server.</p>"},{"location":"install_and_deploy/#1-set-up-configuration-variables","title":"1. Set up configuration variables","text":"<p>For this command to work properly, a set of variables need to be specified, either as enviromnent variables or in a file like <code>.fractal_server.env</code>. An example of such file is <pre><code>JWT_SECRET_KEY=XXX\nSQLITE_PATH=/some/path/to/fractal_server.db\nFRACTAL_TASKS_DIR=/some/path/to/the/task/environment/folder\nFRACTAL_RUNNER_WORKING_BASE_DIR=some_folder_name\nFRACTAL_RUNNER_BACKEND=slurm\nFRACTAL_SLURM_CONFIG_FILE=/some/path/to/slurm_config.json\n</code></pre></p> <p>\u26a0\ufe0f  <code>JWT_SECRET_KEY=XXX</code> must be replaced with a more secure string, that should not be disclosed. \u26a0\ufe0f</p> <p>More details (including default values) are available in the Configuration page.</p>"},{"location":"install_and_deploy/#2-set-up-the-database","title":"2. Set up the database","text":"<p>The command <pre><code>fractalctl set-db\n</code></pre> initalizes the database, according to the configuration variables. When using SQLite, for instance, the database file is created at the <code>SQLITE_PATH</code> path (notice: the parent folder must already exist).</p>"},{"location":"install_and_deploy/#3-start-the-server","title":"3. Start the server","text":"<p>In the environment where Fractal Server is installed, you can run it via The command <pre><code>fractalctl start\n</code></pre> starts the server (with uvicorn) and binds it to the 8000 port on <code>localhost</code>.  You can add more options (e.g. to specify a different port) as in <pre><code>usage: fractalctl start [-h] [--host HOST] [-p PORT] [--reload]\n\nStart the server (with uvicorn)\n\noptions:\n  -h, --help            show this help message and exit\n  --host HOST           bind socket to this host (default: 127.0.0.1)\n  -p PORT, --port PORT  bind socket to this port (default: 8000)\n  --reload              enable auto-reload\n</code></pre></p> <p>Now the server is up, and depending on the intended use case you may have to create/edit some users - see the Users page.</p> <p>Notice that you could also use more explicit startup commands, see below for an example based on Gunicorn.</p>"},{"location":"install_and_deploy/#ports","title":"Ports","text":"<p>You can get details on the open ports e.g. via <code>ss -tulwn</code> or <code>ss -tulp</code>. An entry like <pre><code>Netid                   State                     Recv-Q                    Send-Q                                         Local Address:Port                                               Peer Address:Port\ntcp                     LISTEN                    0                         128                                                  0.0.0.0:8010                                                    0.0.0.0:*\n</code></pre> shows that port 8010 is open for all the current virtual network (this means for instance that you can use the client from the login node of a SLURM cluster, from any computing node, or from any machine in the local network or with an active VPN).</p>"},{"location":"install_and_deploy/#serving-fractal-server-via-gunicorn","title":"Serving Fractal Server via Gunicorn","text":"<p>Fractal Server is served through uvicorn by default, when it is run via the <code>fractalctl start</code> command, but different servers can be used. If Fractal Server is installed with the <code>gunicorn</code> extra, for instance, you can use a command like <pre><code>gunicorn fractal_server.main:app --workers 2 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8010 --access-logfile logs/fractal-server.out --error-logfile logs/fractal-server.err\n</code></pre></p>"},{"location":"install_and_deploy/#postgres-setup","title":"Postgres setup","text":"<p>See preliminary notes at https://github.com/fractal-analytics-platform/fractal-server/issues/388#issuecomment-1366713291.</p>"},{"location":"install_and_deploy/#fractal-server-as-a-daemonservice","title":"Fractal Server as a daemon/service","text":"<p>See preliminary notes at https://github.com/fractal-analytics-platform/fractal-server/issues/388#issuecomment-1366719115.</p>"},{"location":"openapi/","title":"Web API","text":""},{"location":"internals/","title":"Fractal Server internal components","text":"<p>On top of exposing a web API to clients, <code>fractal-server</code> includes several internal subsystems:</p> <ul> <li>Database interface, supporting SQLite and PostgreSQL;</li> <li>Basic logging;</li> <li>Automated task collection for Fractal-compatible task packages;</li> <li>User management, authentication and authorization;</li> <li>Computational backends to execute workflows;</li> </ul>"},{"location":"internals/database_interface/","title":"Database Interface","text":"<p>Fractal Server allows either SQLite or PostgreSQL to be used as database, and the choice is based on the DB_ENGINE environment variable; the other database-related configuration variables for each case are described below (and in the configuration page).</p> <p>To make database operations verbose, set <code>DB_ECHO</code> equal to <code>true</code>, <code>True</code> or <code>1</code>.</p>"},{"location":"internals/database_interface/#sqlite","title":"SQLite","text":"<p>SQLite is the default value for <code>DB_ENGINE</code>, but you can also set it explicitly:</p> <pre><code>DB_ENGINE=sqlite\n</code></pre> <p>You must always provide the path to the database file, either as absolute or relative path: <pre><code>SQLITE_PATH=/path/to/fractal_server.db\n</code></pre> If the <code>SQLITE_PATH</code> file does not exist, it will be created by <code>fractalctl set-db</code>.</p> <p>\u26a0\ufe0f  Important warnings:</p> <ol> <li>SQLite is supported but discouraged in production. Given its partial    support for ForeignKey constraints, consistency of the database cannot be    guaranteed.</li> <li>The minimum required version for the sqlite system library is v3.37 (which    is for instance the one in the    libsqlite3-0 package    package for Ubuntu 22.04LTS).</li> </ol>"},{"location":"internals/database_interface/#postgresql","title":"PostgreSQL","text":""},{"location":"internals/database_interface/#requirements","title":"Requirements","text":"<p>To use PostgreSQL as a database, Fractal Server must be installed with the <code>postgres</code> extra:</p> <p><pre><code>$ pip install \"fractal-server[postgres]\"\n</code></pre> which will install two additional Python libraries (<code>asyncpg</code> and <code>psycopg2</code>).</p> <p>NOTE: the following system libraries are required:</p> <ul> <li><code>postgresql</code>,</li> <li><code>postgresql-contrib</code>,</li> <li><code>libpq-dev</code>,</li> <li><code>gcc</code>.</li> </ul>"},{"location":"internals/database_interface/#setup","title":"Setup","text":"<p>We assume that a PostgreSQL is active, with some host (this can be e.g. <code>localhost</code> or a UNIX socket like <code>/var/run/postgresql/</code>), a port (we use the default 5432 in the examples below) and a user (e.g. <code>postgres</code> or <code>fractal</code>).</p> <p>\u26a0\ufe0f Notes:</p> <ol> <li>The postgres user must be created from outside <code>fractal-server</code>.</li> <li>A given machine user may or may not require a password (e.g. depending on    whether the machine username matches with the PostgreSQL username, and on    whether connection happens via a UNIX socket). See documentation here:    https://www.postgresql.org/docs/current/auth-pg-hba-conf.html.</li> </ol> <p>Here we create a database called <code>fractal_db</code>, through the <code>createdb</code> command:</p> <pre><code>$ createdb \\\n    --host=localhost \\\n    --port=5432 \\\n    --username=postgres \\\n    --no-password \\\n    --owner=fractal \\\n    fractal_db\n</code></pre> <p>All options of this command (and of the ones below) should be aligned with the configuration of a specific PostgreSQL instance. Within <code>fractal-server</code>, this is done by setting the following configuration variables (before running <code>fractalctl set-db</code> or <code>fractalctl start</code>):</p> <ul> <li> <p>Required:</p> <pre><code>DB_ENGINE=postgres\nPOSTGRES_DB=fractal_db\n</code></pre> </li> <li> <p>Optional:</p> <pre><code>POSTGRES_HOST=localhost             # default: localhost\nPOSTGRES_PORT=5432                  # default: 5432\nPOSTGRES_USER=fractal               # example: fractal\nPOSTGRES_PASSWORD=\n</code></pre> </li> </ul> <p><code>fractal-server</code> will then use the <code>URL.create</code> function from <code>SQLalchemy</code> to generate the appropriate URL to connect to:</p> <p><pre><code>URL.create(\n    drivername=\"postgresql+asyncpg\",\n    username=self.POSTGRES_USER,\n    password=self.POSTGRES_PASSWORD,\n    host=self.POSTGRES_HOST,\n    port=self.POSTGRES_PORT,\n    database=self.POSTGRES_DB,\n)\n</code></pre> Note that <code>POSTGRES_HOST</code> can be either a URL or the path to a UNIX domain socket (e.g. <code>/var/run/postgresql</code>).</p>"},{"location":"internals/database_interface/#backup-and-restore","title":"Backup and restore","text":"<p>To backup and restore data, one can use the utilities <code>pg_dump</code> and <code>psql</code>.</p> <p>It is possible to dump/restore data in various formats (see documentation of <code>pg_dump</code>), but in this example we stick with the default plain-text format.</p> <pre><code>$ pg_dump \\\n    --host=localhost \\\n    --port=5432\\\n    --username=fractal \\\n    --format=plain \\\n    --file=fractal_dump.sql \\\n    fractal_db\n</code></pre> <p>In order to restore a database from a dump, we first create a new empty one (<code>new_fractal_db</code>): <pre><code>$ createdb \\\n    --host=localhost \\\n    --port=5432\\\n    --username=postgres \\\n    --no-password \\\n    --owner=fractal \\\n    new_fractal_db\n</code></pre> and then we populate it using the dumped data:</p> <pre><code>$ psql \\\n    --host=localhost \\\n    --port=5432\\\n    --username=fractal \\\n    --dbname=new_fractal_db &lt; fractal_dump.sql\n</code></pre> <p>One of the multiple ways to compress data is to use <code>gzip</code>, by adapting the commands above as in: <pre><code>$ pg_dump ... | gzip -c fractal_dump.sql.gz\n$ gzip --decompress --keep fractal_dump.sql.gz\n$ createdb ...\n$ psql ... &lt; fractal_dump.sql\n</code></pre></p>"},{"location":"internals/logs/","title":"Logs","text":"<p>Logging in <code>fractal-server</code> is based on the standard <code>logging</code> library, and its logging levels are defined here. For a more detailed view on <code>fractal-server</code> logging, see the logger module documentation.</p> <p>The logger module exposes the functions to set/get/close a logger, and it defines where the records are sent to (e.g. the <code>fractal-server</code> console or a specific file). The logging levels of a logger created with <code>set_logger</code> are defined as follows:</p> <ul> <li>The minimum logging level for logs to appear in the console is set by   <code>FRACTAL_LOGGING_LEVEL</code>;</li> <li>The <code>FileHandler</code> logger handlers are alwasy set at the <code>DEBUG</code> level, that   is, they write all log records.</li> </ul> <p>This means that the <code>FRACTAL_LOGGING_LEVEL</code> offers a quick way to switch to very verbose console logging (setting it e.g. to <code>10</code>, that is, <code>DEBUG</code> level) and to switch back to less verbose logging (e.g. <code>FRACTAL_LOGGING_LEVEL=20</code> or <code>30</code>), without ever modifying the on-file logs. Note that the typical reason for having on-file logs in <code>fractal-server</code> is to log information about background tasks, that are not executed as part of an API endpoint.</p>"},{"location":"internals/logs/#example-use-cases","title":"Example use cases","text":"<ol> <li> <p>Module-level logs that should only appear in the <code>fractal-server</code> console <pre><code>from fractal_server.logger import set_logger\n\nmodule_logger = set_logger(__name__)\n\ndef my_function():\n    module_logger.debug(\"This is an DEBUG log, from my_function\")\n    module_logger.info(\"This is an INFO log, from my_function\")\n    module_logger.warning(\"This is a WARNING log, from my_function\")\n</code></pre> Note that only logs with level equal or higher to <code>FRACTAL_LOGGING_LEVEL</code> will be shown.</p> </li> <li> <p>Function-level logs that should only appear in the <code>fractal-server</code> console <pre><code>from fractal_server.logger import set_logger\n\ndef my_function():\n    function_logger = set_logger(\"my_function\")\n    function_logger.debug(\"This is an DEBUG log, from my_function\")\n    function_logger.info(\"This is an INFO log, from my_function\")\n    function_logger.warning(\"This is a WARNING log, from my_function\")\n</code></pre> Note that only logs with level equal or higher to <code>FRACTAL_LOGGING_LEVEL</code> will be shown.</p> </li> <li> <p>Custom logs that should appear both in the fractal-server console and in a    log file <pre><code>from fractal_server.logger import set_logger\nfrom fractal_server.logger import close_logger\n\ndef my_function():\n    this_logger = set_logger(\"this_logger\", log_file_path=\"/tmp/this.log\")\n    this_logger.debug(\"This is an DEBUG log, from my_function\")\n    this_logger.info(\"This is an INFO log, from my_function\")\n    this_logger.warning(\"This is a WARNING log, from my_function\")\n    close_logger(this_logger)\n</code></pre> Note that only logs with level equal or higher to <code>FRACTAL_LOGGING_LEVEL</code> will be shown in the console, but all logs will be written to <code>\"/tmp/this.log\"</code>.</p> </li> </ol>"},{"location":"internals/logs/#future-plans","title":"Future plans","text":"<p>The current page concerns the logs that are emitted from <code>fractal-sever</code>, but not the ones coming from other sources (e.g. <code>fastapi</code> or <code>uvicorn/gunicorn</code>). In a future refactor we may address this point, with the twofold goal of</p> <ol> <li>Integrating different log sources, so that they can be shown in a    homogeneous way (e.g. all with same format);</li> <li>Redirecting all console logs (from different sources) to a rotating file    (e.g. via a RotatingFileHandler).</li> </ol>"},{"location":"internals/task_collection/","title":"Task Collection","text":"<p>In-progress (for the moment, refer to the tasks module).</p>"},{"location":"internals/users/","title":"Fractal Users","text":"<p>Fractal Server's user model and authentication/authorization systems are powered by the FastAPI Users library, and most of the components described below can be identified in the corresponding overview.</p>"},{"location":"internals/users/#user-model","title":"User Model","text":"<p>A Fractal user corresponds to an instance of the <code>UserOAuth</code> class, with the following attributes:</p> Attribute Type Nullable Default id integer incremental email email - hashed_password string - is_active bool true is_superuser bool false is_verified bool false \u00a0slurm_user\u00a0 string * null \u00a0username\u00a0 string * null \u00a0cache_dir\u00a0 string * null <p>The colored attributes are specific for Fractal users, while the other attributes are provided by FastAPI Users.</p> <p>In the startup phase, <code>fractal-server</code> always creates a default user, who also has the superuser privileges that are necessary for managing other users. The credentials for this user are defined via the environment variables <code>FRACTAL_ADMIN_DEFAULT_EMAIL</code> (default: <code>admin@fractal.xy</code>) and <code>FRACTAL_ADMIN_DEFAULT_PASSWORD</code> (default: <code>1234</code>).</p> <p>\u26a0\ufe0f You should always modify the password of the default user after it's created; this can be done with API calls to the <code>PATCH /auth/users/{id}</code> endpoint of the <code>fractal-server</code> API, e.g. through the <code>curl</code> command or the Fractal command-line client. When the API instance is exposed to multiple users, skipping the default-user password update leads to a severe vulnerability! </p> <p>The most common use cases for <code>fractal-server</code> are:</p> <ol> <li>The server is used by a single user (e.g. on their own machine, with the local backend); in this case you may simply customize and use the default user.</li> <li>The server has multiple users; in this case the admin may use the default user (or another user with superuser privileges) to create additional users (with no superuser privileges). For <code>fractal-server</code> to execute jobs on a SLURM cluster (through the corresponding SLURM backend), each Fractal must be associated to a cluster user via the <code>slurm_user</code> attribute (see here for more details about SLURM users).</li> </ol> <p>More details about user management are provided in the User Management section below.</p>"},{"location":"internals/users/#authentication","title":"Authentication","text":""},{"location":"internals/users/#login","title":"Login","text":"<p>An authentication backend is composed of two parts:</p> <ul> <li>the transport, that manages how the token will be carried over the request,</li> <li>the strategy, which manages how the token is generated and secured.</li> </ul> <p>Fractal Server provides two authentication backends (Bearer and Cookie), both based the JWT strategy. Each backend produces both <code>/auth/login</code> and <code>/auth/logout</code> routes.</p> <p>FastAPI Users provides the <code>logout</code> endpoint by default, but this is not relevant in <code>fractal-server</code> since we do not store tokens in the database.</p>"},{"location":"internals/users/#bearer","title":"Bearer","text":"<p>The Bearer transport backend provides login at <code>/auth/token/login</code> <pre><code>$ curl \\\n    -X POST \\\n    -H \"Content-Type: application/x-www-form-urlencoded\" \\\n    -d \"username=admin@fractal.xy&amp;password=1234\" \\\n    http://127.0.0.1:8000/auth/token/login/\n\n{\n    \"access_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxIiwiYXVkIjpbImZyYWN0YWwiXSwiZXhwIjoxNjkzNTc1MzM1fQ.UmkhBKxgBM2mxXlrTlt5HXqtDDOe_mMYiMkKUS5jbXU\",\n    \"token_type\":\"bearer\"\n}\n</code></pre></p>"},{"location":"internals/users/#cookie","title":"Cookie","text":"<p>The Cookie transport backend provides login at <code>/auth/login</code></p> <pre><code>$ curl \\\n    -X POST \\\n    -H \"Content-Type: application/x-www-form-urlencoded\" \\\n    -d \"username=admin@fractal.xy&amp;password=1234\" \\\n    --cookie-jar - \\\n    http://127.0.0.1:8000/auth/login/\n\n\n#HttpOnly_127.0.0.1 FALSE   /   TRUE    0   fastapiusersauth    eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxIiwiYXVkIjpbImZyYWN0YWwiXSwiZXhwIjoxNjkzNjQ4MDI5fQ.UKRdbVjwys4grQrhpGyxcxcVbNSNJ29RQiFubpGYYUk\n</code></pre>"},{"location":"internals/users/#authenticated-calls","title":"Authenticated calls","text":"<p>Once you have the token, you can use it to identify yourself by sending it along in the header of an API request. Here is an example with an API request to <code>/auth/current-user/</code>: <pre><code>$ curl \\\n    -X GET \\\n    -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIyIiwiYXVkIjpbImZyYWN0YWwiXSwiZXhwIjoxNjkzOTI2MTM4fQ.MqWhW0xRgCV9ZgZr1HcdynrIJ7z46IBzO7pyfTUaTU8\" \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n    \"id\":1,\n    \"email\":\"admin@fractal.xy\",\n    \"is_active\":true,\n    \"is_superuser\":true,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":null,\n    \"username\":\"admin\"\n}\n</code></pre></p>"},{"location":"internals/users/#oauth2","title":"OAuth2","text":"<p>Fractal Server also allows a different authentication procedure, not based on the knowledge of a user's password but on external <code>OAuth2</code> authentication clients.</p> <p>Through the <code>httpx-oauth</code> library, we currently support <code>OpenID Connect</code> (aka <code>OIDC</code>), <code>GitHub</code> and <code>Google</code> (and many more clients can be readily included).</p>"},{"location":"internals/users/#configuration","title":"Configuration","text":"<p>To use a certain <code>OAuth2</code> client, you must first register the <code>fractal-server</code> application (see instructions for GitHub and Google). During app registration, you should provide two endpoints:</p> <ul> <li>the <code>Homepage URL</code> (e.g. <code>http://127.0.0.1:8000/</code>),</li> <li>the <code>Authorization callback URL</code> (e.g. <code>http://127.0.0.1:8000/auth/github/callback/</code>, where <code>github</code> could be any client name).</li> </ul> <p>and at the end of this procedure, you will kwnow the Client ID and Client Secret for the app.</p> <p>You can have just one <code>GitHub</code> client and one <code>Google</code> client, but as many <code>OIDC</code> client as you want, as long as you call them with different names.</p> <p>To add an <code>OAuth2</code> client, the following environment variables must be added to <code>fractal-server</code> configuration:</p> OIDC (single client)OIDC (multiple clients)GitHubGoogle <pre><code>OAUTH_MYCLIENT_CLIENT_ID=...\nOAUTH_MYCLIENT_CLIENT_SECRET=...\nOAUTH_MYCLIENT_OIDC_CONFIGURATION_ENDPOINT=https://client.com/.well-known/openid-configuration\nOAUTH_MYCLIENT_REDIRECT_URL=...   # e.g. https://fractal-web.example.org/auth/login/oauth2\n</code></pre> <pre><code>OAUTH_MYCLIENT1_CLIENT_ID=...\nOAUTH_MYCLIENT1_CLIENT_SECRET=...\nOAUTH_MYCLIENT1_OIDC_CONFIGURATION_ENDPOINT=https://client1.com/.well-known/openid-configuration\nOAUTH_MYCLIENT1_REDIRECT_URL=...   # e.g. https://fractal-web.1.example.org/auth/login/oauth2\n\nOAUTH_MYCLIENT2_CLIENT_ID=...\nOAUTH_MYCLIENT2_CLIENT_SECRET=...\nOAUTH_MYCLIENT2_OIDC_CONFIGURATION_ENDPOINT=https://client2.com/.well-known/openid-configuration\nOAUTH_MYCLIENT2_REDIRECT_URL=...   # e.g. https://fractal-web.2.example.org/auth/login/oauth2\n</code></pre> <pre><code>OAUTH_GITHUB_CLIENT_ID=...\nOAUTH_GITHUB_CLIENT_SECRET=...\nOAUTH_GITHUB_REDIRECT_URL=...   # e.g. https://fractal-web.example.org/auth/login/oauth2\n</code></pre> <pre><code>OAUTH_GOOGLE_CLIENT_ID=...\nOAUTH_GOOGLE_CLIENT_SECRET=...\nOAUTH_GOOGLE_REDIRECT_URL=...   # e.g. https://fractal-web.example.org/auth/login/oauth2\n</code></pre> <p>When <code>fractal-server</code> starts, two new routes will be generated for each client:</p> <ul> <li><code>/auth/client-name/authorize</code> ,</li> <li><code>/auth/client-name/callback</code> (the <code>Authorization callback URL</code> of the client).</li> </ul> <p>For <code>GitHub</code> and <code>Google</code> clients the <code>client-name</code> is <code>github</code> or <code>google</code>, while for <code>OIDC</code> clients it comes from the environment variables (e.g. for <code>OAUTH_MYCLIENT_CLIENT_ID</code> the <code>client-name</code> is <code>MYCLIENT</code>).</p> <p>Note that the <code>OAUTH_*_REDIRECT_URL</code> environment variable is optional. It is not relevant for the examples described in this page, since they are all in the command-line interface. However, it is required when OAuth authentication is performed starting from a browser (e.g. through the <code>fractal-web</code> client), since the callback URL should be opened in the browser itself.</p>"},{"location":"internals/users/#authorization-code-flow","title":"Authorization Code Flow","text":"<p>Authentication via OAuth2 client is based on the Authorizion Code Flow, as described in this diagram</p> <p> </p> <p>(adapted from https://auth0.com/docs/get-started/authentication-and-authorization-flow/authorization-code-flow, \u00a9 2023 Okta, Inc.)   </p> <p>We can now review how <code>fractal-server</code> handles these steps:</p> <ul> <li> <p>Steps 1 \u2192 4</p> <ul> <li>The starting point is <code>/auth/client-name/authorize</code>;</li> <li>Here an <code>authorization_url</code> is generated and provided to the user;</li> <li>This URL will redirect the user to the Authorization Server (which is e.g. GitHub or Google, and not related to <code>fractal-server</code>), together with a <code>state</code> code for increased security;</li> <li>The user must authenticate and grant <code>fractal-server</code> the permissions it requires.</li> </ul> </li> <li> <p>Steps 5 \u2192 8</p> <ul> <li>The flow comes back to <code>fractal-server</code> at <code>/auth/client-name/callback</code>, together with the Authorization Code.</li> <li>A FastAPI dependency of the callback endpoint, <code>oauth2_authorize_callback</code>, takes care of exchanging this code for the Access Token.</li> </ul> </li> <li> <p>Steps 9 \u2192 10</p> <ul> <li>The callback endpoint uses the Access Token to obtain the user's email address and an account identifier from the Resource Server (which, depending on the client, may coincide with the Authorization Server).</li> </ul> </li> </ul> <p>After that, the callback endpoint performs some extra operations, which are not stricly part of the <code>OAuth2</code> protocol:</p> <ul> <li>It checks that <code>state</code> is still valid;</li> <li>If a user with the given email address doesn't already exist, it creates one with a random password;</li> <li>If the user has never authenticated with this <code>OAuth2</code> client before, it adds in the database a new entry to the <code>oauthaccount</code> table, properly linked to the <code>user_oauth</code> table`; at subsequent logins that entry will just be updated;</li> <li>It prepares a JWT token for the user and serves it in the Response Cookie.</li> </ul>"},{"location":"internals/users/#full-example","title":"Full example","text":"<p>A given <code>fractal-server</code> instance is registered as a GitHub App, and <code>fractal-server</code> is configured accordingly. A new user comes in, who wants to sign up using her GitHub account (associated to <code>person@university.edu</code>).</p> <p>First, she makes a call to <code>/auth/github/authorize</code>: <pre><code>$ curl \\\n    -X GET \\\n    http://127.0.0.1:8000/auth/github/authorize/\n\n{\n    \"authorization_url\":\"https://github.com/login/oauth/authorize/?\n        response_type=code&amp;\n        client_id=...&amp;\n        redirect_uri=...&amp;\n        state=...&amp;\n        scope=user+user%3Aemail\"\n}\n</code></pre></p> <p>Now the <code>authorization_url</code> must be visited using a browser. After logging in to GitHub, she is asked to grant the app the permissions it requires.</p> <p>After that, she is redirected back to <code>fractal-server</code> at <code>/auth/github/callback</code>, together with two query parameters: <pre><code>http://127.0.0.1:8000/auth/github/callback/?\n    code=...&amp;\n    state=...\n</code></pre></p> <p>The callback function does not return anything, but the response cookie contains a JWT token <pre><code>\"fastapiusersauth\": {\n    \"httpOnly\": true,\n    \"path\": \"/\",\n    \"samesite\": \"None\",\n    \"secure\": true,\n    \"value\": \"ey...\"     &lt;----- This is the JWT token\n}\n</code></pre></p> <p>The user can now make authenticated calls using this token, as in <pre><code>curl \\\n    -X GET \\\n    -H \"Authorization: Bearer ey...\" \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n    \"id\":3,\n    \"email\":\"person@university.edu\",\n    \"is_active\":true,\n    \"is_superuser\":false,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":null,\n    \"username\":null\n}\n</code></pre></p>"},{"location":"internals/users/#authorization","title":"Authorization","text":"<p>On top of being authenticated, a user must be authorized in order to perform specific actions in <code>fratal-server</code>:</p> <ol> <li>Some endpoints require the user to have a specific attribute (e.g. being <code>active</code> or being <code>superuser</code>);</li> <li>Access control is in-place for some database resources, and encode via database relationships with the User table (e.g. for `Project``);</li> <li>Additional business logic to regulate access may be defined within specific endpoints (e.g. for patching or removing a Task).</li> </ol> <p>The three cases are described more in detail below.</p>"},{"location":"internals/users/#user-attributes","title":"User attributes","text":"<p>Some endpoints require the user to have a specific attribute. This is implemented through a FastAPI dependencies, e.g. using fastapi_users.current_user: <pre><code>current_active_user = fastapi_users.current_user(active=True)\n\n# fake endpoint\n@router.get(\"/am/i/active/\")\nasync def am_i_active(\n    user: UserOAuth = Depends(current_active_user)\n):\n    return {f\"User {user.id}\":  \"you are active\"}\n</code></pre></p> <p>Being an active user (i.e. <code>user.is_active==True</code>) is required by</p> <ul> <li>all <code>/api/v1/...</code> endpoints</li> <li>all <code>/auth/users/...</code>,</li> <li>POST <code>/auth/register/</code>,</li> <li>GET <code>/auth/userlist/</code>,</li> <li>GET <code>/auth/current-user/</code>.</li> </ul> <p>Being a superuser (i.e. <code>user.is_superuser==True</code>) is required by</p> <ul> <li>all <code>/auth/users/...</code>,</li> <li>POST <code>/auth/register/</code>,</li> <li>GET <code>/auth/userlist/</code>.</li> </ul> <p>and it also gives full access (without further checks) to</p> <ul> <li>PATCH <code>/api/v1/task/{task_id}/</code></li> <li>DELETE <code>/api/v1/task/{task_id}/</code></li> </ul> <p>No endpoint currently requires the user to be verified (i.e. having <code>user.is_verified==True</code>).</p>"},{"location":"internals/users/#database-relationships","title":"Database relationships","text":"<p>The following resources in the <code>fractal-server</code> database are always related to a single <code>Project</code> (via their foreign key <code>project_id</code>):</p> <ul> <li><code>Dataset</code>,</li> <li><code>Workflow</code>,</li> <li><code>WorkflowTask</code> (through <code>Workflow</code>).</li> <li><code>ApplyWorkflow</code> (i.e. a workflow-execution job),</li> </ul> <p>Each endpoint that operates on one of these resources (or directly on a <code>Project</code>) requires the user to be in the <code>Project.user_list</code>.</p> <p>The <code>fractal-server</code> database structure is general, and the user/project relationships is a many-to-many one. However the API does not currently expose a feature to easily associate multiple users to the same project.</p>"},{"location":"internals/users/#endpoint-logic","title":"Endpoint logic","text":"<p>The User Model includes additional attributes <code>username</code> and <code>slurm_user</code>, which are optional and default to <code>None</code>. Apart from <code>slurm_user</code> being needed for User Impersonation in SLURM, these two attributes are also used for additional access control to <code>Task</code> resources.</p> <p>\u26a0\ufe0f This is an experimental feature, which will likely evolve in the future (possibly towards the implementation of user groups/roles).</p> <p>When a <code>Task</code> is created, the attribute <code>Task.owner</code> is set equal to <code>username</code> or, if not present, to <code>slurm_user</code> (there must be at least one to create a Task). With a similar logic, we consider a user to be the owner of a Task if <code>username==Task.owner</code> or, if <code>username</code> is <code>None</code>, we check that <code>slurm_user==Task.owner</code>. The following endpoints require a non-superuser user to be the owner of the Task:</p> <ul> <li>PATCH <code>/api/v1/task/{task_id}/</code>,</li> <li>DELETE <code>/api/v1/task/{task_id}/</code>.</li> </ul>"},{"location":"internals/users/#user-management","title":"User Management","text":"<p>The endpoints to manage users can be found under the route <code>/auth/</code>. On top of the <code>login/logout</code> ones (described above), several other endpoints are available, including all the ones exposed by FastAPI Users (see here). Here are more details for the most relevant endpoints.</p>"},{"location":"internals/users/#post-authregister","title":"POST <code>/auth/register</code>","text":"<p>\ud83d\udd10 Restricted to superusers.</p> <p>New users can be registred by a superuser at <code>/auth/register</code>:</p> <pre><code>$ curl \\\n    -X POST \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer ey...\" \\\n    -d '{\"email\": \"user@example.com\", \"password\": \"password\"}' \\\n    http://127.0.0.1:8000/auth/register/\n\n{\n    \"id\":2,\n    \"email\":\"user@example.com\",\n    \"is_active\":true,\n    \"is_superuser\":false,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":null,\n    \"username\":null\n}\n</code></pre> <p>Here we provided <code>email</code> and <code>password</code>, which are the only required fields of <code>UserCreate</code>; we could also provide the following attributes: <code>is_active</code>, <code>is_superuser</code>, <code>is_verified</code>, <code>slurm_user</code>, <code>cache_dir</code>, <code>username</code>.</p>"},{"location":"internals/users/#get-authuserlist","title":"GET <code>/auth/userlist</code>","text":"<p>\ud83d\udd10 Restricted to superusers.</p> <p>The route <code>/auth/userlist</code> returns the list of all registred users:</p> <pre><code>$ curl \\\n    -X GET \\\n    -H \"Authorization: Bearer ey...\" \\\n    http://127.0.0.1:8000/auth/userlist/\n\n[\n    {\n        \"id\":1,\n        \"email\":\"admin@fractal.xy\",\n        \"is_active\":true,\n        \"is_superuser\":true,\n        \"is_verified\":false,\n        \"slurm_user\":null,\n        \"cache_dir\":null,\n        \"username\":\"admin\"\n    },\n    {\n        \"id\":2,\n        \"email\":\"user@example.com\",\n        \"is_active\":true,\n        \"is_superuser\":false,\n        \"is_verified\":false,\n        \"slurm_user\":null,\n        \"cache_dir\":null,\n        \"username\":null\n    }\n]\n</code></pre>"},{"location":"internals/users/#get-authcurrent-user","title":"GET <code>/auth/current-user/</code>","text":"<p>At <code>/auth/current-user/</code>, authenticated users can get informations about themself: <pre><code>curl \\\n    -X GET \\\n    -H \"Authorization: Bearer ey...\" \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n    \"id\":2,\n    \"email\":\"user@example.com\",\n    \"is_active\":true,\n    \"is_superuser\":false,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":null,\n    \"username\":null\n}\n</code></pre></p>"},{"location":"internals/users/#patch-authcurrent-user","title":"PATCH <code>/auth/current-user/</code>","text":"<p>At <code>/auth/current-user/</code>, authenticated users can modify some of their attributes (namely <code>cache_dir</code>, as of fractal-server 1.4.0): <pre><code>curl \\\n    -X PATCH \\\n    -H \"Authorization: Bearer ey...\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"cache_dir\": \"/tmp/somewhere\"}' \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n    \"id\":2,\n    \"email\":\"user@example.com\",\n    \"is_active\":true,\n    \"is_superuser\":false,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":\"/tmp/somewhere\",\n    \"username\":null\n}\n</code></pre></p>"},{"location":"internals/users/#users-endpoints","title":"<code>/users</code> endpoints","text":"<p>\ud83d\udd10 Restricted to superusers.</p> <p>The additional user-management routes exposed by FastAPI Users in <code>/users</code> (see here) are available in <code>fractal-server</code> at  <code>/auth/users/</code>. For the moment all these routes are all restricted to superusers.</p> <p>GET <code>/{id}/</code></p> <p>Return the user with a given <code>id</code>.</p> <p>PATCH <code>/{id}/</code></p> <p>Update the user with a given <code>id</code>.</p> <p>Requires a <code>UserUpdate</code>.</p> <p>DELETE <code>/{id}/</code></p> <p>Delete the user with the given <code>id</code>.</p>"},{"location":"internals/runners/","title":"Runner Backends","text":"<p>Runner backends are responsible for scheduling and applying (running) tasks on your data. Fractal currently supports two backends:</p> <ul> <li>local:     This is the reference backend implementation, which runs tasks locally on     the same host where the server is installed.</li> <li>SLURM:     Run tasks by scheduling them on a SLURM cluster.</li> </ul> <p>Both <code>local</code> and <code>SLURM</code> backends leverage on Python's concurrent.futures interface. As such, writing a new backend based on concurrent executors should require not much more effort than copying the reference <code>local</code> interface and swapping the <code>Executor</code> in the public interface coroutine.</p>"},{"location":"internals/runners/#public-interface","title":"Public interface","text":"<p>The backends need to implement the following common public interface.</p> <p>Local Bakend</p> <p>This backend runs Fractal workflows using <code>FractalThreadPoolExecutor</code> (a custom version of Python ThreadPoolExecutor) to run tasks in several threads. Incidentally, it also represents the reference implementation for a backend.</p>"},{"location":"internals/runners/#fractal_server.app.runner.v2._local.process_workflow","title":"<code>process_workflow(*, workflow, dataset, workflow_dir, workflow_dir_user=None, first_task_index=None, last_task_index=None, logger_name, user_cache_dir=None, slurm_user=None, slurm_account=None, worker_init=None)</code>  <code>async</code>","text":"<p>Run a workflow</p> <p>This function is responsible for running a workflow on some input data, saving the output and taking care of any exception raised during the run.</p> <p>NOTE: This is the <code>local</code> backend's public interface, which also works as a reference implementation for other backends.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>WorkflowV2</code> <p>The workflow to be run</p> required <code>dataset</code> <code>DatasetV2</code> <p>Initial dataset.</p> required <code>workflow_dir</code> <code>Path</code> <p>Working directory for this run.</p> required <code>workflow_dir_user</code> <code>Optional[Path]</code> <p>Working directory for this run, on the user side. This argument is present for compatibility with the standard backend interface, but for the <code>local</code> backend it cannot be different from <code>workflow_dir</code>.</p> <code>None</code> <code>first_task_index</code> <code>Optional[int]</code> <p>Positional index of the first task to execute; if <code>None</code>, start from <code>0</code>.</p> <code>None</code> <code>last_task_index</code> <code>Optional[int]</code> <p>Positional index of the last task to execute; if <code>None</code>, proceed until the last task.</p> <code>None</code> <code>logger_name</code> <code>str</code> <p>Logger name</p> required <code>slurm_user</code> <code>Optional[str]</code> <p>Username to impersonate to run the workflow. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>slurm_account</code> <code>Optional[str]</code> <p>SLURM account to use when running the workflow. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>user_cache_dir</code> <code>Optional[str]</code> <p>Cache directory of the user who will run the workflow. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>worker_init</code> <code>Optional[str]</code> <p>Any additional, usually backend specific, information to be passed to the backend executor. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <p>Raises:</p> Type Description <code>TaskExecutionError</code> <p>wrapper for errors raised during tasks' execution                 (positive exit codes).</p> <code>JobExecutionError</code> <p>wrapper for errors raised by the tasks' executors                (negative exit codes).</p> <p>Returns:</p> Name Type Description <code>output_dataset_metadata</code> <code>dict</code> <p>The updated metadata for the dataset, as returned by the last task of the workflow</p> Source code in <code>fractal_server/app/runner/v2/_local/__init__.py</code> <pre><code>async def process_workflow(\n    *,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    first_task_index: Optional[int] = None,\n    last_task_index: Optional[int] = None,\n    logger_name: str,\n    # Slurm-specific\n    user_cache_dir: Optional[str] = None,\n    slurm_user: Optional[str] = None,\n    slurm_account: Optional[str] = None,\n    worker_init: Optional[str] = None,\n) -&gt; dict:\n    \"\"\"\n    Run a workflow\n\n    This function is responsible for running a workflow on some input data,\n    saving the output and taking care of any exception raised during the run.\n\n    NOTE: This is the `local` backend's public interface, which also works as\n    a reference implementation for other backends.\n\n    Args:\n        workflow:\n            The workflow to be run\n        dataset:\n            Initial dataset.\n        workflow_dir:\n            Working directory for this run.\n        workflow_dir_user:\n            Working directory for this run, on the user side. This argument is\n            present for compatibility with the standard backend interface, but\n            for the `local` backend it cannot be different from `workflow_dir`.\n        first_task_index:\n            Positional index of the first task to execute; if `None`, start\n            from `0`.\n        last_task_index:\n            Positional index of the last task to execute; if `None`, proceed\n            until the last task.\n        logger_name: Logger name\n        slurm_user:\n            Username to impersonate to run the workflow. This argument is\n            present for compatibility with the standard backend interface, but\n            is ignored in the `local` backend.\n        slurm_account:\n            SLURM account to use when running the workflow. This argument is\n            present for compatibility with the standard backend interface, but\n            is ignored in the `local` backend.\n        user_cache_dir:\n            Cache directory of the user who will run the workflow. This\n            argument is present for compatibility with the standard backend\n            interface, but is ignored in the `local` backend.\n        worker_init:\n            Any additional, usually backend specific, information to be passed\n            to the backend executor. This argument is present for compatibility\n            with the standard backend interface, but is ignored in the `local`\n            backend.\n\n    Raises:\n        TaskExecutionError: wrapper for errors raised during tasks' execution\n                            (positive exit codes).\n        JobExecutionError: wrapper for errors raised by the tasks' executors\n                           (negative exit codes).\n\n    Returns:\n        output_dataset_metadata:\n            The updated metadata for the dataset, as returned by the last task\n            of the workflow\n    \"\"\"\n\n    if workflow_dir_user and (workflow_dir_user != workflow_dir):\n        raise NotImplementedError(\n            \"Local backend does not support different directories \"\n            f\"{workflow_dir=} and {workflow_dir_user=}\"\n        )\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    new_dataset_attributes = await async_wrap(_process_workflow)(\n        workflow=workflow,\n        dataset=dataset,\n        logger_name=logger_name,\n        workflow_dir=workflow_dir,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n    return new_dataset_attributes\n</code></pre>"},{"location":"internals/runners/folders/","title":"Working directories","text":""},{"location":"internals/runners/folders/#local-backend-a-single-working-directory-per-job","title":"Local backend: A single working directory per job","text":"<p>When <code>fractal-server</code> executes a task, it has to read/write several execution-related files (note: these are files used internally by <code>fractal-server</code>, and not the actual scientific data being processed). These files are stored in a working directory which is unique for each workflow execution, with a name like <code>workflow_000001_job_000001</code> (and within the parent directory specified in <code>FRACTAL_RUNNER_WORKING_BASE_DIR</code>). All files in this folder are owned by the user running <code>fractal-server</code>, and the permission set of each folder like <code>workflow_000001_job_000001</code> is set to 755.  Note that the folder is created as part of the submit_workflow function.</p>"},{"location":"internals/runners/folders/#slurm-backend-server-side-and-user-side-working-directories","title":"SLURM backend: Server-side and user-side working directories","text":"<p>When using the SLURM backend, parts of the execution-related files are written by the user who submitted the workflow for execution, who is impersonated through the <code>sudo -u</code> command. Such user has no access to the server-side working directory (the one in <code>FRACTAL_RUNNER_WORKING_BASE_DIR</code>), and therefore uses a local (i.e. user-side) working directory. By default, as a parent directory we use the <code>cache_dir</code> attribute of the current <code>User</code>. Since the user running <code>fractal-server</code> may not have access to this user-side directory, several operations are run through <code>sudo -u</code> (e.g. checking whether a file exists, or copying its content).</p>"},{"location":"internals/runners/local/","title":"Local backend","text":"<p>(refer to the local module for more details)</p>"},{"location":"internals/runners/local/#configuration","title":"Configuration","text":"<p>The logic for setting up the local-backend configuration of a given <code>WorkflowTask</code> is implemented in the local._local_config submodule.</p> <p>For the moment, this configuration includes a single (optional) parameter, namely the ingeger variable <code>parallel_tasks_per_job</code>. This parameter is related to tasks that needs to be run in parallel over several inputs: When <code>parallel_tasks_per_job</code> is set, it will represent the maximum number of tasks that the backend will run at the same time.</p> <p>The typical intended use case is that setting <code>parallel_tasks_per_job</code> to a small number (e.g. <code>1</code>) will limit parallelism when executing tasks requiring a large amount of resources (e.g. memory).</p> <p>The different sources for <code>parallel_tasks_per_job</code> are:</p> <ol> <li>If the <code>WorkflowTask.meta</code> field has a <code>parallel_tasks_per_job</code> key, the    corresponding value takes highest priority;</li> <li>Next priority goes to a <code>parallel_tasks_per_job</code> entry in    <code>WorkflowTask.task.meta</code>;</li> <li>Next priority goes to the configuration in <code>FRACTAL_LOCAL_CONFIG_FILE</code>, a    JSON file that may contain a definition of a    <code>LocalBackendConfig</code>    object like <pre><code>{\n  \"parallel_tasks_per_job\": 1\n}\n</code></pre></li> <li>Lowest-priority (that is, the default) is to set    <code>parallel_tasks_per_job=None</code>, which corresponds to not limiting     parallelism at all.</li> </ol>"},{"location":"internals/runners/slurm/","title":"SLURM backend","text":"<p>Refer to the slurm module for more details.</p> <p>NOTE: <code>SlurmConfig</code> objects are created internally in <code>fractal-server</code>, and they are not meant to be initialized by the user; the same holds for <code>SlurmConfig</code> attributes (e.g. <code>mem_per_task_MB</code>) which are not meant to be part of the <code>FRACTAL_SLURM_CONFIG_FILE</code> JSON file (details on the expected file content are defined in <code>SlurmConfigFile</code>).</p>"},{"location":"internals/runners/slurm/#slurm-configuration","title":"SLURM configuration","text":"<p>The logic for setting up the SLURM configuration of a given <code>WorkflowTask</code> is implemented in the slurm._slurm_config submodule.</p> <p>The different sources for SLURM configuration options (like <code>partition</code>, <code>cpus_per_task</code>, ...) are:</p> <ol> <li>All attributes that are explicitly set in the <code>WorkflowTask.meta</code> dictionary    attribute take highest priority;</li> <li>Next priority goes to all attributes that are explicitly set in the    <code>WorkflowTask.task.meta</code> dictionary attribute;</li> <li>Lowest-priority (that is default) values come from the configuration in    <code>FRACTAL_SLURM_CONFIG_FILE</code>. This JSON file follows these specifications.</li> </ol>"},{"location":"internals/runners/slurm/#example","title":"Example","text":"<p>The configuration file could be the one defined here, while a certain <code>WorkflowTask</code> could have <pre><code>workflow_task.meta = {\"cpus_per_task\": 3}\nworkflow_task.task.meta = {\"cpus_per_task\": 2, \"mem\": \"10G\"}\n</code></pre> In this case, the SLURM configuration for this <code>WorkflowTask</code> will correspond to <pre><code>partition=main\ncpus_per_task=3\nmem=10G\n</code></pre></p>"},{"location":"internals/runners/slurm/#exporting-environment-variables","title":"Exporting environment variables","text":"<p>The <code>fractal-server</code> admin may need to set some global variables that need to be included in all SLURM submission scripts; this can be achieved via the <code>extra_lines</code> field in the SLURM configuration file, for instance as in <pre><code>{\n  \"default_slurm_config\": {\n    \"partition\": \"main\",\n    \"extra_lines\": [\n      \"export SOMEVARIABLE=123\",\n      \"export ANOTHERVARIABLE=ABC\"\n    ]\n  }\n}\n</code></pre></p> <p>There exists another use case where the value of a variable depends on the user who runs a certain task. A relevant example is that user A (who will run the task via SLURM) needs to define the cache-directory paths for some libraries they use (and those must be paths where user A can write).  This use case is also supported in the specs of <code>fractal-server</code> SLURM configuration file: If this file includes a block like <pre><code>{\n  ...\n  \"user_local_exports\": {\n    \"LIBRARY_1_CACHE_DIR\": \"somewhere/library_1\",\n    \"LIBRARY_2_FILE\": \"somewhere/else/library_2.json\"\n  }\n}\n</code></pre> then the SLURM submission script will include the lines <pre><code>...\nexport LIBRARY_1_CACHE_DIR=/my/cache/somewhere/library_1\nexport LIBRARY_2_FILE=/my/cache/somewhere/else/library_2.json\n...\n</code></pre> Note that all paths in the values of <code>user_local_exports</code> are interpreted as relative to a base directory which is user-specific (for instance <code>/my/cache/</code>, in the example above), and which is defined in the <code>User.cache_dir</code> attribute. Also note that in this case <code>fractal-server</code> only compiles the configuration options into lines of the SLURM submission script, without performing any check on the validity of the given paths.</p>"},{"location":"internals/runners/slurm/#slurm-batching","title":"SLURM batching","text":"<p>The SLURM backend in <code>fractal-server</code> may combine multiple tasks in the same SLURM job (AKA batching), in order to reduce the total number of SLURM jobs that are submitted. This is especially relevant for clusters with constraints on the number of jobs that a user is allowed to submit over a certain timespan.</p> <p>The logic for handling the batching parameters (that is, how many tasks can be combined in the same SLURM job, and how many of them can run in parallel) is implemented in the slurm._batching submodule, and especially in its <code>heuristics</code> function.</p>"},{"location":"internals/runners/slurm/#user-impersonation","title":"User impersonation","text":"<p>The user who runs <code>fractal-server</code> must have sufficient priviliges for running some commands via <code>sudo -u</code> to impersonate other users of the SLURM cluster without any password. The required commands include <code>sbatch</code>, <code>scancel</code>, <code>cat</code>, <code>ls</code> and <code>mkdir</code>. An example of how to achieve this is to add this block to the <code>sudoers</code> file: <pre><code>Runas_Alias FRACTAL_IMPERSONATE_USERS = fractal, user1, user2, user3\nCmnd_Alias FRACTAL_CMD = /usr/bin/sbatch, /usr/bin/scancel, /usr/bin/cat, /usr/bin/ls, /usr/bin/mkdir\nfractal ALL=(FRACTAL_IMPERSONATE_USERS) NOPASSWD:FRACTAL_CMD\n</code></pre> where <code>fractal</code> is the user running <code>fractal-server</code>, and <code>{user1,user2,user3}</code> are the users who can be impersonated. Note that one could also grant <code>fractal</code> the option of impersonating a whole UNIX group, instead of listing users one by one.</p>"},{"location":"internals/version_upgrades/","title":"Version upgrades","text":"<p>Here are additional details about some version upgrades:</p> <ul> <li>Upgrade from fractal-server 1.2.5 to 1.3.0.</li> </ul>"},{"location":"internals/version_upgrades/upgrade_1_2_5_to_1_3_0/","title":"Upgrade from 1.2.5 to 1.3.0","text":"<p>A large part of endpoints were updated, mostly to move the foreign-key IDs from the Pydantic models used to validate the request payload to the endpoint path. When necessary, some of those same foreign-key IDs are now passed as query parameters (rather than body or path parameters). The rationale behind this refactor is that endpoints paths are now more consistent:</p> <ul> <li>They have a hierarchical structure, when appropriate (e.g. a project may   contain a datasets, workflows and jobs, while a workflow may contain   workflowtasks, ...).</li> <li>They are more consistently defined and more transparently related to CRUD   operations (see e.g.   here).</li> </ul> <p>The list of updated endpoints is below. Note that the IDs that are now part of the path/query parameters are not required any more as part of the body parameters.</p> <pre><code>OLD GET /api/v1/job/download/{job_id}\nNEW GET /api/v1/project/{project_id}/job/{job_id}/download/\n\nOLD GET /api/v1/job/{job_id}\nNEW GET /api/v1/project/{project_id}/job/{job_id}\n\nOLD GET /api/v1/project/{project_id}/jobs/\nNEW GET /api/v1/project/{project_id}/job/\n\nOLD POST /api/v1/project/{project_id}/{dataset_id}\nNEW POST /api/v1/project/{project_id}/dataset/{dataset_id}/resource/\n\nOLD GET /api/v1/project/{project_id}/{dataset_id}\nNEW GET /api/v1/project/{project_id}/dataset/{dataset_id}\n\nOLD GET /api/v1/project/{project_id}/{dataset_id}/resources/\nNEW GET /api/v1/project/{project_id}/dataset/{dataset_id}/resource/\n\nOLD PATCH /api/v1/project/{project_id}/{dataset_id}\nNEW PATCH /api/v1/project/{project_id}/dataset/{dataset_id}\n\nOLD PATCH /api/v1/project/{project_id}/{dataset_id}/{resource_id}\nNEW PATCH /api/v1/project/{project_id}/dataset/{dataset_id}/resource/{resource_id}\n\nOLD DELETE /api/v1/project/{project_id}/{dataset_id}\nNEW DELETE /api/v1/project/{project_id}/dataset/{dataset_id}\n\nOLD DELETE /api/v1/project/{project_id}/{dataset_id}/{resource_id}\nNEW DELETE /api/v1/project/{project_id}/dataset/{dataset_id}/resource/{resource_id}\n\nOLD PATCH /api/v1/workflow/{workflow_id}/edit-task/{workflow_task_id}\nNEW PATCH /api/v1/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}\n\nOLD DELETE /api/v1/workflow/{workflow_id}/rm-task/{workflow_task_id}\nNEW DELETE /api/v1/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}\n\nOLD POST /api/v1/workflow/{workflow_id}/add-task/\nNEW POST /api/v1/project/{project_id}/workflow/{workflow_id}/wftask/\nNEW QUERY PARAMETERS: ['task_id']\n\nOLD POST /api/v1/project/{project_id}/import-workflow/\nNEW POST /api/v1/project/{project_id}/workflow/import/\n\nOLD POST /api/v1/project/apply/\nNEW POST /api/v1/project/{project_id}/workflow/{workflow_id}/apply/\nNEW QUERY PARAMETERS: ['input_dataset_id', 'output_dataset_id']\n\nOLD POST /api/v1/workflow/\nNEW POST /api/v1/project/{project_id}/workflow/\n\nOLD GET /api/v1/project/{project_id}/workflows/\nNEW GET /api/v1/project/{project_id}/workflow/\n\nOLD GET /api/v1/workflow/{workflow_id}\nNEW GET /api/v1/project/{project_id}/workflow/{workflow_id}\n\nOLD GET /api/v1/workflow/{workflow_id}/export/\nNEW GET /api/v1/project/{project_id}/workflow/{workflow_id}/export/\n\nOLD PATCH /api/v1/workflow/{workflow_id}\nNEW PATCH /api/v1/project/{project_id}/workflow/{workflow_id}\n\nOLD DELETE /api/v1/workflow/{workflow_id}\nNEW DELETE /api/v1/project/{project_id}/workflow/{workflow_id}\n\nOLD POST /api/v1/project/{project_id}/\nNEW POST /api/v1/project/{project_id}/dataset/\n\nNEW GET /api/v1/project/{project_id}/job/{job_id}/stop/\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>fractal_server<ul> <li>app<ul> <li>db</li> <li>models<ul> <li>linkuserproject</li> <li>security</li> <li>v1<ul> <li>dataset</li> <li>job</li> <li>project</li> <li>state</li> <li>task</li> <li>workflow</li> </ul> </li> <li>v2<ul> <li>collection_state</li> <li>dataset</li> <li>job</li> <li>project</li> <li>task</li> <li>workflow</li> <li>workflowtask</li> </ul> </li> </ul> </li> <li>routes<ul> <li>admin<ul> <li>v1</li> <li>v2</li> </ul> </li> <li>api<ul> <li>v1<ul> <li>_aux_functions</li> <li>dataset</li> <li>job</li> <li>project</li> <li>task</li> <li>task_collection</li> <li>workflow</li> <li>workflowtask</li> </ul> </li> <li>v2<ul> <li>_aux_functions</li> <li>dataset</li> <li>images</li> <li>job</li> <li>project</li> <li>status</li> <li>submit</li> <li>task</li> <li>task_collection</li> <li>task_legacy</li> <li>workflow</li> <li>workflowtask</li> </ul> </li> </ul> </li> <li>auth</li> <li>aux<ul> <li>_job</li> <li>_runner</li> </ul> </li> </ul> </li> <li>runner<ul> <li>async_wrap</li> <li>components</li> <li>exceptions</li> <li>executors<ul> <li>slurm<ul> <li>_batching</li> <li>_check_jobs_status</li> <li>_executor_wait_thread</li> <li>_slurm_config</li> <li>_subprocess_run_as_user</li> <li>executor</li> <li>remote</li> </ul> </li> </ul> </li> <li>filenames</li> <li>set_start_and_last_task_index</li> <li>task_files</li> <li>v1<ul> <li>_common</li> <li>_local<ul> <li>_local_config</li> <li>_submit_setup</li> <li>executor</li> </ul> </li> <li>_slurm<ul> <li>_submit_setup</li> <li>get_slurm_config</li> </ul> </li> <li>common</li> <li>handle_failed_job</li> </ul> </li> <li>v2<ul> <li>_local<ul> <li>_local_config</li> <li>_submit_setup</li> <li>executor</li> </ul> </li> <li>_slurm<ul> <li>_submit_setup</li> <li>get_slurm_config</li> </ul> </li> <li>deduplicate_list</li> <li>handle_failed_job</li> <li>merge_outputs</li> <li>runner</li> <li>runner_functions</li> <li>runner_functions_low_level</li> <li>task_interface</li> <li>v1_compat</li> </ul> </li> </ul> </li> <li>schemas<ul> <li>_validators</li> <li>state</li> <li>user</li> <li>v1<ul> <li>applyworkflow</li> <li>dataset</li> <li>dumps</li> <li>manifest</li> <li>project</li> <li>task</li> <li>task_collection</li> <li>workflow</li> </ul> </li> <li>v2<ul> <li>dataset</li> <li>dumps</li> <li>job</li> <li>manifest</li> <li>project</li> <li>status</li> <li>task</li> <li>task_collection</li> <li>workflow</li> <li>workflowtask</li> </ul> </li> </ul> </li> <li>security</li> </ul> </li> <li>config</li> <li>images<ul> <li>models</li> <li>tools</li> </ul> </li> <li>logger</li> <li>main</li> <li>syringe</li> <li>tasks<ul> <li>endpoint_operations</li> <li>utils</li> <li>v1<ul> <li>_TaskCollectPip</li> <li>background_operations</li> <li>get_collection_data</li> </ul> </li> <li>v2<ul> <li>_TaskCollectPip</li> <li>background_operations</li> <li>get_collection_data</li> </ul> </li> </ul> </li> <li>urls</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/fractal_server/","title":"fractal_server","text":""},{"location":"reference/fractal_server/config/","title":"config","text":""},{"location":"reference/fractal_server/config/#fractal_server.config.OAuthClientConfig","title":"<code>OAuthClientConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>OAuth Client Config Model</p> <p>This model wraps the variables that define a client against an Identity Provider. As some providers are supported by the libraries used within the server, some attributes are optional.</p> <p>Attributes:</p> Name Type Description <code>CLIENT_NAME</code> <code>str</code> <p>The name of the client</p> <code>CLIENT_ID</code> <code>str</code> <p>ID of client</p> <code>CLIENT_SECRET</code> <code>str</code> <p>Secret to authorise against the identity provider</p> <code>OIDC_CONFIGURATION_ENDPOINT</code> <code>Optional[str]</code> <p>OpenID configuration endpoint, allowing to discover the required endpoints automatically</p> <code>REDIRECT_URL</code> <code>Optional[str]</code> <p>String to be used as <code>redirect_url</code> argument for <code>fastapi_users.get_oauth_router</code>, and then in <code>httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback</code>.</p> Source code in <code>fractal_server/config.py</code> <pre><code>class OAuthClientConfig(BaseModel):\n    \"\"\"\n    OAuth Client Config Model\n\n    This model wraps the variables that define a client against an Identity\n    Provider. As some providers are supported by the libraries used within the\n    server, some attributes are optional.\n\n    Attributes:\n        CLIENT_NAME:\n            The name of the client\n        CLIENT_ID:\n            ID of client\n        CLIENT_SECRET:\n            Secret to authorise against the identity provider\n        OIDC_CONFIGURATION_ENDPOINT:\n            OpenID configuration endpoint,\n            allowing to discover the required endpoints automatically\n        REDIRECT_URL:\n            String to be used as `redirect_url` argument for\n            `fastapi_users.get_oauth_router`, and then in\n            `httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback`.\n    \"\"\"\n\n    CLIENT_NAME: str\n    CLIENT_ID: str\n    CLIENT_SECRET: str\n    OIDC_CONFIGURATION_ENDPOINT: Optional[str]\n    REDIRECT_URL: Optional[str] = None\n\n    @root_validator\n    def check_configuration(cls, values):\n        if values.get(\"CLIENT_NAME\") not in [\"GOOGLE\", \"GITHUB\"]:\n            if not values.get(\"OIDC_CONFIGURATION_ENDPOINT\"):\n                raise FractalConfigurationError(\n                    f\"Missing OAUTH_{values.get('CLIENT_NAME')}\"\n                    \"_OIDC_CONFIGURATION_ENDPOINT\"\n                )\n        return values\n</code></pre>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings","title":"<code>Settings</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Contains all the configuration variables for Fractal Server</p> <p>The attributes of this class are set from the environtment.</p> Source code in <code>fractal_server/config.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"\n    Contains all the configuration variables for Fractal Server\n\n    The attributes of this class are set from the environtment.\n    \"\"\"\n\n    class Config:\n        case_sensitive = True\n\n    PROJECT_NAME: str = \"Fractal Server\"\n    PROJECT_VERSION: str = fractal_server.__VERSION__\n\n    ###########################################################################\n    # AUTH\n    ###########################################################################\n\n    OAUTH_CLIENTS_CONFIG: list[OAuthClientConfig] = Field(default_factory=list)\n\n    # JWT TOKEN\n    JWT_EXPIRE_SECONDS: int = 180\n    \"\"\"\n    JWT token lifetime, in seconds.\n    \"\"\"\n\n    JWT_SECRET_KEY: Optional[str]\n    \"\"\"\n    JWT secret\n\n    \u26a0\ufe0f **IMPORTANT**: set this variable to a secure string, and do not disclose\n    it.\n    \"\"\"\n\n    # COOKIE TOKEN\n    COOKIE_EXPIRE_SECONDS: int = 86400\n    \"\"\"\n    Cookie token lifetime, in seconds.\n    \"\"\"\n\n    @root_validator(pre=True)\n    def collect_oauth_clients(cls, values):\n        \"\"\"\n        Automatic collection of OAuth Clients\n\n        This method collects the environment variables relative to a single\n        OAuth client and saves them within the `Settings` object in the form\n        of an `OAuthClientConfig` instance.\n\n        Fractal can support an arbitrary number of OAuth providers, which are\n        automatically detected by parsing the environment variable names. In\n        particular, to set the provider `FOO`, one must specify the variables\n\n            OAUTH_FOO_CLIENT_ID\n            OAUTH_FOO_CLIENT_SECRET\n            ...\n\n        etc (cf. OAuthClientConfig).\n        \"\"\"\n        oauth_env_variable_keys = [\n            key for key in environ.keys() if key.startswith(\"OAUTH_\")\n        ]\n        clients_available = {\n            var.split(\"_\")[1] for var in oauth_env_variable_keys\n        }\n\n        values[\"OAUTH_CLIENTS_CONFIG\"] = []\n        for client in clients_available:\n            prefix = f\"OAUTH_{client}\"\n            oauth_client_config = OAuthClientConfig(\n                CLIENT_NAME=client,\n                CLIENT_ID=getenv(f\"{prefix}_CLIENT_ID\", None),\n                CLIENT_SECRET=getenv(f\"{prefix}_CLIENT_SECRET\", None),\n                OIDC_CONFIGURATION_ENDPOINT=getenv(\n                    f\"{prefix}_OIDC_CONFIGURATION_ENDPOINT\", None\n                ),\n                REDIRECT_URL=getenv(f\"{prefix}_REDIRECT_URL\", None),\n            )\n            values[\"OAUTH_CLIENTS_CONFIG\"].append(oauth_client_config)\n        return values\n\n    ###########################################################################\n    # DATABASE\n    ###########################################################################\n    DB_ENGINE: Literal[\"sqlite\", \"postgres\"] = \"sqlite\"\n    \"\"\"\n    Select which database engine to use (supported: `sqlite` and `postgres`).\n    \"\"\"\n    DB_ECHO: bool = False\n    \"\"\"\n    If `True`, make database operations verbose.\n    \"\"\"\n    POSTGRES_USER: Optional[str]\n    \"\"\"\n    User to use when connecting to the PostgreSQL database.\n    \"\"\"\n    POSTGRES_PASSWORD: Optional[str]\n    \"\"\"\n    Password to use when connecting to the PostgreSQL database.\n    \"\"\"\n    POSTGRES_HOST: Optional[str] = \"localhost\"\n    \"\"\"\n    URL to the PostgreSQL server or path to a UNIX domain socket.\n    \"\"\"\n    POSTGRES_PORT: Optional[str] = \"5432\"\n    \"\"\"\n    Port number to use when connecting to the PostgreSQL server.\n    \"\"\"\n    POSTGRES_DB: Optional[str]\n    \"\"\"\n    Name of the PostgreSQL database to connect to.\n    \"\"\"\n\n    SQLITE_PATH: Optional[str]\n    \"\"\"\n    File path where the SQLite database is located (or will be located).\n    \"\"\"\n\n    @property\n    def DATABASE_URL(self) -&gt; URL:\n        if self.DB_ENGINE == \"sqlite\":\n            if not self.SQLITE_PATH:\n                raise FractalConfigurationError(\n                    \"SQLITE_PATH path cannot be None\"\n                )\n            sqlite_path = abspath(self.SQLITE_PATH)\n            url = URL.create(\n                drivername=\"sqlite+aiosqlite\",\n                database=sqlite_path,\n            )\n            return url\n        elif \"postgres\":\n            url = URL.create(\n                drivername=\"postgresql+asyncpg\",\n                username=self.POSTGRES_USER,\n                password=self.POSTGRES_PASSWORD,\n                host=self.POSTGRES_HOST,\n                port=self.POSTGRES_PORT,\n                database=self.POSTGRES_DB,\n            )\n            return url\n\n    @property\n    def DATABASE_SYNC_URL(self):\n        if self.DB_ENGINE == \"sqlite\":\n            if not self.SQLITE_PATH:\n                raise FractalConfigurationError(\n                    \"SQLITE_PATH path cannot be None\"\n                )\n            return self.DATABASE_URL.set(drivername=\"sqlite\")\n        elif self.DB_ENGINE == \"postgres\":\n            return self.DATABASE_URL.set(drivername=\"postgresql+psycopg2\")\n\n    ###########################################################################\n    # FRACTAL SPECIFIC\n    ###########################################################################\n\n    FRACTAL_DEFAULT_ADMIN_EMAIL: str = \"admin@fractal.xy\"\n    \"\"\"\n    Admin default email, used upon creation of the first superuser during\n    server startup.\n\n    \u26a0\ufe0f  **IMPORTANT**: After the server startup, you should always edit the\n    default admin credentials.\n    \"\"\"\n\n    FRACTAL_DEFAULT_ADMIN_PASSWORD: str = \"1234\"\n    \"\"\"\n    Admin default password, used upon creation of the first superuser during\n    server startup.\n\n    \u26a0\ufe0f **IMPORTANT**: After the server startup, you should always edit the\n    default admin credentials.\n    \"\"\"\n\n    FRACTAL_DEFAULT_ADMIN_USERNAME: str = \"admin\"\n    \"\"\"\n    Admin default username, used upon creation of the first superuser during\n    server startup.\n\n    \u26a0\ufe0f **IMPORTANT**: After the server startup, you should always edit the\n    default admin credentials.\n    \"\"\"\n\n    FRACTAL_TASKS_DIR: Optional[Path]\n    \"\"\"\n    Directory under which all the tasks will be saved (either an absolute path\n    or a path relative to current working directory).\n    \"\"\"\n\n    @validator(\"FRACTAL_TASKS_DIR\", always=True)\n    def make_FRACTAL_TASKS_DIR_absolute(cls, v):\n        \"\"\"\n        If `FRACTAL_TASKS_DIR` is a non-absolute path, make it absolute (based\n        on the current working directory).\n        \"\"\"\n        if v is None:\n            return None\n        FRACTAL_TASKS_DIR_path = Path(v)\n        if not FRACTAL_TASKS_DIR_path.is_absolute():\n            FRACTAL_TASKS_DIR_path = FRACTAL_TASKS_DIR_path.resolve()\n            logging.warning(\n                f'FRACTAL_TASKS_DIR=\"{v}\" is not an absolute path; '\n                f'converting it to \"{str(FRACTAL_TASKS_DIR_path)}\"'\n            )\n        return FRACTAL_TASKS_DIR_path\n\n    FRACTAL_RUNNER_BACKEND: Literal[\"local\", \"slurm\"] = \"local\"\n    \"\"\"\n    Select which runner backend to use.\n    \"\"\"\n\n    FRACTAL_RUNNER_WORKING_BASE_DIR: Optional[Path]\n    \"\"\"\n    Base directory for running jobs / workflows. All artifacts required to set\n    up, run and tear down jobs are placed in subdirs of this directory.\n    \"\"\"\n\n    FRACTAL_LOGGING_LEVEL: int = logging.INFO\n    \"\"\"\n    Logging-level threshold for logging\n\n    Only logs of with this level (or higher) will appear in the console logs;\n    see details [here](../internals/logs/).\n    \"\"\"\n\n    FRACTAL_LOCAL_CONFIG_FILE: Optional[Path]\n    \"\"\"\n    Path of JSON file with configuration for the local backend.\n    \"\"\"\n\n    FRACTAL_SLURM_CONFIG_FILE: Optional[Path]\n    \"\"\"\n    Path of JSON file with configuration for the SLURM backend.\n    \"\"\"\n\n    FRACTAL_SLURM_WORKER_PYTHON: Optional[str] = None\n    \"\"\"\n    Path to Python interpreter that will run the jobs on the SLURM nodes. If\n    not specified, the same interpreter that runs the server is used.\n    \"\"\"\n\n    FRACTAL_SLURM_POLL_INTERVAL: int = 5\n    \"\"\"\n    Interval to wait (in seconds) before checking whether unfinished job are\n    still running on SLURM (see `SlurmWaitThread` in\n    [`clusterfutures`](https://github.com/sampsyo/clusterfutures/blob/master/cfut/__init__.py)).\n    \"\"\"\n\n    FRACTAL_SLURM_ERROR_HANDLING_INTERVAL: int = 5\n    \"\"\"\n    Interval to wait (in seconds) when the SLURM backend does not find an\n    output pickle file - which could be due to several reasons (e.g. the SLURM\n    job was cancelled or failed, or writing the file is taking long). If the\n    file is still missing after this time interval, this leads to a\n    `JobExecutionError`.\n    \"\"\"\n\n    FRACTAL_API_SUBMIT_RATE_LIMIT: int = 2\n    \"\"\"\n    Interval to wait (in seconds) to be allowed to call again\n    `POST api/v1/{project_id}/workflow/{workflow_id}/apply/`\n    with the same path and query parameters.\n    \"\"\"\n\n    FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE: str = (\n        \"Copy OME-Zarr structure;Convert Metadata Components from 2D to 3D\"\n    )\n    \"\"\"\n    `;`-separated list of names for task that require the `metadata[\"image\"]`\n    attribute in their input-arguments JSON file.\n    \"\"\"\n\n    ###########################################################################\n    # BUSINESS LOGIC\n    ###########################################################################\n    def check_db(self) -&gt; None:\n        \"\"\"\n        Checks that db environment variables are properly set.\n        \"\"\"\n        if self.DB_ENGINE == \"postgres\":\n            if not self.POSTGRES_DB:\n                raise FractalConfigurationError(\n                    \"POSTGRES_DB cannot be None when DB_ENGINE=postgres.\"\n                )\n            try:\n                import psycopg2  # noqa: F401\n                import asyncpg  # noqa: F401\n            except ModuleNotFoundError:\n                raise FractalConfigurationError(\n                    \"DB engine is `postgres` but `psycopg2` or `asyncpg` \"\n                    \"are not available\"\n                )\n        else:\n            if not self.SQLITE_PATH:\n                raise FractalConfigurationError(\n                    \"SQLITE_PATH cannot be None when DB_ENGINE=sqlite.\"\n                )\n\n    def check_runner(self) -&gt; None:\n\n        if not self.FRACTAL_RUNNER_WORKING_BASE_DIR:\n            raise FractalConfigurationError(\n                \"FRACTAL_RUNNER_WORKING_BASE_DIR cannot be None.\"\n            )\n\n        info = f\"FRACTAL_RUNNER_BACKEND={self.FRACTAL_RUNNER_BACKEND}\"\n        if self.FRACTAL_RUNNER_BACKEND == \"slurm\":\n\n            from fractal_server.app.runner.executors.slurm._slurm_config import (  # noqa: E501\n                load_slurm_config_file,\n            )\n\n            if not self.FRACTAL_SLURM_CONFIG_FILE:\n                raise FractalConfigurationError(\n                    f\"Must set FRACTAL_SLURM_CONFIG_FILE when {info}\"\n                )\n            else:\n                if not self.FRACTAL_SLURM_CONFIG_FILE.exists():\n                    raise FractalConfigurationError(\n                        f\"{info} but FRACTAL_SLURM_CONFIG_FILE=\"\n                        f\"{self.FRACTAL_SLURM_CONFIG_FILE} not found.\"\n                    )\n\n                load_slurm_config_file(self.FRACTAL_SLURM_CONFIG_FILE)\n                if not shutil.which(\"sbatch\"):\n                    raise FractalConfigurationError(\n                        f\"{info} but `sbatch` command not found.\"\n                    )\n                if not shutil.which(\"squeue\"):\n                    raise FractalConfigurationError(\n                        f\"{info} but `squeue` command not found.\"\n                    )\n        else:  # i.e. self.FRACTAL_RUNNER_BACKEND == \"local\"\n            if self.FRACTAL_LOCAL_CONFIG_FILE:\n                if not self.FRACTAL_LOCAL_CONFIG_FILE.exists():\n                    raise FractalConfigurationError(\n                        f\"{info} but FRACTAL_LOCAL_CONFIG_FILE=\"\n                        f\"{self.FRACTAL_LOCAL_CONFIG_FILE} not found.\"\n                    )\n\n    def check(self):\n        \"\"\"\n        Make sure that required variables are set\n\n        This method must be called before the server starts\n        \"\"\"\n\n        if not self.JWT_SECRET_KEY:\n            raise FractalConfigurationError(\"JWT_SECRET_KEY cannot be None\")\n\n        if not self.FRACTAL_TASKS_DIR:\n            raise FractalConfigurationError(\"FRACTAL_TASKS_DIR cannot be None\")\n\n        self.check_db()\n        self.check_runner()\n</code></pre>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.COOKIE_EXPIRE_SECONDS","title":"<code>COOKIE_EXPIRE_SECONDS: int = 86400</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Cookie token lifetime, in seconds.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.DB_ECHO","title":"<code>DB_ECHO: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>True</code>, make database operations verbose.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.DB_ENGINE","title":"<code>DB_ENGINE: Literal['sqlite', 'postgres'] = 'sqlite'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Select which database engine to use (supported: <code>sqlite</code> and <code>postgres</code>).</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_API_SUBMIT_RATE_LIMIT","title":"<code>FRACTAL_API_SUBMIT_RATE_LIMIT: int = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Interval to wait (in seconds) to be allowed to call again <code>POST api/v1/{project_id}/workflow/{workflow_id}/apply/</code> with the same path and query parameters.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_DEFAULT_ADMIN_EMAIL","title":"<code>FRACTAL_DEFAULT_ADMIN_EMAIL: str = 'admin@fractal.xy'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Admin default email, used upon creation of the first superuser during server startup.</p> <p>\u26a0\ufe0f  IMPORTANT: After the server startup, you should always edit the default admin credentials.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_DEFAULT_ADMIN_PASSWORD","title":"<code>FRACTAL_DEFAULT_ADMIN_PASSWORD: str = '1234'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Admin default password, used upon creation of the first superuser during server startup.</p> <p>\u26a0\ufe0f IMPORTANT: After the server startup, you should always edit the default admin credentials.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_DEFAULT_ADMIN_USERNAME","title":"<code>FRACTAL_DEFAULT_ADMIN_USERNAME: str = 'admin'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Admin default username, used upon creation of the first superuser during server startup.</p> <p>\u26a0\ufe0f IMPORTANT: After the server startup, you should always edit the default admin credentials.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_LOCAL_CONFIG_FILE","title":"<code>FRACTAL_LOCAL_CONFIG_FILE: Optional[Path]</code>  <code>instance-attribute</code>","text":"<p>Path of JSON file with configuration for the local backend.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_LOGGING_LEVEL","title":"<code>FRACTAL_LOGGING_LEVEL: int = logging.INFO</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Logging-level threshold for logging</p> <p>Only logs of with this level (or higher) will appear in the console logs; see details here.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_RUNNER_BACKEND","title":"<code>FRACTAL_RUNNER_BACKEND: Literal['local', 'slurm'] = 'local'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Select which runner backend to use.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE","title":"<code>FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE: str = 'Copy OME-Zarr structure;Convert Metadata Components from 2D to 3D'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p><code>;</code>-separated list of names for task that require the <code>metadata[\"image\"]</code> attribute in their input-arguments JSON file.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_RUNNER_WORKING_BASE_DIR","title":"<code>FRACTAL_RUNNER_WORKING_BASE_DIR: Optional[Path]</code>  <code>instance-attribute</code>","text":"<p>Base directory for running jobs / workflows. All artifacts required to set up, run and tear down jobs are placed in subdirs of this directory.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_SLURM_CONFIG_FILE","title":"<code>FRACTAL_SLURM_CONFIG_FILE: Optional[Path]</code>  <code>instance-attribute</code>","text":"<p>Path of JSON file with configuration for the SLURM backend.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_SLURM_ERROR_HANDLING_INTERVAL","title":"<code>FRACTAL_SLURM_ERROR_HANDLING_INTERVAL: int = 5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Interval to wait (in seconds) when the SLURM backend does not find an output pickle file - which could be due to several reasons (e.g. the SLURM job was cancelled or failed, or writing the file is taking long). If the file is still missing after this time interval, this leads to a <code>JobExecutionError</code>.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_SLURM_POLL_INTERVAL","title":"<code>FRACTAL_SLURM_POLL_INTERVAL: int = 5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Interval to wait (in seconds) before checking whether unfinished job are still running on SLURM (see <code>SlurmWaitThread</code> in <code>clusterfutures</code>).</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_SLURM_WORKER_PYTHON","title":"<code>FRACTAL_SLURM_WORKER_PYTHON: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to Python interpreter that will run the jobs on the SLURM nodes. If not specified, the same interpreter that runs the server is used.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.FRACTAL_TASKS_DIR","title":"<code>FRACTAL_TASKS_DIR: Optional[Path]</code>  <code>instance-attribute</code>","text":"<p>Directory under which all the tasks will be saved (either an absolute path or a path relative to current working directory).</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.JWT_EXPIRE_SECONDS","title":"<code>JWT_EXPIRE_SECONDS: int = 180</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>JWT token lifetime, in seconds.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.JWT_SECRET_KEY","title":"<code>JWT_SECRET_KEY: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>JWT secret</p> <p>\u26a0\ufe0f IMPORTANT: set this variable to a secure string, and do not disclose it.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.POSTGRES_DB","title":"<code>POSTGRES_DB: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>Name of the PostgreSQL database to connect to.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.POSTGRES_HOST","title":"<code>POSTGRES_HOST: Optional[str] = 'localhost'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>URL to the PostgreSQL server or path to a UNIX domain socket.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.POSTGRES_PASSWORD","title":"<code>POSTGRES_PASSWORD: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>Password to use when connecting to the PostgreSQL database.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.POSTGRES_PORT","title":"<code>POSTGRES_PORT: Optional[str] = '5432'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Port number to use when connecting to the PostgreSQL server.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.POSTGRES_USER","title":"<code>POSTGRES_USER: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>User to use when connecting to the PostgreSQL database.</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.SQLITE_PATH","title":"<code>SQLITE_PATH: Optional[str]</code>  <code>instance-attribute</code>","text":"<p>File path where the SQLite database is located (or will be located).</p>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.check","title":"<code>check()</code>","text":"<p>Make sure that required variables are set</p> <p>This method must be called before the server starts</p> Source code in <code>fractal_server/config.py</code> <pre><code>def check(self):\n    \"\"\"\n    Make sure that required variables are set\n\n    This method must be called before the server starts\n    \"\"\"\n\n    if not self.JWT_SECRET_KEY:\n        raise FractalConfigurationError(\"JWT_SECRET_KEY cannot be None\")\n\n    if not self.FRACTAL_TASKS_DIR:\n        raise FractalConfigurationError(\"FRACTAL_TASKS_DIR cannot be None\")\n\n    self.check_db()\n    self.check_runner()\n</code></pre>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.check_db","title":"<code>check_db()</code>","text":"<p>Checks that db environment variables are properly set.</p> Source code in <code>fractal_server/config.py</code> <pre><code>def check_db(self) -&gt; None:\n    \"\"\"\n    Checks that db environment variables are properly set.\n    \"\"\"\n    if self.DB_ENGINE == \"postgres\":\n        if not self.POSTGRES_DB:\n            raise FractalConfigurationError(\n                \"POSTGRES_DB cannot be None when DB_ENGINE=postgres.\"\n            )\n        try:\n            import psycopg2  # noqa: F401\n            import asyncpg  # noqa: F401\n        except ModuleNotFoundError:\n            raise FractalConfigurationError(\n                \"DB engine is `postgres` but `psycopg2` or `asyncpg` \"\n                \"are not available\"\n            )\n    else:\n        if not self.SQLITE_PATH:\n            raise FractalConfigurationError(\n                \"SQLITE_PATH cannot be None when DB_ENGINE=sqlite.\"\n            )\n</code></pre>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.collect_oauth_clients","title":"<code>collect_oauth_clients(values)</code>","text":"<p>Automatic collection of OAuth Clients</p> <p>This method collects the environment variables relative to a single OAuth client and saves them within the <code>Settings</code> object in the form of an <code>OAuthClientConfig</code> instance.</p> <p>Fractal can support an arbitrary number of OAuth providers, which are automatically detected by parsing the environment variable names. In particular, to set the provider <code>FOO</code>, one must specify the variables</p> <pre><code>OAUTH_FOO_CLIENT_ID\nOAUTH_FOO_CLIENT_SECRET\n...\n</code></pre> <p>etc (cf. OAuthClientConfig).</p> Source code in <code>fractal_server/config.py</code> <pre><code>@root_validator(pre=True)\ndef collect_oauth_clients(cls, values):\n    \"\"\"\n    Automatic collection of OAuth Clients\n\n    This method collects the environment variables relative to a single\n    OAuth client and saves them within the `Settings` object in the form\n    of an `OAuthClientConfig` instance.\n\n    Fractal can support an arbitrary number of OAuth providers, which are\n    automatically detected by parsing the environment variable names. In\n    particular, to set the provider `FOO`, one must specify the variables\n\n        OAUTH_FOO_CLIENT_ID\n        OAUTH_FOO_CLIENT_SECRET\n        ...\n\n    etc (cf. OAuthClientConfig).\n    \"\"\"\n    oauth_env_variable_keys = [\n        key for key in environ.keys() if key.startswith(\"OAUTH_\")\n    ]\n    clients_available = {\n        var.split(\"_\")[1] for var in oauth_env_variable_keys\n    }\n\n    values[\"OAUTH_CLIENTS_CONFIG\"] = []\n    for client in clients_available:\n        prefix = f\"OAUTH_{client}\"\n        oauth_client_config = OAuthClientConfig(\n            CLIENT_NAME=client,\n            CLIENT_ID=getenv(f\"{prefix}_CLIENT_ID\", None),\n            CLIENT_SECRET=getenv(f\"{prefix}_CLIENT_SECRET\", None),\n            OIDC_CONFIGURATION_ENDPOINT=getenv(\n                f\"{prefix}_OIDC_CONFIGURATION_ENDPOINT\", None\n            ),\n            REDIRECT_URL=getenv(f\"{prefix}_REDIRECT_URL\", None),\n        )\n        values[\"OAUTH_CLIENTS_CONFIG\"].append(oauth_client_config)\n    return values\n</code></pre>"},{"location":"reference/fractal_server/config/#fractal_server.config.Settings.make_FRACTAL_TASKS_DIR_absolute","title":"<code>make_FRACTAL_TASKS_DIR_absolute(v)</code>","text":"<p>If <code>FRACTAL_TASKS_DIR</code> is a non-absolute path, make it absolute (based on the current working directory).</p> Source code in <code>fractal_server/config.py</code> <pre><code>@validator(\"FRACTAL_TASKS_DIR\", always=True)\ndef make_FRACTAL_TASKS_DIR_absolute(cls, v):\n    \"\"\"\n    If `FRACTAL_TASKS_DIR` is a non-absolute path, make it absolute (based\n    on the current working directory).\n    \"\"\"\n    if v is None:\n        return None\n    FRACTAL_TASKS_DIR_path = Path(v)\n    if not FRACTAL_TASKS_DIR_path.is_absolute():\n        FRACTAL_TASKS_DIR_path = FRACTAL_TASKS_DIR_path.resolve()\n        logging.warning(\n            f'FRACTAL_TASKS_DIR=\"{v}\" is not an absolute path; '\n            f'converting it to \"{str(FRACTAL_TASKS_DIR_path)}\"'\n        )\n    return FRACTAL_TASKS_DIR_path\n</code></pre>"},{"location":"reference/fractal_server/logger/","title":"logger","text":"<p>This module provides logging utilities</p>"},{"location":"reference/fractal_server/logger/#fractal_server.logger.close_logger","title":"<code>close_logger(logger)</code>","text":"<p>Close all handlers associated to a <code>logging.Logger</code> object</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>The actual logger</p> required Source code in <code>fractal_server/logger.py</code> <pre><code>def close_logger(logger: logging.Logger) -&gt; None:\n    \"\"\"\n    Close all handlers associated to a `logging.Logger` object\n\n    Arguments:\n        logger: The actual logger\n    \"\"\"\n    for handle in logger.handlers:\n        handle.close()\n</code></pre>"},{"location":"reference/fractal_server/logger/#fractal_server.logger.get_logger","title":"<code>get_logger(logger_name=None)</code>","text":"<p>Wrap the <code>logging.getLogger</code> function.</p> <p>The typical use case for this function is to retrieve a logger that was already defined, as in the following example: <pre><code>def function1(logger_name):\n    logger = get_logger(logger_name)\n    logger.info(\"Info from function1\")\n\ndef funtion2():\n    logger_name = \"my_logger\"\n    logger = set_logger(logger_name)\n    logger.info(\"Info from function2\")\n    function1(logger_name)\n    close_logger(logger)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>logger_name</code> <code>Optional[str]</code> <p>Name of logger</p> <code>None</code> <p>Returns:     Logger with name <code>logger_name</code></p> Source code in <code>fractal_server/logger.py</code> <pre><code>def get_logger(logger_name: Optional[str] = None) -&gt; logging.Logger:\n    \"\"\"\n    Wrap the\n    [`logging.getLogger`](https://docs.python.org/3/library/logging.html#logging.getLogger)\n    function.\n\n    The typical use case for this function is to retrieve a logger that was\n    already defined, as in the following example:\n    ```python\n    def function1(logger_name):\n        logger = get_logger(logger_name)\n        logger.info(\"Info from function1\")\n\n    def funtion2():\n        logger_name = \"my_logger\"\n        logger = set_logger(logger_name)\n        logger.info(\"Info from function2\")\n        function1(logger_name)\n        close_logger(logger)\n    ```\n\n    Arguments:\n        logger_name: Name of logger\n    Returns:\n        Logger with name `logger_name`\n    \"\"\"\n    return logging.getLogger(logger_name)\n</code></pre>"},{"location":"reference/fractal_server/logger/#fractal_server.logger.set_logger","title":"<code>set_logger(logger_name, *, log_file_path=None)</code>","text":"<p>Set up a <code>fractal-server</code> logger</p> <p>The logger (a <code>logging.Logger</code> object) will have the following properties:</p> <ul> <li>The attribute <code>Logger.propagate</code> set to <code>False</code>;</li> <li>One and only one <code>logging.StreamHandler</code> handler, with severity level set to <code>FRACTAL_LOGGING_LEVEL</code> and formatter set as in the <code>logger.LOG_FORMAT</code> variable from the current module;</li> <li>One or many <code>logging.FileHandler</code> handlers, including one pointint to <code>log_file_path</code> (if set); all these handlers have severity level set to <code>logging.DEBUG</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>logger_name</code> <code>str</code> <p>The identifier of the logger.</p> required <code>log_file_path</code> <code>Optional[Union[str, Path]]</code> <p>Path to the log file.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>The logger, as configured by the arguments.</p> Source code in <code>fractal_server/logger.py</code> <pre><code>def set_logger(\n    logger_name: str,\n    *,\n    log_file_path: Optional[Union[str, Path]] = None,\n) -&gt; logging.Logger:\n    \"\"\"\n    Set up a `fractal-server` logger\n\n    The logger (a `logging.Logger` object) will have the following properties:\n\n    * The attribute `Logger.propagate` set to `False`;\n    * One and only one `logging.StreamHandler` handler, with severity level set\n    to\n    [`FRACTAL_LOGGING_LEVEL`](../../../../configuration/#fractal_server.config.Settings.FRACTAL_LOGGING_LEVEL)\n    and formatter set as in the `logger.LOG_FORMAT` variable from the current\n    module;\n    * One or many `logging.FileHandler` handlers, including one pointint to\n    `log_file_path` (if set); all these handlers have severity level set to\n    `logging.DEBUG`.\n\n    Args:\n        logger_name: The identifier of the logger.\n        log_file_path: Path to the log file.\n\n    Returns:\n        logger: The logger, as configured by the arguments.\n    \"\"\"\n\n    logger = logging.getLogger(logger_name)\n    logger.propagate = False\n    logger.setLevel(logging.DEBUG)\n\n    current_stream_handlers = [\n        handler\n        for handler in logger.handlers\n        if isinstance(handler, logging.StreamHandler)\n    ]\n\n    if not current_stream_handlers:\n        stream_handler = logging.StreamHandler()\n        settings = Inject(get_settings)\n        stream_handler.setLevel(settings.FRACTAL_LOGGING_LEVEL)\n        stream_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(stream_handler)\n\n    if log_file_path is not None:\n        file_handler = logging.FileHandler(log_file_path, mode=\"a\")\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(LOG_FORMATTER)\n        file_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(file_handler)\n        current_file_handlers = [\n            handler\n            for handler in logger.handlers\n            if isinstance(handler, logging.FileHandler)\n        ]\n        if len(current_file_handlers) &gt; 1:\n            logger.warning(f\"Logger {logger_name} has multiple file handlers.\")\n\n    return logger\n</code></pre>"},{"location":"reference/fractal_server/main/","title":"main","text":""},{"location":"reference/fractal_server/main/#fractal_server.main--application-factory","title":"Application factory","text":"<p>This module sets up the FastAPI application that serves the Fractal Server.</p>"},{"location":"reference/fractal_server/main/#fractal_server.main.__on_startup","title":"<code>__on_startup()</code>  <code>async</code>","text":"<p>Private wrapper for routines that need to be executed at server start-up.</p> <p>It should only be called from a <code>@app.on_event(\"startup\")</code>-decorated callable.</p> Source code in <code>fractal_server/main.py</code> <pre><code>async def __on_startup() -&gt; None:\n    \"\"\"\n    Private wrapper for routines that need to be executed at server start-up.\n\n    It should only be called from a `@app.on_event(\"startup\")`-decorated\n    callable.\n    \"\"\"\n    check_settings()\n</code></pre>"},{"location":"reference/fractal_server/main/#fractal_server.main.check_settings","title":"<code>check_settings()</code>","text":"<p>Check and register the settings</p> <p>Verify the consistency of the settings, in particular that required variables are set.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the configuration is invalid.</p> Source code in <code>fractal_server/main.py</code> <pre><code>def check_settings() -&gt; None:\n    \"\"\"\n    Check and register the settings\n\n    Verify the consistency of the settings, in particular that required\n    variables are set.\n\n    Raises:\n        ValidationError: If the configuration is invalid.\n    \"\"\"\n    settings = Inject(get_settings)\n    settings.check()\n</code></pre>"},{"location":"reference/fractal_server/main/#fractal_server.main.collect_routers","title":"<code>collect_routers(app)</code>","text":"<p>Register the routers to the application</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>FastAPI</code> <p>The application to register the routers to.</p> required Source code in <code>fractal_server/main.py</code> <pre><code>def collect_routers(app: FastAPI) -&gt; None:\n    \"\"\"\n    Register the routers to the application\n\n    Args:\n        app:\n            The application to register the routers to.\n    \"\"\"\n    from .app.routes.api import router_api\n    from .app.routes.api.v1 import router_api_v1\n    from .app.routes.api.v2 import router_api_v2\n    from .app.routes.admin.v1 import router_admin_v1\n    from .app.routes.admin.v2 import router_admin_v2\n    from .app.routes.auth import router_auth\n\n    app.include_router(router_api, prefix=\"/api\")\n    app.include_router(router_api_v1, prefix=\"/api/v1\")\n    app.include_router(router_api_v2, prefix=\"/api/v2\")\n    app.include_router(\n        router_admin_v1, prefix=\"/admin/v1\", tags=[\"V1 Admin area\"]\n    )\n    app.include_router(\n        router_admin_v2, prefix=\"/admin/v2\", tags=[\"V2 Admin area\"]\n    )\n    app.include_router(router_auth, prefix=\"/auth\", tags=[\"Authentication\"])\n</code></pre>"},{"location":"reference/fractal_server/main/#fractal_server.main.on_startup","title":"<code>on_startup()</code>  <code>async</code>","text":"<p>Register the starup calls</p> <p>If the calls raise any error, the application startup is aborted.</p> Source code in <code>fractal_server/main.py</code> <pre><code>@app.on_event(\"startup\")\nasync def on_startup() -&gt; None:\n    \"\"\"\n    Register the starup calls\n\n    If the calls raise any error, the application startup is aborted.\n    \"\"\"\n    settings = Inject(get_settings)\n    await _create_first_user(\n        email=settings.FRACTAL_DEFAULT_ADMIN_EMAIL,\n        password=settings.FRACTAL_DEFAULT_ADMIN_PASSWORD,\n        username=settings.FRACTAL_DEFAULT_ADMIN_USERNAME,\n        is_superuser=True,\n        is_verified=True,\n    )\n    await __on_startup()\n</code></pre>"},{"location":"reference/fractal_server/main/#fractal_server.main.start_application","title":"<code>start_application()</code>","text":"<p>Create the application, initialise it and collect all available routers.</p> <p>Returns:</p> Name Type Description <code>app</code> <code>FastAPI</code> <p>The fully initialised application.</p> Source code in <code>fractal_server/main.py</code> <pre><code>def start_application() -&gt; FastAPI:\n    \"\"\"\n    Create the application, initialise it and collect all available routers.\n\n    Returns:\n        app:\n            The fully initialised application.\n    \"\"\"\n    app = FastAPI()\n    collect_routers(app)\n    return app\n</code></pre>"},{"location":"reference/fractal_server/syringe/","title":"syringe","text":"<p>This module provides an extremely simple utility for dependency injection.</p> <p>It's made up of a single singleton class that provides a directory for the dependencies. The dependencies are stored in a dictionary and can be overridden or popped from the directory.</p>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe--usage","title":"Usage:","text":"<pre><code>&gt;&gt;&gt; from syringe import Inject\n&gt;&gt;&gt; def foo():\n&gt;&gt;&gt;     return 42\n&gt;&gt;&gt;\n&gt;&gt;&gt; def oof():\n&gt;&gt;&gt;     return 24\n&gt;&gt;&gt;\n&gt;&gt;&gt; def bar():\n&gt;&gt;&gt;     return Inject(foo)\n&gt;&gt;&gt;\n&gt;&gt;&gt; bar()\n42\n&gt;&gt;&gt; Inject.override(foo, oof)\n&gt;&gt;&gt; bar()\n24\n&gt;&gt;&gt; Inject.pop(foo)\n&gt;&gt;&gt; bar()\n42\n</code></pre>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe.Inject","title":"<code>Inject = _Inject()</code>  <code>module-attribute</code>","text":"<p>The singleton instance of <code>_Inject</code>, the only public member of this module.</p>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe._Inject","title":"<code>_Inject</code>","text":"<p>Injection class</p> <p>This is a private class that is never directly instantiated.</p> <p>Attributes:</p> Name Type Description <code>_dependencies</code> <code>dict[Any, Any]</code> <p>The dependency directory</p> Source code in <code>fractal_server/syringe.py</code> <pre><code>class _Inject:\n    \"\"\"\n    Injection class\n\n    This is a private class that is never directly instantiated.\n\n    Attributes:\n        _dependencies:\n            The dependency directory\n    \"\"\"\n\n    _dependencies: dict[Any, Any] = {}\n\n    def __init__(self):\n        global _instance_count\n        if _instance_count == 1:\n            raise RuntimeError(\"You must only instance this class once\")\n        _instance_count += 1\n\n    @classmethod\n    def __call__(cls, _callable: Callable[..., T]) -&gt; T:\n        \"\"\"\n        Call the dependency\n\n        Args:\n            _callable:\n                Callable dependency object\n\n        Returns:\n            The output of calling `_callalbe` or its dependency override.\n        \"\"\"\n        try:\n            return cls._dependencies[_callable]()\n        except KeyError:\n            return _callable()\n\n    @classmethod\n    def pop(cls, _callable: Callable[..., T]) -&gt; T:\n        \"\"\"\n        Remove the dependency from the directory\n\n        Args:\n            _callable:\n                Callable dependency object\n        \"\"\"\n        try:\n            return cls._dependencies.pop(_callable)\n        except KeyError:\n            raise RuntimeError(f\"No dependency override for {_callable}\")\n\n    @classmethod\n    def override(\n        cls, _callable: Callable[..., T], value: Callable[..., T]\n    ) -&gt; None:\n        \"\"\"\n        Override dependency\n\n        Substitute a dependency with a different arbitrary callable.\n\n        Args:\n            _callable:\n                Callable dependency object\n            value:\n                Callable override\n        \"\"\"\n        cls._dependencies[_callable] = value\n</code></pre>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe._Inject.__call__","title":"<code>__call__(_callable)</code>  <code>classmethod</code>","text":"<p>Call the dependency</p> <p>Parameters:</p> Name Type Description Default <code>_callable</code> <code>Callable[..., T]</code> <p>Callable dependency object</p> required <p>Returns:</p> Type Description <code>T</code> <p>The output of calling <code>_callalbe</code> or its dependency override.</p> Source code in <code>fractal_server/syringe.py</code> <pre><code>@classmethod\ndef __call__(cls, _callable: Callable[..., T]) -&gt; T:\n    \"\"\"\n    Call the dependency\n\n    Args:\n        _callable:\n            Callable dependency object\n\n    Returns:\n        The output of calling `_callalbe` or its dependency override.\n    \"\"\"\n    try:\n        return cls._dependencies[_callable]()\n    except KeyError:\n        return _callable()\n</code></pre>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe._Inject.override","title":"<code>override(_callable, value)</code>  <code>classmethod</code>","text":"<p>Override dependency</p> <p>Substitute a dependency with a different arbitrary callable.</p> <p>Parameters:</p> Name Type Description Default <code>_callable</code> <code>Callable[..., T]</code> <p>Callable dependency object</p> required <code>value</code> <code>Callable[..., T]</code> <p>Callable override</p> required Source code in <code>fractal_server/syringe.py</code> <pre><code>@classmethod\ndef override(\n    cls, _callable: Callable[..., T], value: Callable[..., T]\n) -&gt; None:\n    \"\"\"\n    Override dependency\n\n    Substitute a dependency with a different arbitrary callable.\n\n    Args:\n        _callable:\n            Callable dependency object\n        value:\n            Callable override\n    \"\"\"\n    cls._dependencies[_callable] = value\n</code></pre>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe._Inject.pop","title":"<code>pop(_callable)</code>  <code>classmethod</code>","text":"<p>Remove the dependency from the directory</p> <p>Parameters:</p> Name Type Description Default <code>_callable</code> <code>Callable[..., T]</code> <p>Callable dependency object</p> required Source code in <code>fractal_server/syringe.py</code> <pre><code>@classmethod\ndef pop(cls, _callable: Callable[..., T]) -&gt; T:\n    \"\"\"\n    Remove the dependency from the directory\n\n    Args:\n        _callable:\n            Callable dependency object\n    \"\"\"\n    try:\n        return cls._dependencies.pop(_callable)\n    except KeyError:\n        raise RuntimeError(f\"No dependency override for {_callable}\")\n</code></pre>"},{"location":"reference/fractal_server/urls/","title":"urls","text":""},{"location":"reference/fractal_server/utils/","title":"utils","text":"<p>This module provides general purpose utilities that are not specific to any subsystem.</p>"},{"location":"reference/fractal_server/utils/#fractal_server.utils.execute_command","title":"<code>execute_command(*, cwd, command, logger_name=None)</code>  <code>async</code>","text":"<p>Execute arbitrary command</p> <p>If the command returns a return code different from zero, a RuntimeError containing the stderr is raised.</p> <p>Parameters:</p> Name Type Description Default <code>cwd</code> <code>Path</code> <p>The working directory for the command execution.</p> required <code>command</code> <code>str</code> <p>The command to execute.</p> required <p>Returns:</p> Name Type Description <code>stdout</code> <code>str</code> <p>The stdout from the command execution.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the process exited with non-zero status. The error string is set to the <code>stderr</code> of the process.</p> Source code in <code>fractal_server/utils.py</code> <pre><code>async def execute_command(\n    *,\n    cwd: Path,\n    command: str,\n    logger_name: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Execute arbitrary command\n\n    If the command returns a return code different from zero, a RuntimeError\n    containing the stderr is raised.\n\n    Args:\n        cwd:\n            The working directory for the command execution.\n        command:\n            The command to execute.\n\n    Returns:\n        stdout:\n            The stdout from the command execution.\n\n    Raises:\n        RuntimeError: if the process exited with non-zero status. The error\n            string is set to the `stderr` of the process.\n    \"\"\"\n    command_split = shlex_split(command)\n    cmd, *args = command_split\n\n    logger = get_logger(logger_name)\n    proc = await asyncio.create_subprocess_exec(\n        cmd,\n        *args,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE,\n        cwd=cwd,\n    )\n    stdout, stderr = await proc.communicate()\n    logger.debug(f\"Subprocess call to: {command}\")\n    logger.debug(stdout.decode(\"utf-8\"))\n    logger.debug(stderr.decode(\"utf-8\"))\n    if proc.returncode != 0:\n        raise RuntimeError(stderr.decode(\"utf-8\"))\n    return stdout.decode(\"utf-8\")\n</code></pre>"},{"location":"reference/fractal_server/utils/#fractal_server.utils.get_timestamp","title":"<code>get_timestamp()</code>","text":"<p>Get timezone aware timestamp.</p> Source code in <code>fractal_server/utils.py</code> <pre><code>def get_timestamp() -&gt; datetime:\n    \"\"\"\n    Get timezone aware timestamp.\n    \"\"\"\n    return datetime.now(tz=timezone.utc)\n</code></pre>"},{"location":"reference/fractal_server/app/","title":"app","text":""},{"location":"reference/fractal_server/app/db/","title":"db","text":"<p><code>db</code> module, loosely adapted from https://testdriven.io/blog/fastapi-sqlmodel/#async-sqlmodel</p>"},{"location":"reference/fractal_server/app/db/#fractal_server.app.db.DB","title":"<code>DB</code>","text":"<p>DB class</p> Source code in <code>fractal_server/app/db/__init__.py</code> <pre><code>class DB:\n    \"\"\"\n    DB class\n    \"\"\"\n\n    @classmethod\n    def engine_async(cls):\n        try:\n            return cls._engine_async\n        except AttributeError:\n            cls.set_async_db()\n            return cls._engine_async\n\n    @classmethod\n    def engine_sync(cls):\n        try:\n            return cls._engine_sync\n        except AttributeError:\n            cls.set_sync_db()\n            return cls._engine_sync\n\n    @classmethod\n    def set_async_db(cls):\n        settings = Inject(get_settings)\n        settings.check_db()\n\n        if settings.DB_ENGINE == \"sqlite\":\n            logger.warning(SQLITE_WARNING_MESSAGE)\n            # Set some sqlite-specific options\n            engine_kwargs_async = dict(poolclass=StaticPool)\n        else:\n            engine_kwargs_async = {\n                \"pool_pre_ping\": True,\n            }\n\n        cls._engine_async = create_async_engine(\n            settings.DATABASE_URL,\n            echo=settings.DB_ECHO,\n            future=True,\n            **engine_kwargs_async,\n        )\n        cls._async_session_maker = sessionmaker(\n            cls._engine_async,\n            class_=AsyncSession,\n            expire_on_commit=False,\n            future=True,\n        )\n\n    @classmethod\n    def set_sync_db(cls):\n        settings = Inject(get_settings)\n        settings.check_db()\n\n        if settings.DB_ENGINE == \"sqlite\":\n            logger.warning(SQLITE_WARNING_MESSAGE)\n            # Set some sqlite-specific options\n            engine_kwargs_sync = dict(\n                poolclass=StaticPool,\n                connect_args={\"check_same_thread\": False},\n            )\n        else:\n            engine_kwargs_sync = {}\n\n        cls._engine_sync = create_engine(\n            settings.DATABASE_SYNC_URL,\n            echo=settings.DB_ECHO,\n            future=True,\n            **engine_kwargs_sync,\n        )\n\n        cls._sync_session_maker = sessionmaker(\n            bind=cls._engine_sync,\n            autocommit=False,\n            autoflush=False,\n            future=True,\n        )\n\n        @event.listens_for(cls._engine_sync, \"connect\")\n        def set_sqlite_pragma(dbapi_connection, connection_record):\n            if settings.DB_ENGINE == \"sqlite\":\n                cursor = dbapi_connection.cursor()\n                cursor.execute(\"PRAGMA journal_mode=WAL\")\n                cursor.close()\n\n    @classmethod\n    async def get_async_db(cls) -&gt; AsyncGenerator[AsyncSession, None]:\n        \"\"\"\n        Get async database session\n        \"\"\"\n        try:\n            session_maker = cls._async_session_maker()\n        except AttributeError:\n            cls.set_async_db()\n            session_maker = cls._async_session_maker()\n        async with session_maker as async_session:\n            yield async_session\n\n    @classmethod\n    def get_sync_db(cls) -&gt; Generator[DBSyncSession, None, None]:\n        \"\"\"\n        Get sync database session\n        \"\"\"\n        try:\n            session_maker = cls._sync_session_maker()\n        except AttributeError:\n            cls.set_sync_db()\n            session_maker = cls._sync_session_maker()\n        with session_maker as sync_session:\n            yield sync_session\n</code></pre>"},{"location":"reference/fractal_server/app/db/#fractal_server.app.db.DB.get_async_db","title":"<code>get_async_db()</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get async database session</p> Source code in <code>fractal_server/app/db/__init__.py</code> <pre><code>@classmethod\nasync def get_async_db(cls) -&gt; AsyncGenerator[AsyncSession, None]:\n    \"\"\"\n    Get async database session\n    \"\"\"\n    try:\n        session_maker = cls._async_session_maker()\n    except AttributeError:\n        cls.set_async_db()\n        session_maker = cls._async_session_maker()\n    async with session_maker as async_session:\n        yield async_session\n</code></pre>"},{"location":"reference/fractal_server/app/db/#fractal_server.app.db.DB.get_sync_db","title":"<code>get_sync_db()</code>  <code>classmethod</code>","text":"<p>Get sync database session</p> Source code in <code>fractal_server/app/db/__init__.py</code> <pre><code>@classmethod\ndef get_sync_db(cls) -&gt; Generator[DBSyncSession, None, None]:\n    \"\"\"\n    Get sync database session\n    \"\"\"\n    try:\n        session_maker = cls._sync_session_maker()\n    except AttributeError:\n        cls.set_sync_db()\n        session_maker = cls._sync_session_maker()\n    with session_maker as sync_session:\n        yield sync_session\n</code></pre>"},{"location":"reference/fractal_server/app/models/","title":"models","text":"<p><code>models</code> module</p>"},{"location":"reference/fractal_server/app/models/linkuserproject/","title":"linkuserproject","text":""},{"location":"reference/fractal_server/app/models/linkuserproject/#fractal_server.app.models.linkuserproject.LinkUserProject","title":"<code>LinkUserProject</code>","text":"<p>             Bases: <code>SQLModel</code></p> <p>Crossing table between User and Project</p> Source code in <code>fractal_server/app/models/linkuserproject.py</code> <pre><code>class LinkUserProject(SQLModel, table=True):\n    \"\"\"\n    Crossing table between User and Project\n    \"\"\"\n\n    project_id: int = Field(foreign_key=\"project.id\", primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", primary_key=True)\n</code></pre>"},{"location":"reference/fractal_server/app/models/linkuserproject/#fractal_server.app.models.linkuserproject.LinkUserProjectV2","title":"<code>LinkUserProjectV2</code>","text":"<p>             Bases: <code>SQLModel</code></p> <p>Crossing table between User and ProjectV2</p> Source code in <code>fractal_server/app/models/linkuserproject.py</code> <pre><code>class LinkUserProjectV2(SQLModel, table=True):\n    \"\"\"\n    Crossing table between User and ProjectV2\n    \"\"\"\n\n    project_id: int = Field(foreign_key=\"projectv2.id\", primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", primary_key=True)\n</code></pre>"},{"location":"reference/fractal_server/app/models/security/","title":"security","text":""},{"location":"reference/fractal_server/app/models/security/#fractal_server.app.models.security.OAuthAccount","title":"<code>OAuthAccount</code>","text":"<p>             Bases: <code>SQLModel</code></p> <p>OAuth account model</p> <p>This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount. Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> <code>user_id</code> <code>int</code> <code>user</code> <code>Optional[UserOAuth]</code> <code>oauth_name</code> <code>str</code> <code>access_token</code> <code>str</code> <code>expires_at</code> <code>Optional[int]</code> <code>refresh_token</code> <code>Optional[str]</code> <code>account_id</code> <code>str</code> <code>account_email</code> <code>str</code> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class OAuthAccount(SQLModel, table=True):\n    \"\"\"\n    OAuth account model\n\n    This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount.\n    Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.\n\n    Attributes:\n        id:\n        user_id:\n        user:\n        oauth_name:\n        access_token:\n        expires_at:\n        refresh_token:\n        account_id:\n        account_email:\n    \"\"\"\n\n    __tablename__ = \"oauthaccount\"\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    user: Optional[\"UserOAuth\"] = Relationship(back_populates=\"oauth_accounts\")\n    oauth_name: str = Field(index=True, nullable=False)\n    access_token: str = Field(nullable=False)\n    expires_at: Optional[int] = Field(nullable=True)\n    refresh_token: Optional[str] = Field(nullable=True)\n    account_id: str = Field(index=True, nullable=False)\n    account_email: str = Field(nullable=False)\n\n    class Config:\n        orm_mode = True\n</code></pre>"},{"location":"reference/fractal_server/app/models/security/#fractal_server.app.models.security.UserOAuth","title":"<code>UserOAuth</code>","text":"<p>             Bases: <code>SQLModel</code></p> <p>User model</p> <p>This class is a modification of SQLModelBaseUserDB from from fastapi_users_db_sqlmodel. Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> <code>email</code> <code>EmailStr</code> <code>hashed_password</code> <code>str</code> <code>is_active</code> <code>bool</code> <code>is_superuser</code> <code>bool</code> <code>is_verified</code> <code>bool</code> <code>slurm_user</code> <code>Optional[str]</code> <code>slurm_accounts</code> <code>list[str]</code> <code>cache_dir</code> <code>Optional[str]</code> <code>username</code> <code>Optional[str]</code> <code>oauth_accounts</code> <code>list[OAuthAccount]</code> <code>project_list</code> <code>list[Project]</code> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserOAuth(SQLModel, table=True):\n    \"\"\"\n    User model\n\n    This class is a modification of SQLModelBaseUserDB from from\n    fastapi_users_db_sqlmodel. Original Copyright: 2022 Fran\u00e7ois Voron,\n    released under MIT licence.\n\n    Attributes:\n        id:\n        email:\n        hashed_password:\n        is_active:\n        is_superuser:\n        is_verified:\n        slurm_user:\n        slurm_accounts:\n        cache_dir:\n        username:\n        oauth_accounts:\n        project_list:\n    \"\"\"\n\n    __tablename__ = \"user_oauth\"\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n\n    email: EmailStr = Field(\n        sa_column_kwargs={\"unique\": True, \"index\": True}, nullable=False\n    )\n    hashed_password: str\n    is_active: bool = Field(True, nullable=False)\n    is_superuser: bool = Field(False, nullable=False)\n    is_verified: bool = Field(False, nullable=False)\n\n    slurm_user: Optional[str]\n    slurm_accounts: list[str] = Field(\n        sa_column=Column(JSON, server_default=\"[]\", nullable=False)\n    )\n    cache_dir: Optional[str]\n    username: Optional[str]\n\n    oauth_accounts: list[\"OAuthAccount\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"lazy\": \"joined\", \"cascade\": \"all, delete\"},\n    )\n    project_list: list[\"Project\"] = Relationship(  # noqa\n        back_populates=\"user_list\",\n        link_model=LinkUserProject,\n        sa_relationship_kwargs={\"lazy\": \"selectin\"},\n    )\n    project_list_v2: list[\"ProjectV2\"] = Relationship(  # noqa\n        back_populates=\"user_list\",\n        link_model=LinkUserProjectV2,\n        sa_relationship_kwargs={\"lazy\": \"selectin\"},\n    )\n\n    class Config:\n        orm_mode = True\n</code></pre>"},{"location":"reference/fractal_server/app/models/v1/","title":"v1","text":"<p><code>models</code> module</p>"},{"location":"reference/fractal_server/app/models/v1/dataset/","title":"dataset","text":""},{"location":"reference/fractal_server/app/models/v1/dataset/#fractal_server.app.models.v1.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>_DatasetBaseV1</code>, <code>SQLModel</code></p> <p>Represent a dataset</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> <p>Primary key</p> <code>project_id</code> <code>int</code> <p>ID of the project the workflow belongs to.</p> <code>meta</code> <code>dict[str, Any]</code> <p>Metadata of the Dataset</p> <code>history</code> <code>list[dict[str, Any]]</code> <p>History of the Dataset</p> <code>resource_list</code> <code>list[Resource]</code> <p>(Mapper attribute)</p> Source code in <code>fractal_server/app/models/v1/dataset.py</code> <pre><code>class Dataset(_DatasetBaseV1, SQLModel, table=True):\n    \"\"\"\n    Represent a dataset\n\n    Attributes:\n        id:\n            Primary key\n        project_id:\n            ID of the project the workflow belongs to.\n        meta:\n            Metadata of the Dataset\n        history:\n            History of the Dataset\n        resource_list:\n            (Mapper attribute)\n\n    \"\"\"\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n    project_id: int = Field(foreign_key=\"project.id\")\n    project: \"Project\" = Relationship(  # noqa: F821\n        sa_relationship_kwargs=dict(lazy=\"selectin\"),\n    )\n\n    resource_list: list[Resource] = Relationship(\n        sa_relationship_kwargs={\n            \"lazy\": \"selectin\",\n            \"order_by\": \"Resource.id\",\n            \"collection_class\": ordering_list(\"id\"),\n            \"cascade\": \"all, delete-orphan\",\n        }\n    )\n\n    meta: dict[str, Any] = Field(sa_column=Column(JSON), default={})\n    history: list[dict[str, Any]] = Field(\n        sa_column=Column(JSON, server_default=\"[]\", nullable=False)\n    )\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    @property\n    def paths(self) -&gt; list[str]:\n        return [r.path for r in self.resource_list]\n</code></pre>"},{"location":"reference/fractal_server/app/models/v1/job/","title":"job","text":""},{"location":"reference/fractal_server/app/models/v1/job/#fractal_server.app.models.v1.job.ApplyWorkflow","title":"<code>ApplyWorkflow</code>","text":"<p>             Bases: <code>_ApplyWorkflowBaseV1</code>, <code>SQLModel</code></p> <p>Represent a workflow run</p> <p>This table is responsible for storing the state of a workflow execution in the database.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> <p>Primary key.</p> <code>project_id</code> <code>Optional[int]</code> <p>ID of the project the workflow belongs to, or <code>None</code> if the project was deleted.</p> <code>input_dataset_id</code> <code>Optional[int]</code> <p>ID of the input dataset, or <code>None</code> if the dataset was deleted.</p> <code>output_dataset_id</code> <code>Optional[int]</code> <p>ID of the output dataset, or <code>None</code> if the dataset was deleted.</p> <code>workflow_id</code> <code>Optional[int]</code> <p>ID of the workflow being applied, or <code>None</code> if the workflow was deleted.</p> <code>status</code> <code>str</code> <p>Job status</p> <code>workflow_dump</code> <code>dict[str, Any]</code> <p>Copy of the submitted workflow at submission.</p> <code>input_dataset_dump</code> <code>dict[str, Any]</code> <p>Copy of the input_dataset at submission.</p> <code>output_dataset_dump</code> <code>dict[str, Any]</code> <p>Copy of the output_dataset at submission.</p> <code>start_timestamp</code> <code>datetime</code> <p>Timestamp of when the run began.</p> <code>end_timestamp</code> <code>Optional[datetime]</code> <p>Timestamp of when the run ended or failed.</p> <code>status</code> <code>str</code> <p>Status of the run.</p> <code>log</code> <code>Optional[str]</code> <p>Forward of the workflow logs.</p> <code>user_email</code> <code>str</code> <p>Email address of the user who submitted the job.</p> <code>slurm_account</code> <code>Optional[str]</code> <p>Account to be used when submitting the job to SLURM (see \"account\" option in <code>sbatch</code> documentation).</p> <code>first_task_index</code> <code>int</code> <code>last_task_index</code> <code>int</code> Source code in <code>fractal_server/app/models/v1/job.py</code> <pre><code>class ApplyWorkflow(_ApplyWorkflowBaseV1, SQLModel, table=True):\n    \"\"\"\n    Represent a workflow run\n\n    This table is responsible for storing the state of a workflow execution in\n    the database.\n\n    Attributes:\n        id:\n            Primary key.\n        project_id:\n            ID of the project the workflow belongs to, or `None` if the project\n            was deleted.\n        input_dataset_id:\n            ID of the input dataset, or `None` if the dataset was deleted.\n        output_dataset_id:\n            ID of the output dataset, or `None` if the dataset was deleted.\n        workflow_id:\n            ID of the workflow being applied, or `None` if the workflow was\n            deleted.\n        status:\n            Job status\n        workflow_dump:\n            Copy of the submitted workflow at submission.\n        input_dataset_dump:\n            Copy of the input_dataset at submission.\n        output_dataset_dump:\n            Copy of the output_dataset at submission.\n        start_timestamp:\n            Timestamp of when the run began.\n        end_timestamp:\n            Timestamp of when the run ended or failed.\n        status:\n            Status of the run.\n        log:\n            Forward of the workflow logs.\n        user_email:\n            Email address of the user who submitted the job.\n        slurm_account:\n            Account to be used when submitting the job to SLURM (see \"account\"\n            option in [`sbatch`\n            documentation](https://slurm.schedmd.com/sbatch.html#SECTION_OPTIONS)).\n        first_task_index:\n        last_task_index:\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n\n    project_id: Optional[int] = Field(foreign_key=\"project.id\")\n    workflow_id: Optional[int] = Field(foreign_key=\"workflow.id\")\n    input_dataset_id: Optional[int] = Field(foreign_key=\"dataset.id\")\n    output_dataset_id: Optional[int] = Field(foreign_key=\"dataset.id\")\n\n    user_email: str = Field(nullable=False)\n    slurm_account: Optional[str]\n\n    input_dataset_dump: dict[str, Any] = Field(\n        sa_column=Column(JSON, nullable=False)\n    )\n    output_dataset_dump: dict[str, Any] = Field(\n        sa_column=Column(JSON, nullable=False)\n    )\n    workflow_dump: dict[str, Any] = Field(\n        sa_column=Column(JSON, nullable=False)\n    )\n    project_dump: dict[str, Any] = Field(\n        sa_column=Column(JSON, nullable=False)\n    )\n\n    working_dir: Optional[str]\n    working_dir_user: Optional[str]\n    first_task_index: int\n    last_task_index: int\n\n    start_timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    end_timestamp: Optional[datetime] = Field(\n        default=None, sa_column=Column(DateTime(timezone=True))\n    )\n    status: str = JobStatusTypeV1.SUBMITTED\n    log: Optional[str] = None\n</code></pre>"},{"location":"reference/fractal_server/app/models/v1/project/","title":"project","text":""},{"location":"reference/fractal_server/app/models/v1/state/","title":"state","text":""},{"location":"reference/fractal_server/app/models/v1/state/#fractal_server.app.models.v1.state.State","title":"<code>State</code>","text":"<p>             Bases: <code>_StateBase</code>, <code>SQLModel</code></p> <p>Store arbitrary data in the database</p> <p>This table is just a state interchange that allows the system to store arbitrary data for later retrieval. This is particuarly important for long background tasks, in which it is not possible to return a meaningful response to the client within a single request lifespan.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> <p>Primary key</p> <code>data</code> <code>dict[str, Any]</code> <p>Content of the <code>State</code></p> <code>timestamp</code> <code>datetime</code> <p>Timestap of the <code>State</code></p> Source code in <code>fractal_server/app/models/v1/state.py</code> <pre><code>class State(_StateBase, SQLModel, table=True):\n    \"\"\"\n    Store arbitrary data in the database\n\n    This table is just a state interchange that allows the system to store\n    arbitrary data for later retrieval. This is particuarly important for long\n    background tasks, in which it is not possible to return a meaningful\n    response to the client within a single request lifespan.\n\n    Attributes:\n        id: Primary key\n        data: Content of the `State`\n        timestamp: Timestap of the `State`\n    \"\"\"\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n    data: dict[str, Any] = Field(sa_column=Column(JSON), default={})\n    timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True)),\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/models/v1/task/","title":"task","text":""},{"location":"reference/fractal_server/app/models/v1/task/#fractal_server.app.models.v1.task.Task","title":"<code>Task</code>","text":"<p>             Bases: <code>_TaskBaseV1</code>, <code>SQLModel</code></p> <p>Task model</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> <p>Primary key</p> <code>command</code> <code>str</code> <p>Executable command</p> <code>input_type</code> <code>str</code> <p>Expected type of input <code>Dataset</code></p> <code>output_type</code> <code>str</code> <p>Expected type of output <code>Dataset</code></p> <code>meta</code> <code>Optional[dict[str, Any]]</code> <p>Additional metadata related to execution (e.g. computational resources)</p> <code>source</code> <code>str</code> <p>inherited from <code>_TaskBase</code></p> <code>name</code> <code>str</code> <p>inherited from <code>_TaskBase</code></p> <code>args_schema</code> <code>Optional[dict[str, Any]]</code> <p>JSON schema of task arguments</p> <code>args_schema_version</code> <code>Optional[str]</code> <p>label pointing at how the JSON schema of task arguments was generated</p> Source code in <code>fractal_server/app/models/v1/task.py</code> <pre><code>class Task(_TaskBaseV1, SQLModel, table=True):\n    \"\"\"\n    Task model\n\n    Attributes:\n        id: Primary key\n        command: Executable command\n        input_type: Expected type of input `Dataset`\n        output_type: Expected type of output `Dataset`\n        meta:\n            Additional metadata related to execution (e.g. computational\n            resources)\n        source: inherited from `_TaskBase`\n        name: inherited from `_TaskBase`\n        args_schema: JSON schema of task arguments\n        args_schema_version:\n            label pointing at how the JSON schema of task arguments was\n            generated\n    \"\"\"\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    command: str\n    source: str = Field(unique=True)\n    input_type: str\n    output_type: str\n    meta: Optional[dict[str, Any]] = Field(sa_column=Column(JSON), default={})\n    owner: Optional[str] = None\n    version: Optional[str] = None\n    args_schema: Optional[dict[str, Any]] = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_version: Optional[str]\n    docs_info: Optional[str] = None\n    docs_link: Optional[HttpUrl] = None\n\n    is_v2_compatible: bool = Field(\n        default=False, sa_column_kwargs={\"server_default\": sql.false()}\n    )\n\n    @property\n    def parallelization_level(self) -&gt; Optional[str]:\n        try:\n            return self.meta[\"parallelization_level\"]\n        except KeyError:\n            return None\n\n    @property\n    def is_parallel(self) -&gt; bool:\n        return bool(self.parallelization_level)\n\n    @property\n    def default_args_from_args_schema(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Extract default arguments from args_schema\n        \"\"\"\n        # Return {} if there is no args_schema\n        if self.args_schema is None:\n            return {}\n        # Try to construct default_args\n        try:\n            default_args = {}\n            properties = self.args_schema[\"properties\"]\n            for prop_name, prop_schema in properties.items():\n                default_value = prop_schema.get(\"default\", None)\n                if default_value is not None:\n                    default_args[prop_name] = default_value\n            return default_args\n        except KeyError as e:\n            logging.warning(\n                \"Cannot set default_args from args_schema=\"\n                f\"{json.dumps(self.args_schema)}\\n\"\n                f\"Original KeyError: {str(e)}\"\n            )\n            return {}\n</code></pre>"},{"location":"reference/fractal_server/app/models/v1/task/#fractal_server.app.models.v1.task.Task.default_args_from_args_schema","title":"<code>default_args_from_args_schema: dict[str, Any]</code>  <code>property</code>","text":"<p>Extract default arguments from args_schema</p>"},{"location":"reference/fractal_server/app/models/v1/workflow/","title":"workflow","text":""},{"location":"reference/fractal_server/app/models/v1/workflow/#fractal_server.app.models.v1.workflow.Workflow","title":"<code>Workflow</code>","text":"<p>             Bases: <code>_WorkflowBaseV1</code>, <code>SQLModel</code></p> <p>Workflow</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> <p>Primary key</p> <code>project_id</code> <code>int</code> <p>ID of the project the workflow belongs to.</p> <code>task_list</code> <code>list[WorkflowTask]</code> <p>List of associations to tasks.</p> Source code in <code>fractal_server/app/models/v1/workflow.py</code> <pre><code>class Workflow(_WorkflowBaseV1, SQLModel, table=True):\n    \"\"\"\n    Workflow\n\n    Attributes:\n        id:\n            Primary key\n        project_id:\n            ID of the project the workflow belongs to.\n        task_list:\n            List of associations to tasks.\n    \"\"\"\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n    project_id: int = Field(foreign_key=\"project.id\")\n    project: \"Project\" = Relationship(  # noqa: F821\n        sa_relationship_kwargs=dict(lazy=\"selectin\"),\n    )\n\n    task_list: list[WorkflowTask] = Relationship(\n        sa_relationship_kwargs=dict(\n            lazy=\"selectin\",\n            order_by=\"WorkflowTask.order\",\n            collection_class=ordering_list(\"order\"),\n            cascade=\"all, delete-orphan\",\n        ),\n    )\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n\n    @property\n    def input_type(self):\n        return self.task_list[0].task.input_type\n\n    @property\n    def output_type(self):\n        return self.task_list[-1].task.output_type\n</code></pre>"},{"location":"reference/fractal_server/app/models/v1/workflow/#fractal_server.app.models.v1.workflow.WorkflowTask","title":"<code>WorkflowTask</code>","text":"<p>             Bases: <code>_WorkflowTaskBaseV1</code>, <code>SQLModel</code></p> <p>A Task as part of a Workflow</p> <p>This is a crossing table between Task and Workflow. In addition to the foreign keys, it allows for parameter overriding and keeps the order within the list of tasks of the workflow.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> <p>Primary key</p> <code>workflow_id</code> <code>int</code> <p>ID of the <code>Workflow</code> the <code>WorkflowTask</code> belongs to</p> <code>task_id</code> <code>int</code> <p>ID of the task corresponding to the <code>WorkflowTask</code></p> <code>order</code> <code>Optional[int]</code> <p>Positional order of the <code>WorkflowTask</code> in <code>Workflow.task_list</code></p> <code>meta</code> <code>Optional[dict[str, Any]]</code> <p>Additional parameters useful for execution</p> <code>args</code> <code>Optional[dict[str, Any]]</code> <p>Task arguments</p> <code>task</code> <code>Task</code> <p><code>Task</code> object associated with the current <code>WorkflowTask</code></p> Source code in <code>fractal_server/app/models/v1/workflow.py</code> <pre><code>class WorkflowTask(_WorkflowTaskBaseV1, SQLModel, table=True):\n    \"\"\"\n    A Task as part of a Workflow\n\n    This is a crossing table between Task and Workflow. In addition to the\n    foreign keys, it allows for parameter overriding and keeps the order\n    within the list of tasks of the workflow.\n\n\n    Attributes:\n        id:\n            Primary key\n        workflow_id:\n            ID of the `Workflow` the `WorkflowTask` belongs to\n        task_id:\n            ID of the task corresponding to the `WorkflowTask`\n        order:\n            Positional order of the `WorkflowTask` in `Workflow.task_list`\n        meta:\n            Additional parameters useful for execution\n        args:\n            Task arguments\n        task:\n            `Task` object associated with the current `WorkflowTask`\n\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n        fields = {\"parent\": {\"exclude\": True}}\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n\n    workflow_id: int = Field(foreign_key=\"workflow.id\")\n    task_id: int = Field(foreign_key=\"task.id\")\n    order: Optional[int]\n    meta: Optional[dict[str, Any]] = Field(sa_column=Column(JSON))\n    args: Optional[dict[str, Any]] = Field(sa_column=Column(JSON))\n    task: Task = Relationship(sa_relationship_kwargs=dict(lazy=\"selectin\"))\n\n    @validator(\"args\")\n    def validate_args(cls, value: dict = None):\n        \"\"\"\n        Prevent fractal task reserved parameter names from entering args\n\n        Forbidden argument names are `input_paths`, `output_path`, `metadata`,\n        `component`.\n        \"\"\"\n        if value is None:\n            return\n        forbidden_args_keys = {\n            \"input_paths\",\n            \"output_path\",\n            \"metadata\",\n            \"component\",\n        }\n        args_keys = set(value.keys())\n        intersect_keys = forbidden_args_keys.intersection(args_keys)\n        if intersect_keys:\n            raise ValueError(\n                \"`args` contains the following forbidden keys: \"\n                f\"{intersect_keys}\"\n            )\n        return value\n\n    @property\n    def is_parallel(self) -&gt; bool:\n        return self.task.is_parallel\n\n    @property\n    def parallelization_level(self) -&gt; Union[str, None]:\n        return self.task.parallelization_level\n</code></pre>"},{"location":"reference/fractal_server/app/models/v1/workflow/#fractal_server.app.models.v1.workflow.WorkflowTask.validate_args","title":"<code>validate_args(value=None)</code>","text":"<p>Prevent fractal task reserved parameter names from entering args</p> <p>Forbidden argument names are <code>input_paths</code>, <code>output_path</code>, <code>metadata</code>, <code>component</code>.</p> Source code in <code>fractal_server/app/models/v1/workflow.py</code> <pre><code>@validator(\"args\")\ndef validate_args(cls, value: dict = None):\n    \"\"\"\n    Prevent fractal task reserved parameter names from entering args\n\n    Forbidden argument names are `input_paths`, `output_path`, `metadata`,\n    `component`.\n    \"\"\"\n    if value is None:\n        return\n    forbidden_args_keys = {\n        \"input_paths\",\n        \"output_path\",\n        \"metadata\",\n        \"component\",\n    }\n    args_keys = set(value.keys())\n    intersect_keys = forbidden_args_keys.intersection(args_keys)\n    if intersect_keys:\n        raise ValueError(\n            \"`args` contains the following forbidden keys: \"\n            f\"{intersect_keys}\"\n        )\n    return value\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/","title":"v2","text":"<p>v2 <code>models</code> module</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.LinkUserProjectV2","title":"<code>LinkUserProjectV2</code>","text":"<p>             Bases: <code>SQLModel</code></p> <p>Crossing table between User and ProjectV2</p> Source code in <code>fractal_server/app/models/linkuserproject.py</code> <pre><code>class LinkUserProjectV2(SQLModel, table=True):\n    \"\"\"\n    Crossing table between User and ProjectV2\n    \"\"\"\n\n    project_id: int = Field(foreign_key=\"projectv2.id\", primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", primary_key=True)\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.TaskV2","title":"<code>TaskV2</code>","text":"<p>             Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/task.py</code> <pre><code>class TaskV2(SQLModel, table=True):\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n\n    type: str\n    command_non_parallel: Optional[str] = None\n    command_parallel: Optional[str] = None\n    source: str = Field(unique=True)\n\n    meta_non_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n    meta_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n\n    owner: Optional[str] = None\n    version: Optional[str] = None\n    args_schema_non_parallel: Optional[dict[str, Any]] = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_parallel: Optional[dict[str, Any]] = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_version: Optional[str]\n    docs_info: Optional[str] = None\n    docs_link: Optional[HttpUrl] = None\n\n    input_types: dict[str, bool] = Field(sa_column=Column(JSON), default={})\n    output_types: dict[str, bool] = Field(sa_column=Column(JSON), default={})\n\n    @property\n    def default_args_non_parallel_from_args_schema(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Extract default arguments from args_schema\n        \"\"\"\n        # Return {} if there is no args_schema\n        if self.args_schema_non_parallel is None:\n            return {}\n        # Try to construct default_args\n        try:\n            default_args = {}\n            properties = self.args_schema_non_parallel[\"properties\"]\n            for prop_name, prop_schema in properties.items():\n                default_value = prop_schema.get(\"default\", None)\n                if default_value is not None:\n                    default_args[prop_name] = default_value\n            return default_args\n        except KeyError as e:\n            logging.warning(\n                \"Cannot set default_args from args_schema_non_parallel=\"\n                f\"{json.dumps(self.args_schema_non_parallel)}\\n\"\n                f\"Original KeyError: {str(e)}\"\n            )\n            return {}\n\n    @property\n    def default_args_parallel_from_args_schema(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Extract default arguments from args_schema\n        \"\"\"\n        # Return {} if there is no args_schema\n        if self.args_schema_parallel is None:\n            return {}\n        # Try to construct default_args\n        try:\n            default_args = {}\n            properties = self.args_schema_parallel[\"properties\"]\n            for prop_name, prop_schema in properties.items():\n                default_value = prop_schema.get(\"default\", None)\n                if default_value is not None:\n                    default_args[prop_name] = default_value\n            return default_args\n        except KeyError as e:\n            logging.warning(\n                \"Cannot set default_args from args_schema_parallel=\"\n                f\"{json.dumps(self.args_schema_parallel)}\\n\"\n                f\"Original KeyError: {str(e)}\"\n            )\n            return {}\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.TaskV2.default_args_non_parallel_from_args_schema","title":"<code>default_args_non_parallel_from_args_schema: dict[str, Any]</code>  <code>property</code>","text":"<p>Extract default arguments from args_schema</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.TaskV2.default_args_parallel_from_args_schema","title":"<code>default_args_parallel_from_args_schema: dict[str, Any]</code>  <code>property</code>","text":"<p>Extract default arguments from args_schema</p>"},{"location":"reference/fractal_server/app/models/v2/collection_state/","title":"collection_state","text":""},{"location":"reference/fractal_server/app/models/v2/dataset/","title":"dataset","text":""},{"location":"reference/fractal_server/app/models/v2/job/","title":"job","text":""},{"location":"reference/fractal_server/app/models/v2/project/","title":"project","text":""},{"location":"reference/fractal_server/app/models/v2/task/","title":"task","text":""},{"location":"reference/fractal_server/app/models/v2/task/#fractal_server.app.models.v2.task.TaskV2","title":"<code>TaskV2</code>","text":"<p>             Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/task.py</code> <pre><code>class TaskV2(SQLModel, table=True):\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n\n    type: str\n    command_non_parallel: Optional[str] = None\n    command_parallel: Optional[str] = None\n    source: str = Field(unique=True)\n\n    meta_non_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n    meta_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n\n    owner: Optional[str] = None\n    version: Optional[str] = None\n    args_schema_non_parallel: Optional[dict[str, Any]] = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_parallel: Optional[dict[str, Any]] = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_version: Optional[str]\n    docs_info: Optional[str] = None\n    docs_link: Optional[HttpUrl] = None\n\n    input_types: dict[str, bool] = Field(sa_column=Column(JSON), default={})\n    output_types: dict[str, bool] = Field(sa_column=Column(JSON), default={})\n\n    @property\n    def default_args_non_parallel_from_args_schema(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Extract default arguments from args_schema\n        \"\"\"\n        # Return {} if there is no args_schema\n        if self.args_schema_non_parallel is None:\n            return {}\n        # Try to construct default_args\n        try:\n            default_args = {}\n            properties = self.args_schema_non_parallel[\"properties\"]\n            for prop_name, prop_schema in properties.items():\n                default_value = prop_schema.get(\"default\", None)\n                if default_value is not None:\n                    default_args[prop_name] = default_value\n            return default_args\n        except KeyError as e:\n            logging.warning(\n                \"Cannot set default_args from args_schema_non_parallel=\"\n                f\"{json.dumps(self.args_schema_non_parallel)}\\n\"\n                f\"Original KeyError: {str(e)}\"\n            )\n            return {}\n\n    @property\n    def default_args_parallel_from_args_schema(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Extract default arguments from args_schema\n        \"\"\"\n        # Return {} if there is no args_schema\n        if self.args_schema_parallel is None:\n            return {}\n        # Try to construct default_args\n        try:\n            default_args = {}\n            properties = self.args_schema_parallel[\"properties\"]\n            for prop_name, prop_schema in properties.items():\n                default_value = prop_schema.get(\"default\", None)\n                if default_value is not None:\n                    default_args[prop_name] = default_value\n            return default_args\n        except KeyError as e:\n            logging.warning(\n                \"Cannot set default_args from args_schema_parallel=\"\n                f\"{json.dumps(self.args_schema_parallel)}\\n\"\n                f\"Original KeyError: {str(e)}\"\n            )\n            return {}\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/task/#fractal_server.app.models.v2.task.TaskV2.default_args_non_parallel_from_args_schema","title":"<code>default_args_non_parallel_from_args_schema: dict[str, Any]</code>  <code>property</code>","text":"<p>Extract default arguments from args_schema</p>"},{"location":"reference/fractal_server/app/models/v2/task/#fractal_server.app.models.v2.task.TaskV2.default_args_parallel_from_args_schema","title":"<code>default_args_parallel_from_args_schema: dict[str, Any]</code>  <code>property</code>","text":"<p>Extract default arguments from args_schema</p>"},{"location":"reference/fractal_server/app/models/v2/workflow/","title":"workflow","text":""},{"location":"reference/fractal_server/app/models/v2/workflowtask/","title":"workflowtask","text":""},{"location":"reference/fractal_server/app/routes/","title":"routes","text":""},{"location":"reference/fractal_server/app/routes/auth/","title":"auth","text":"<p>Definition of <code>/auth</code> routes.</p>"},{"location":"reference/fractal_server/app/routes/auth/#fractal_server.app.routes.auth.get_current_user","title":"<code>get_current_user(user=Depends(current_active_user))</code>  <code>async</code>","text":"<p>Return current user</p> Source code in <code>fractal_server/app/routes/auth.py</code> <pre><code>@router_auth.get(\"/current-user/\", response_model=UserRead)\nasync def get_current_user(user: User = Depends(current_active_user)):\n    \"\"\"\n    Return current user\n    \"\"\"\n    return user\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/#fractal_server.app.routes.auth.list_users","title":"<code>list_users(user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return list of all users</p> Source code in <code>fractal_server/app/routes/auth.py</code> <pre><code>@router_auth.get(\"/users/\", response_model=list[UserRead])\nasync def list_users(\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Return list of all users\n    \"\"\"\n    stm = select(User)\n    res = await db.execute(stm)\n    user_list = res.scalars().unique().all()\n    await db.close()\n    return user_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/","title":"admin","text":""},{"location":"reference/fractal_server/app/routes/admin/v1/","title":"v1","text":"<p>Definition of <code>/admin</code> routes.</p>"},{"location":"reference/fractal_server/app/routes/admin/v1/#fractal_server.app.routes.admin.v1._convert_to_db_timestamp","title":"<code>_convert_to_db_timestamp(dt)</code>","text":"<p>This function takes a timezone-aware datetime and converts it to UTC. If using SQLite, it also removes the timezone information in order to make the datetime comparable with datetimes in the database.</p> Source code in <code>fractal_server/app/routes/admin/v1.py</code> <pre><code>def _convert_to_db_timestamp(dt: datetime) -&gt; datetime:\n    \"\"\"\n    This function takes a timezone-aware datetime and converts it to UTC.\n    If using SQLite, it also removes the timezone information in order to make\n    the datetime comparable with datetimes in the database.\n    \"\"\"\n    if dt.tzinfo is None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"The timestamp provided has no timezone information: {dt}\",\n        )\n    _dt = dt.astimezone(timezone.utc)\n    if Inject(get_settings).DB_ENGINE == \"sqlite\":\n        return _dt.replace(tzinfo=None)\n    return _dt\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v1/#fractal_server.app.routes.admin.v1.download_job_logs","title":"<code>download_job_logs(job_id, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Download job folder</p> Source code in <code>fractal_server/app/routes/admin/v1.py</code> <pre><code>@router_admin_v1.get(\n    \"/job/{job_id}/download/\",\n    response_class=StreamingResponse,\n)\nasync def download_job_logs(\n    job_id: int,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Download job folder\n    \"\"\"\n    # Get job from DB\n    job = await db.get(ApplyWorkflow, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n    # Create and return byte stream for zipped log folder\n    PREFIX_ZIP = Path(job.working_dir).name\n    zip_filename = f\"{PREFIX_ZIP}_archive.zip\"\n    byte_stream = _zip_folder_to_byte_stream(\n        folder=job.working_dir, zip_filename=zip_filename\n    )\n    return StreamingResponse(\n        iter([byte_stream.getvalue()]),\n        media_type=\"application/x-zip-compressed\",\n        headers={\"Content-Disposition\": f\"attachment;filename={zip_filename}\"},\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v1/#fractal_server.app.routes.admin.v1.stop_job","title":"<code>stop_job(job_id, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Stop execution of a workflow job.</p> <p>Only available for slurm backend.</p> Source code in <code>fractal_server/app/routes/admin/v1.py</code> <pre><code>@router_admin_v1.get(\"/job/{job_id}/stop/\", status_code=202)\nasync def stop_job(\n    job_id: int,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Stop execution of a workflow job.\n\n    Only available for slurm backend.\n    \"\"\"\n\n    _check_backend_is_slurm()\n\n    job = await db.get(ApplyWorkflow, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n\n    _write_shutdown_file(job=job)\n\n    return Response(status_code=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v1/#fractal_server.app.routes.admin.v1.update_job","title":"<code>update_job(job_update, job_id, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Change the status of an existing job.</p> <p>This endpoint is only open to superusers, and it does not apply project-based access-control to jobs.</p> Source code in <code>fractal_server/app/routes/admin/v1.py</code> <pre><code>@router_admin_v1.patch(\n    \"/job/{job_id}/\",\n    response_model=ApplyWorkflowReadV1,\n)\nasync def update_job(\n    job_update: ApplyWorkflowUpdateV1,\n    job_id: int,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[ApplyWorkflowReadV1]:\n    \"\"\"\n    Change the status of an existing job.\n\n    This endpoint is only open to superusers, and it does not apply\n    project-based access-control to jobs.\n    \"\"\"\n    job = await db.get(ApplyWorkflow, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n\n    if job_update.status != JobStatusTypeV1.FAILED:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Cannot set job status to {job_update.status}\",\n        )\n\n    setattr(job, \"status\", job_update.status)\n    setattr(job, \"end_timestamp\", get_timestamp())\n    await db.commit()\n    await db.refresh(job)\n    await db.close()\n    return job\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v1/#fractal_server.app.routes.admin.v1.view_dataset","title":"<code>view_dataset(id=None, user_id=None, project_id=None, name_contains=None, type=None, timestamp_created_min=None, timestamp_created_max=None, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>dataset</code> table.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>dataset.id</code>.</p> <code>None</code> <code>project_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>dataset.project_id</code>.</p> <code>None</code> <code>name_contains</code> <code>Optional[str]</code> <p>If not <code>None</code>, select datasets such that their <code>name</code> attribute contains <code>name_contains</code> (case-insensitive).</p> <code>None</code> <code>type</code> <code>Optional[str]</code> <p>If not <code>None</code>, select a given <code>dataset.type</code>.</p> <code>None</code> Source code in <code>fractal_server/app/routes/admin/v1.py</code> <pre><code>@router_admin_v1.get(\"/dataset/\", response_model=list[DatasetReadV1])\nasync def view_dataset(\n    id: Optional[int] = None,\n    user_id: Optional[int] = None,\n    project_id: Optional[int] = None,\n    name_contains: Optional[str] = None,\n    type: Optional[str] = None,\n    timestamp_created_min: Optional[datetime] = None,\n    timestamp_created_max: Optional[datetime] = None,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[DatasetReadV1]:\n    \"\"\"\n    Query `dataset` table.\n\n    Args:\n        id: If not `None`, select a given `dataset.id`.\n        project_id: If not `None`, select a given `dataset.project_id`.\n        name_contains: If not `None`, select datasets such that their\n            `name` attribute contains `name_contains` (case-insensitive).\n        type: If not `None`, select a given `dataset.type`.\n    \"\"\"\n    stm = select(Dataset)\n\n    if user_id is not None:\n        stm = stm.join(Project).where(\n            Project.user_list.any(User.id == user_id)\n        )\n    if id is not None:\n        stm = stm.where(Dataset.id == id)\n    if project_id is not None:\n        stm = stm.where(Dataset.project_id == project_id)\n    if name_contains is not None:\n        # SQLAlchemy2: use icontains\n        stm = stm.where(\n            func.lower(Dataset.name).contains(name_contains.lower())\n        )\n    if type is not None:\n        stm = stm.where(Dataset.type == type)\n    if timestamp_created_min is not None:\n        timestamp_created_min = _convert_to_db_timestamp(timestamp_created_min)\n        stm = stm.where(Dataset.timestamp_created &gt;= timestamp_created_min)\n    if timestamp_created_max is not None:\n        timestamp_created_max = _convert_to_db_timestamp(timestamp_created_max)\n        stm = stm.where(Dataset.timestamp_created &lt;= timestamp_created_max)\n\n    res = await db.execute(stm)\n    dataset_list = res.scalars().all()\n    await db.close()\n\n    return dataset_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v1/#fractal_server.app.routes.admin.v1.view_job","title":"<code>view_job(id=None, user_id=None, project_id=None, input_dataset_id=None, output_dataset_id=None, workflow_id=None, status=None, start_timestamp_min=None, start_timestamp_max=None, end_timestamp_min=None, end_timestamp_max=None, log=True, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>ApplyWorkflow</code> table.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.id</code>.</p> <code>None</code> <code>project_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.project_id</code>.</p> <code>None</code> <code>input_dataset_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.input_dataset_id</code>.</p> <code>None</code> <code>output_dataset_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.output_dataset_id</code>.</p> <code>None</code> <code>workflow_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.workflow_id</code>.</p> <code>None</code> <code>status</code> <code>Optional[JobStatusTypeV1]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.status</code>.</p> <code>None</code> <code>start_timestamp_min</code> <code>Optional[datetime]</code> <p>If not <code>None</code>, select a rows with <code>start_timestamp</code> after <code>start_timestamp_min</code>.</p> <code>None</code> <code>start_timestamp_max</code> <code>Optional[datetime]</code> <p>If not <code>None</code>, select a rows with <code>start_timestamp</code> before <code>start_timestamp_min</code>.</p> <code>None</code> <code>end_timestamp_min</code> <code>Optional[datetime]</code> <p>If not <code>None</code>, select a rows with <code>end_timestamp</code> after <code>end_timestamp_min</code>.</p> <code>None</code> <code>end_timestamp_max</code> <code>Optional[datetime]</code> <p>If not <code>None</code>, select a rows with <code>end_timestamp</code> before <code>end_timestamp_min</code>.</p> <code>None</code> <code>log</code> <code>bool</code> <p>If <code>True</code>, include <code>job.log</code>, if <code>False</code> <code>job.log</code> is set to <code>None</code>.</p> <code>True</code> Source code in <code>fractal_server/app/routes/admin/v1.py</code> <pre><code>@router_admin_v1.get(\"/job/\", response_model=list[ApplyWorkflowReadV1])\nasync def view_job(\n    id: Optional[int] = None,\n    user_id: Optional[int] = None,\n    project_id: Optional[int] = None,\n    input_dataset_id: Optional[int] = None,\n    output_dataset_id: Optional[int] = None,\n    workflow_id: Optional[int] = None,\n    status: Optional[JobStatusTypeV1] = None,\n    start_timestamp_min: Optional[datetime] = None,\n    start_timestamp_max: Optional[datetime] = None,\n    end_timestamp_min: Optional[datetime] = None,\n    end_timestamp_max: Optional[datetime] = None,\n    log: bool = True,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ApplyWorkflowReadV1]:\n    \"\"\"\n    Query `ApplyWorkflow` table.\n\n    Args:\n        id: If not `None`, select a given `applyworkflow.id`.\n        project_id: If not `None`, select a given `applyworkflow.project_id`.\n        input_dataset_id: If not `None`, select a given\n            `applyworkflow.input_dataset_id`.\n        output_dataset_id: If not `None`, select a given\n            `applyworkflow.output_dataset_id`.\n        workflow_id: If not `None`, select a given `applyworkflow.workflow_id`.\n        status: If not `None`, select a given `applyworkflow.status`.\n        start_timestamp_min: If not `None`, select a rows with\n            `start_timestamp` after `start_timestamp_min`.\n        start_timestamp_max: If not `None`, select a rows with\n            `start_timestamp` before `start_timestamp_min`.\n        end_timestamp_min: If not `None`, select a rows with `end_timestamp`\n            after `end_timestamp_min`.\n        end_timestamp_max: If not `None`, select a rows with `end_timestamp`\n            before `end_timestamp_min`.\n        log: If `True`, include `job.log`, if `False`\n            `job.log` is set to `None`.\n    \"\"\"\n    stm = select(ApplyWorkflow)\n\n    if id is not None:\n        stm = stm.where(ApplyWorkflow.id == id)\n    if user_id is not None:\n        stm = stm.join(Project).where(\n            Project.user_list.any(User.id == user_id)\n        )\n    if project_id is not None:\n        stm = stm.where(ApplyWorkflow.project_id == project_id)\n    if input_dataset_id is not None:\n        stm = stm.where(ApplyWorkflow.input_dataset_id == input_dataset_id)\n    if output_dataset_id is not None:\n        stm = stm.where(ApplyWorkflow.output_dataset_id == output_dataset_id)\n    if workflow_id is not None:\n        stm = stm.where(ApplyWorkflow.workflow_id == workflow_id)\n    if status is not None:\n        stm = stm.where(ApplyWorkflow.status == status)\n    if start_timestamp_min is not None:\n        start_timestamp_min = _convert_to_db_timestamp(start_timestamp_min)\n        stm = stm.where(ApplyWorkflow.start_timestamp &gt;= start_timestamp_min)\n    if start_timestamp_max is not None:\n        start_timestamp_max = _convert_to_db_timestamp(start_timestamp_max)\n        stm = stm.where(ApplyWorkflow.start_timestamp &lt;= start_timestamp_max)\n    if end_timestamp_min is not None:\n        end_timestamp_min = _convert_to_db_timestamp(end_timestamp_min)\n        stm = stm.where(ApplyWorkflow.end_timestamp &gt;= end_timestamp_min)\n    if end_timestamp_max is not None:\n        end_timestamp_max = _convert_to_db_timestamp(end_timestamp_max)\n        stm = stm.where(ApplyWorkflow.end_timestamp &lt;= end_timestamp_max)\n\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    await db.close()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v1/#fractal_server.app.routes.admin.v1.view_project","title":"<code>view_project(id=None, user_id=None, timestamp_created_min=None, timestamp_created_max=None, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>project</code> table.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>project.id</code>.</p> <code>None</code> <code>user_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>project.user_id</code>.</p> <code>None</code> Source code in <code>fractal_server/app/routes/admin/v1.py</code> <pre><code>@router_admin_v1.get(\"/project/\", response_model=list[ProjectReadV1])\nasync def view_project(\n    id: Optional[int] = None,\n    user_id: Optional[int] = None,\n    timestamp_created_min: Optional[datetime] = None,\n    timestamp_created_max: Optional[datetime] = None,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProjectReadV1]:\n    \"\"\"\n    Query `project` table.\n\n    Args:\n        id: If not `None`, select a given `project.id`.\n        user_id: If not `None`, select a given `project.user_id`.\n    \"\"\"\n\n    stm = select(Project)\n\n    if id is not None:\n        stm = stm.where(Project.id == id)\n\n    if user_id is not None:\n        stm = stm.where(Project.user_list.any(User.id == user_id))\n    if timestamp_created_min is not None:\n        timestamp_created_min = _convert_to_db_timestamp(timestamp_created_min)\n        stm = stm.where(Project.timestamp_created &gt;= timestamp_created_min)\n    if timestamp_created_max is not None:\n        timestamp_created_max = _convert_to_db_timestamp(timestamp_created_max)\n        stm = stm.where(Project.timestamp_created &lt;= timestamp_created_max)\n\n    res = await db.execute(stm)\n    project_list = res.scalars().all()\n    await db.close()\n\n    return project_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v1/#fractal_server.app.routes.admin.v1.view_workflow","title":"<code>view_workflow(id=None, user_id=None, project_id=None, name_contains=None, timestamp_created_min=None, timestamp_created_max=None, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>workflow</code> table.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>workflow.id</code>.</p> <code>None</code> <code>project_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>workflow.project_id</code>.</p> <code>None</code> <code>name_contains</code> <code>Optional[str]</code> <p>If not <code>None</code>, select workflows such that their <code>name</code> attribute contains <code>name_contains</code> (case-insensitive).</p> <code>None</code> Source code in <code>fractal_server/app/routes/admin/v1.py</code> <pre><code>@router_admin_v1.get(\"/workflow/\", response_model=list[WorkflowReadV1])\nasync def view_workflow(\n    id: Optional[int] = None,\n    user_id: Optional[int] = None,\n    project_id: Optional[int] = None,\n    name_contains: Optional[str] = None,\n    timestamp_created_min: Optional[datetime] = None,\n    timestamp_created_max: Optional[datetime] = None,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[WorkflowReadV1]:\n    \"\"\"\n    Query `workflow` table.\n\n    Args:\n        id: If not `None`, select a given `workflow.id`.\n        project_id: If not `None`, select a given `workflow.project_id`.\n        name_contains: If not `None`, select workflows such that their\n            `name` attribute contains `name_contains` (case-insensitive).\n    \"\"\"\n    stm = select(Workflow)\n\n    if user_id is not None:\n        stm = stm.join(Project).where(\n            Project.user_list.any(User.id == user_id)\n        )\n    if id is not None:\n        stm = stm.where(Workflow.id == id)\n    if project_id is not None:\n        stm = stm.where(Workflow.project_id == project_id)\n    if name_contains is not None:\n        # SQLAlchemy2: use icontains\n        stm = stm.where(\n            func.lower(Workflow.name).contains(name_contains.lower())\n        )\n    if timestamp_created_min is not None:\n        timestamp_created_min = _convert_to_db_timestamp(timestamp_created_min)\n        stm = stm.where(Workflow.timestamp_created &gt;= timestamp_created_min)\n    if timestamp_created_max is not None:\n        timestamp_created_max = _convert_to_db_timestamp(timestamp_created_max)\n        stm = stm.where(Workflow.timestamp_created &lt;= timestamp_created_max)\n\n    res = await db.execute(stm)\n    workflow_list = res.scalars().all()\n    await db.close()\n\n    return workflow_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/","title":"v2","text":"<p>Definition of <code>/admin</code> routes.</p>"},{"location":"reference/fractal_server/app/routes/admin/v2/#fractal_server.app.routes.admin.v2._convert_to_db_timestamp","title":"<code>_convert_to_db_timestamp(dt)</code>","text":"<p>This function takes a timezone-aware datetime and converts it to UTC. If using SQLite, it also removes the timezone information in order to make the datetime comparable with datetimes in the database.</p> Source code in <code>fractal_server/app/routes/admin/v2.py</code> <pre><code>def _convert_to_db_timestamp(dt: datetime) -&gt; datetime:\n    \"\"\"\n    This function takes a timezone-aware datetime and converts it to UTC.\n    If using SQLite, it also removes the timezone information in order to make\n    the datetime comparable with datetimes in the database.\n    \"\"\"\n    if dt.tzinfo is None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"The timestamp provided has no timezone information: {dt}\",\n        )\n    _dt = dt.astimezone(timezone.utc)\n    if Inject(get_settings).DB_ENGINE == \"sqlite\":\n        return _dt.replace(tzinfo=None)\n    return _dt\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/#fractal_server.app.routes.admin.v2.download_job_logs","title":"<code>download_job_logs(job_id, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Download job folder</p> Source code in <code>fractal_server/app/routes/admin/v2.py</code> <pre><code>@router_admin_v2.get(\n    \"/job/{job_id}/download/\",\n    response_class=StreamingResponse,\n)\nasync def download_job_logs(\n    job_id: int,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Download job folder\n    \"\"\"\n    # Get job from DB\n    job = await db.get(JobV2, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n    # Create and return byte stream for zipped log folder\n    PREFIX_ZIP = Path(job.working_dir).name\n    zip_filename = f\"{PREFIX_ZIP}_archive.zip\"\n    byte_stream = _zip_folder_to_byte_stream(\n        folder=job.working_dir, zip_filename=zip_filename\n    )\n    return StreamingResponse(\n        iter([byte_stream.getvalue()]),\n        media_type=\"application/x-zip-compressed\",\n        headers={\"Content-Disposition\": f\"attachment;filename={zip_filename}\"},\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/#fractal_server.app.routes.admin.v2.stop_job","title":"<code>stop_job(job_id, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Stop execution of a workflow job.</p> <p>Only available for slurm backend.</p> Source code in <code>fractal_server/app/routes/admin/v2.py</code> <pre><code>@router_admin_v2.get(\"/job/{job_id}/stop/\", status_code=202)\nasync def stop_job(\n    job_id: int,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Stop execution of a workflow job.\n\n    Only available for slurm backend.\n    \"\"\"\n\n    _check_backend_is_slurm()\n\n    job = await db.get(JobV2, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n\n    _write_shutdown_file(job=job)\n\n    return Response(status_code=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/#fractal_server.app.routes.admin.v2.update_job","title":"<code>update_job(job_update, job_id, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Change the status of an existing job.</p> <p>This endpoint is only open to superusers, and it does not apply project-based access-control to jobs.</p> Source code in <code>fractal_server/app/routes/admin/v2.py</code> <pre><code>@router_admin_v2.patch(\n    \"/job/{job_id}/\",\n    response_model=JobReadV2,\n)\nasync def update_job(\n    job_update: JobUpdateV2,\n    job_id: int,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[JobReadV2]:\n    \"\"\"\n    Change the status of an existing job.\n\n    This endpoint is only open to superusers, and it does not apply\n    project-based access-control to jobs.\n    \"\"\"\n    job = await db.get(JobV2, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n\n    if job_update.status != JobStatusTypeV2.FAILED:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Cannot set job status to {job_update.status}\",\n        )\n\n    setattr(job, \"status\", job_update.status)\n    setattr(job, \"end_timestamp\", get_timestamp())\n    await db.commit()\n    await db.refresh(job)\n    await db.close()\n    return job\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/#fractal_server.app.routes.admin.v2.view_job","title":"<code>view_job(id=None, user_id=None, project_id=None, dataset_id=None, workflow_id=None, status=None, start_timestamp_min=None, start_timestamp_max=None, end_timestamp_min=None, end_timestamp_max=None, log=True, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>ApplyWorkflow</code> table.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.id</code>.</p> <code>None</code> <code>project_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.project_id</code>.</p> <code>None</code> <code>dataset_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.input_dataset_id</code>.</p> <code>None</code> <code>workflow_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.workflow_id</code>.</p> <code>None</code> <code>status</code> <code>Optional[JobStatusTypeV2]</code> <p>If not <code>None</code>, select a given <code>applyworkflow.status</code>.</p> <code>None</code> <code>start_timestamp_min</code> <code>Optional[datetime]</code> <p>If not <code>None</code>, select a rows with <code>start_timestamp</code> after <code>start_timestamp_min</code>.</p> <code>None</code> <code>start_timestamp_max</code> <code>Optional[datetime]</code> <p>If not <code>None</code>, select a rows with <code>start_timestamp</code> before <code>start_timestamp_min</code>.</p> <code>None</code> <code>end_timestamp_min</code> <code>Optional[datetime]</code> <p>If not <code>None</code>, select a rows with <code>end_timestamp</code> after <code>end_timestamp_min</code>.</p> <code>None</code> <code>end_timestamp_max</code> <code>Optional[datetime]</code> <p>If not <code>None</code>, select a rows with <code>end_timestamp</code> before <code>end_timestamp_min</code>.</p> <code>None</code> <code>log</code> <code>bool</code> <p>If <code>True</code>, include <code>job.log</code>, if <code>False</code> <code>job.log</code> is set to <code>None</code>.</p> <code>True</code> Source code in <code>fractal_server/app/routes/admin/v2.py</code> <pre><code>@router_admin_v2.get(\"/job/\", response_model=list[JobReadV2])\nasync def view_job(\n    id: Optional[int] = None,\n    user_id: Optional[int] = None,\n    project_id: Optional[int] = None,\n    dataset_id: Optional[int] = None,\n    workflow_id: Optional[int] = None,\n    status: Optional[JobStatusTypeV2] = None,\n    start_timestamp_min: Optional[datetime] = None,\n    start_timestamp_max: Optional[datetime] = None,\n    end_timestamp_min: Optional[datetime] = None,\n    end_timestamp_max: Optional[datetime] = None,\n    log: bool = True,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[JobReadV2]:\n    \"\"\"\n    Query `ApplyWorkflow` table.\n\n    Args:\n        id: If not `None`, select a given `applyworkflow.id`.\n        project_id: If not `None`, select a given `applyworkflow.project_id`.\n        dataset_id: If not `None`, select a given\n            `applyworkflow.input_dataset_id`.\n        workflow_id: If not `None`, select a given `applyworkflow.workflow_id`.\n        status: If not `None`, select a given `applyworkflow.status`.\n        start_timestamp_min: If not `None`, select a rows with\n            `start_timestamp` after `start_timestamp_min`.\n        start_timestamp_max: If not `None`, select a rows with\n            `start_timestamp` before `start_timestamp_min`.\n        end_timestamp_min: If not `None`, select a rows with `end_timestamp`\n            after `end_timestamp_min`.\n        end_timestamp_max: If not `None`, select a rows with `end_timestamp`\n            before `end_timestamp_min`.\n        log: If `True`, include `job.log`, if `False`\n            `job.log` is set to `None`.\n    \"\"\"\n    stm = select(JobV2)\n\n    if id is not None:\n        stm = stm.where(JobV2.id == id)\n    if user_id is not None:\n        stm = stm.join(ProjectV2).where(\n            ProjectV2.user_list.any(User.id == user_id)\n        )\n    if project_id is not None:\n        stm = stm.where(JobV2.project_id == project_id)\n    if dataset_id is not None:\n        stm = stm.where(JobV2.dataset_id == dataset_id)\n    if workflow_id is not None:\n        stm = stm.where(JobV2.workflow_id == workflow_id)\n    if status is not None:\n        stm = stm.where(JobV2.status == status)\n    if start_timestamp_min is not None:\n        start_timestamp_min = _convert_to_db_timestamp(start_timestamp_min)\n        stm = stm.where(JobV2.start_timestamp &gt;= start_timestamp_min)\n    if start_timestamp_max is not None:\n        start_timestamp_max = _convert_to_db_timestamp(start_timestamp_max)\n        stm = stm.where(JobV2.start_timestamp &lt;= start_timestamp_max)\n    if end_timestamp_min is not None:\n        end_timestamp_min = _convert_to_db_timestamp(end_timestamp_min)\n        stm = stm.where(JobV2.end_timestamp &gt;= end_timestamp_min)\n    if end_timestamp_max is not None:\n        end_timestamp_max = _convert_to_db_timestamp(end_timestamp_max)\n        stm = stm.where(JobV2.end_timestamp &lt;= end_timestamp_max)\n\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    await db.close()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/#fractal_server.app.routes.admin.v2.view_project","title":"<code>view_project(id=None, user_id=None, user=Depends(current_active_superuser), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>ProjectV2</code> table.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>project.id</code>.</p> <code>None</code> <code>user_id</code> <code>Optional[int]</code> <p>If not <code>None</code>, select a given <code>project.user_id</code>.</p> <code>None</code> Source code in <code>fractal_server/app/routes/admin/v2.py</code> <pre><code>@router_admin_v2.get(\"/project/\", response_model=list[ProjectReadV2])\nasync def view_project(\n    id: Optional[int] = None,\n    user_id: Optional[int] = None,\n    user: User = Depends(current_active_superuser),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProjectReadV2]:\n    \"\"\"\n    Query `ProjectV2` table.\n\n    Args:\n        id: If not `None`, select a given `project.id`.\n        user_id: If not `None`, select a given `project.user_id`.\n    \"\"\"\n\n    stm = select(ProjectV2)\n\n    if id is not None:\n        stm = stm.where(ProjectV2.id == id)\n    if user_id is not None:\n        stm = stm.where(ProjectV2.user_list.any(User.id == user_id))\n\n    res = await db.execute(stm)\n    project_list = res.scalars().all()\n    await db.close()\n\n    return project_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/","title":"api","text":"<p><code>api</code> module</p>"},{"location":"reference/fractal_server/app/routes/api/v1/","title":"v1","text":"<p><code>api/v1</code> module</p>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/","title":"_aux_functions","text":"<p>Auxiliary functions to get object from the database or perform simple checks</p>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._check_project_exists","title":"<code>_check_project_exists(*, project_name, user_id, db)</code>  <code>async</code>","text":"<p>Check that no other project with this name exists for this user.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Project name</p> required <code>user_id</code> <code>int</code> <p>User ID</p> required <code>db</code> <code>AsyncSession</code> required <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If such a project already exists</p> Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>async def _check_project_exists(\n    *,\n    project_name: str,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Check that no other project with this name exists for this user.\n\n    Args:\n        project_name: Project name\n        user_id: User ID\n        db:\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If such a project already exists\n    \"\"\"\n    stm = (\n        select(Project)\n        .join(LinkUserProject)\n        .where(Project.name == project_name)\n        .where(LinkUserProject.user_id == user_id)\n    )\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Project name ({project_name}) already in use\",\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._check_workflow_exists","title":"<code>_check_workflow_exists(*, name, project_id, db)</code>  <code>async</code>","text":"<p>Check that no other workflow of this project has the same name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Workflow name</p> required <code>project_id</code> <code>int</code> <p>Project ID</p> required <code>db</code> <code>AsyncSession</code> required <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If such a workflow already exists</p> Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>async def _check_workflow_exists(\n    *,\n    name: str,\n    project_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Check that no other workflow of this project has the same name.\n\n    Args:\n        name: Workflow name\n        project_id: Project ID\n        db:\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If such a workflow already exists\n    \"\"\"\n    stm = (\n        select(Workflow)\n        .where(Workflow.name == name)\n        .where(Workflow.project_id == project_id)\n    )\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Workflow with {name=} and {project_id=} already exists.\",\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._get_dataset_check_owner","title":"<code>_get_dataset_check_owner(*, project_id, dataset_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a dataset and a project, after access control on the project</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>dataset_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>dict[Literal['dataset', 'project'], Union[Dataset, Project]]</code> <p>Dictionary with the dataset and project objects (keys: <code>dataset</code>, <code>project</code>).</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the dataset is not associated to the project</p> Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>async def _get_dataset_check_owner(\n    *,\n    project_id: int,\n    dataset_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; dict[Literal[\"dataset\", \"project\"], Union[Dataset, Project]]:\n    \"\"\"\n    Get a dataset and a project, after access control on the project\n\n    Args:\n        project_id:\n        dataset_id:\n        user_id:\n        db:\n\n    Returns:\n        Dictionary with the dataset and project objects (keys: `dataset`,\n            `project`).\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the dataset is not associated to the project\n    \"\"\"\n\n    # Access control for project\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user_id, db=db\n    )\n    # Get dataset\n    dataset = await db.get(Dataset, dataset_id)\n    if not dataset:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Dataset not found\"\n        )\n    if dataset.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Invalid {project_id=} for {dataset_id=}\",\n        )\n\n    # Refresh so that dataset.project relationship is loaded (see discussion\n    # in issue #1063)\n    await db.refresh(dataset)\n\n    return dict(dataset=dataset, project=project)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._get_job_check_owner","title":"<code>_get_job_check_owner(*, project_id, job_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a job and a project, after access control on the project</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>job_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>dict[Literal['job', 'project'], Union[ApplyWorkflow, Project]]</code> <p>Dictionary with the job and project objects (keys: <code>job</code>, <code>project</code>).</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the job is not associated to the project</p> Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>async def _get_job_check_owner(\n    *,\n    project_id: int,\n    job_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; dict[Literal[\"job\", \"project\"], Union[ApplyWorkflow, Project]]:\n    \"\"\"\n    Get a job and a project, after access control on the project\n\n    Args:\n        project_id:\n        job_id:\n        user_id:\n        db:\n\n    Returns:\n        Dictionary with the job and project objects (keys: `job`,\n            `project`).\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the job is not associated to the project\n    \"\"\"\n    # Access control for project\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user_id, db=db\n    )\n    # Get dataset\n    job = await db.get(ApplyWorkflow, job_id)\n    if not job:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Job not found\"\n        )\n    if job.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Invalid {project_id=} for {job_id=}\",\n        )\n    return dict(job=job, project=project)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._get_project_check_owner","title":"<code>_get_project_check_owner(*, project_id, user_id, db)</code>  <code>async</code>","text":"<p>Check that user is a member of project and return the project.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>Project</code> <p>The project object</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=403_FORBIDDEN)</code> <p>If the user is not a member of the project</p> <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the project does not exist</p> Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>async def _get_project_check_owner(\n    *,\n    project_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; Project:\n    \"\"\"\n    Check that user is a member of project and return the project.\n\n    Args:\n        project_id:\n        user_id:\n        db:\n\n    Returns:\n        The project object\n\n    Raises:\n        HTTPException(status_code=403_FORBIDDEN):\n            If the user is not a member of the project\n        HTTPException(status_code=404_NOT_FOUND):\n            If the project does not exist\n    \"\"\"\n    project = await db.get(Project, project_id)\n    link_user_project = await db.get(LinkUserProject, (project_id, user_id))\n    if not project:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Project not found\"\n        )\n    if not link_user_project:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=f\"Not allowed on project {project_id}\",\n        )\n    return project\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._get_submitted_jobs_statement","title":"<code>_get_submitted_jobs_statement()</code>","text":"<p>Returns:</p> Type Description <code>SelectOfScalar</code> <p>A sqlmodel statement that selects all <code>ApplyWorkflow</code>s with</p> <code>SelectOfScalar</code> <p><code>ApplyWorkflow.status</code> equal to <code>submitted</code>.</p> Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>def _get_submitted_jobs_statement() -&gt; SelectOfScalar:\n    \"\"\"\n    Returns:\n        A sqlmodel statement that selects all `ApplyWorkflow`s with\n        `ApplyWorkflow.status` equal to `submitted`.\n    \"\"\"\n    stm = select(ApplyWorkflow).where(\n        ApplyWorkflow.status == JobStatusTypeV1.SUBMITTED\n    )\n    return stm\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._get_task_check_owner","title":"<code>_get_task_check_owner(*, task_id, user, db)</code>  <code>async</code>","text":"<p>Get a task, after access control.</p> <p>This check constitutes a preliminary version of access control: if the current user is not a superuser and differs from the task owner (including when <code>owner is None</code>), we raise an 403 HTTP Exception.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> required <code>user</code> <code>User</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>Task</code> <p>The task object.</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the task does not exist</p> <code>HTTPException(status_code=403_FORBIDDEN)</code> <p>If the user does not have rights to edit this task.</p> Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>async def _get_task_check_owner(\n    *,\n    task_id: int,\n    user: User,\n    db: AsyncSession,\n) -&gt; Task:\n    \"\"\"\n    Get a task, after access control.\n\n    This check constitutes a preliminary version of access control:\n    if the current user is not a superuser and differs from the task owner\n    (including when `owner is None`), we raise an 403 HTTP Exception.\n\n    Args:\n        task_id:\n        user:\n        db:\n\n    Returns:\n        The task object.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the task does not exist\n        HTTPException(status_code=403_FORBIDDEN):\n            If the user does not have rights to edit this task.\n    \"\"\"\n    task = await db.get(Task, task_id)\n    if not task:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Task {task_id} not found.\",\n        )\n\n    if not user.is_superuser:\n        if task.owner is None:\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN,\n                detail=(\n                    \"Only a superuser can modify a Task with `owner=None`.\"\n                ),\n            )\n        else:\n            owner = user.username or user.slurm_user\n            if owner != task.owner:\n                raise HTTPException(\n                    status_code=status.HTTP_403_FORBIDDEN,\n                    detail=(\n                        f\"Current user ({owner}) cannot modify Task {task.id} \"\n                        f\"with different owner ({task.owner}).\"\n                    ),\n                )\n    return task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._get_workflow_check_owner","title":"<code>_get_workflow_check_owner(*, workflow_id, project_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a workflow and a project, after access control on the project.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>int</code> required <code>project_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>Workflow</code> <p>The workflow object.</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the workflow does not exist</p> <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the workflow is not associated to the project</p> Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>async def _get_workflow_check_owner(\n    *,\n    workflow_id: int,\n    project_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; Workflow:\n    \"\"\"\n    Get a workflow and a project, after access control on the project.\n\n    Args:\n        workflow_id:\n        project_id:\n        user_id:\n        db:\n\n    Returns:\n        The workflow object.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the workflow does not exist\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the workflow is not associated to the project\n    \"\"\"\n\n    # Access control for project\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user_id, db=db\n    )\n    # Get workflow\n    workflow = await db.get(Workflow, workflow_id)\n    if not workflow:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Workflow not found\"\n        )\n    if workflow.project_id != project.id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(f\"Invalid {project_id=} for {workflow_id=}.\"),\n        )\n\n    # Refresh so that workflow.project relationship is loaded (see discussion\n    # in issue #1063)\n    await db.refresh(workflow)\n\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._get_workflow_task_check_owner","title":"<code>_get_workflow_task_check_owner(*, project_id, workflow_id, workflow_task_id, user_id, db)</code>  <code>async</code>","text":"<p>Check that user has access to Workflow and WorkflowTask.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>workflow_id</code> <code>int</code> required <code>workflow_task_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>tuple[WorkflowTask, Workflow]</code> <p>Tuple of WorkflowTask and Workflow objects.</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the WorkflowTask does not exist</p> <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the WorkflowTask is not associated to the Workflow</p> Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>async def _get_workflow_task_check_owner(\n    *,\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; tuple[WorkflowTask, Workflow]:\n    \"\"\"\n    Check that user has access to Workflow and WorkflowTask.\n\n    Args:\n        project_id:\n        workflow_id:\n        workflow_task_id:\n        user_id:\n        db:\n\n    Returns:\n        Tuple of WorkflowTask and Workflow objects.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the WorkflowTask does not exist\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the WorkflowTask is not associated to the Workflow\n    \"\"\"\n\n    # Access control for workflow\n    workflow = await _get_workflow_check_owner(\n        workflow_id=workflow_id, project_id=project_id, user_id=user_id, db=db\n    )\n\n    # If WorkflowTask is not in the db, exit\n    workflow_task = await db.get(WorkflowTask, workflow_task_id)\n    if not workflow_task:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"WorkflowTask not found\",\n        )\n\n    # If WorkflowTask is not part of the expected Workflow, exit\n    if workflow_id != workflow_task.workflow_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Invalid {workflow_id=} for {workflow_task_id=}\",\n        )\n\n    return workflow_task, workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/_aux_functions/#fractal_server.app.routes.api.v1._aux_functions._workflow_insert_task","title":"<code>_workflow_insert_task(*, workflow_id, task_id, args=None, meta=None, order=None, db)</code>  <code>async</code>","text":"<p>Insert a new WorkflowTask into Workflow.task_list</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> <p>TBD</p> required <code>args</code> <code>Optional[dict[str, Any]]</code> <p>TBD</p> <code>None</code> <code>meta</code> <code>Optional[dict[str, Any]]</code> <p>TBD</p> <code>None</code> <code>order</code> <code>Optional[int]</code> <p>TBD</p> <code>None</code> <code>db</code> <code>AsyncSession</code> <p>TBD</p> required Source code in <code>fractal_server/app/routes/api/v1/_aux_functions.py</code> <pre><code>async def _workflow_insert_task(\n    *,\n    workflow_id: int,\n    task_id: int,\n    args: Optional[dict[str, Any]] = None,\n    meta: Optional[dict[str, Any]] = None,\n    order: Optional[int] = None,\n    db: AsyncSession,\n) -&gt; WorkflowTask:\n    \"\"\"\n    Insert a new WorkflowTask into Workflow.task_list\n\n    Args:\n        task_id: TBD\n        args: TBD\n        meta: TBD\n        order: TBD\n        db: TBD\n    \"\"\"\n    db_workflow = await db.get(Workflow, workflow_id)\n    if db_workflow is None:\n        raise ValueError(f\"Workflow {workflow_id} does not exist\")\n\n    if order is None:\n        order = len(db_workflow.task_list)\n\n    # Get task from db, and extract default arguments via a Task property\n    # method\n    db_task = await db.get(Task, task_id)\n    if db_task is None:\n        raise ValueError(f\"Task {task_id} does not exist\")\n\n    default_args = db_task.default_args_from_args_schema\n    # Override default_args with args\n    actual_args = default_args.copy()\n    if args is not None:\n        for k, v in args.items():\n            actual_args[k] = v\n    if not actual_args:\n        actual_args = None\n\n    # Combine meta (higher priority) and db_task.meta (lower priority)\n    wt_meta = (db_task.meta or {}).copy()\n    wt_meta.update(meta or {})\n    if not wt_meta:\n        wt_meta = None\n\n    # Create DB entry\n    wf_task = WorkflowTask(task_id=task_id, args=actual_args, meta=wt_meta)\n    db.add(wf_task)\n    db_workflow.task_list.insert(order, wf_task)\n    db_workflow.task_list.reorder()  # type: ignore\n    await db.commit()\n    await db.refresh(wf_task)\n\n    return wf_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/","title":"dataset","text":""},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.create_dataset","title":"<code>create_dataset(project_id, dataset, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add new dataset to current project</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/dataset/\",\n    response_model=DatasetReadV1,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_dataset(\n    project_id: int,\n    dataset: DatasetCreateV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[DatasetReadV1]:\n    \"\"\"\n    Add new dataset to current project\n    \"\"\"\n    await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    db_dataset = Dataset(project_id=project_id, **dataset.dict())\n    db.add(db_dataset)\n    await db.commit()\n    await db.refresh(db_dataset)\n    await db.close()\n\n    return db_dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.create_resource","title":"<code>create_resource(project_id, dataset_id, resource, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add resource to an existing dataset</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/dataset/{dataset_id}/resource/\",\n    response_model=ResourceReadV1,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_resource(\n    project_id: int,\n    dataset_id: int,\n    resource: ResourceCreateV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[ResourceReadV1]:\n    \"\"\"\n    Add resource to an existing dataset\n    \"\"\"\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n    db_resource = Resource(dataset_id=dataset.id, **resource.dict())\n    db.add(db_resource)\n    await db.commit()\n    await db.refresh(db_resource)\n    await db.close()\n    return db_resource\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.delete_dataset","title":"<code>delete_dataset(project_id, dataset_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    status_code=204,\n)\nasync def delete_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a dataset associated to the current project\n    \"\"\"\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current dataset.\n    stm = _get_submitted_jobs_statement().where(\n        or_(\n            ApplyWorkflow.input_dataset_id == dataset_id,\n            ApplyWorkflow.output_dataset_id == dataset_id,\n        )\n    )\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot delete dataset {dataset.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Cascade operations: set foreign-keys to null for jobs which are in\n    # relationship with the current dataset\n    # input_dataset\n    stm = select(ApplyWorkflow).where(\n        ApplyWorkflow.input_dataset_id == dataset_id\n    )\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    for job in jobs:\n        job.input_dataset_id = None\n        await db.merge(job)\n    await db.commit()\n    # output_dataset\n    stm = select(ApplyWorkflow).where(\n        ApplyWorkflow.output_dataset_id == dataset_id\n    )\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    for job in jobs:\n        job.output_dataset_id = None\n        await db.merge(job)\n    await db.commit()\n\n    # Delete dataset\n    await db.delete(dataset)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.delete_resource","title":"<code>delete_resource(project_id, dataset_id, resource_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a resource of a dataset</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/dataset/{dataset_id}/resource/{resource_id}/\",\n    status_code=204,\n)\nasync def delete_resource(\n    project_id: int,\n    dataset_id: int,\n    resource_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a resource of a dataset\n    \"\"\"\n    # Get the dataset DB entry\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n    resource = await db.get(Resource, resource_id)\n    if not resource or resource.dataset_id != dataset.id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"Resource does not exist or does not belong to dataset\",\n        )\n    await db.delete(resource)\n    await db.commit()\n    await db.close()\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.export_history_as_workflow","title":"<code>export_history_as_workflow(project_id, dataset_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Extract a reproducible workflow from the dataset history.</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/export_history/\",\n    response_model=WorkflowExportV1,\n)\nasync def export_history_as_workflow(\n    project_id: int,\n    dataset_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowExportV1]:\n    \"\"\"\n    Extract a reproducible workflow from the dataset history.\n    \"\"\"\n    # Get the dataset DB entry\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n\n    # Check whether there exists a submitted job such that\n    # `job.output_dataset_id==dataset_id`.\n    # If at least one such job exists, then this endpoint will fail.\n    # We do not support the use case of exporting a reproducible workflow when\n    # job execution is in progress; this may change in the future.\n    stm = _get_submitted_jobs_statement().where(\n        ApplyWorkflow.output_dataset_id == dataset_id\n    )\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot export history because dataset {dataset.id} \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # It such a job does not exist, continue with the endpoint. Note that this\n    # means that the history in the DB is up-to-date.\n\n    # Read history from DB\n    history = dataset.history\n\n    # Construct reproducible workflow\n    task_list = []\n    for history_item in history:\n        wftask = history_item[\"workflowtask\"]\n        wftask_status = history_item[\"status\"]\n        if wftask_status == \"done\":\n            task_list.append(WorkflowTaskExportV1(**wftask))\n\n    def _slugify_dataset_name(_name: str) -&gt; str:\n        _new_name = _name\n        for char in (\" \", \".\", \"/\", \"\\\\\"):\n            _new_name = _new_name.replace(char, \"_\")\n        return _new_name\n\n    name = f\"history_{_slugify_dataset_name(dataset.name)}\"\n\n    workflow = WorkflowExportV1(name=name, task_list=task_list)\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.get_resource_list","title":"<code>get_resource_list(project_id, dataset_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get resources from a dataset</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/resource/\",\n    response_model=list[ResourceReadV1],\n)\nasync def get_resource_list(\n    project_id: int,\n    dataset_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[list[ResourceReadV1]]:\n    \"\"\"\n    Get resources from a dataset\n    \"\"\"\n    await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    stm = select(Resource).where(Resource.dataset_id == dataset_id)\n    res = await db.execute(stm)\n    resource_list = res.scalars().all()\n    await db.close()\n    return resource_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.get_user_datasets","title":"<code>get_user_datasets(history=True, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the datasets of the current user</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.get(\"/dataset/\", response_model=list[DatasetReadV1])\nasync def get_user_datasets(\n    history: bool = True,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[DatasetReadV1]:\n    \"\"\"\n    Returns all the datasets of the current user\n    \"\"\"\n    stm = select(Dataset)\n    stm = stm.join(Project).where(Project.user_list.any(User.id == user.id))\n    res = await db.execute(stm)\n    dataset_list = res.scalars().all()\n    await db.close()\n    if not history:\n        for ds in dataset_list:\n            setattr(ds, \"history\", [])\n    return dataset_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.get_workflowtask_status","title":"<code>get_workflowtask_status(project_id, dataset_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Extract the status of all <code>WorkflowTask</code>s that ran on a given <code>Dataset</code>.</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/status/\",\n    response_model=DatasetStatusReadV1,\n)\nasync def get_workflowtask_status(\n    project_id: int,\n    dataset_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[DatasetStatusReadV1]:\n    \"\"\"\n    Extract the status of all `WorkflowTask`s that ran on a given `Dataset`.\n    \"\"\"\n    # Get the dataset DB entry\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n\n    # Check whether there exists a job such that\n    # 1. `job.output_dataset_id == dataset_id`, and\n    # 2. `job.status` is either submitted or running.\n    # If one such job exists, it will be used later. If there are multiple\n    # jobs, raise an error.\n    # Note: see\n    # https://sqlmodel.tiangolo.com/tutorial/where/#type-annotations-and-errors\n    # regarding the type-ignore in this code block\n    stm = _get_submitted_jobs_statement().where(\n        ApplyWorkflow.output_dataset_id == dataset_id\n    )\n    res = await db.execute(stm)\n    running_jobs = res.scalars().all()\n    if len(running_jobs) == 0:\n        running_job = None\n    elif len(running_jobs) == 1:\n        running_job = running_jobs[0]\n    else:\n        string_ids = str([job.id for job in running_jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot get WorkflowTask statuses as dataset {dataset.id} \"\n                f\"is linked to multiple active jobs: {string_ids}\"\n            ),\n        )\n\n    # Initialize empty dictionary for workflowtasks status\n    workflow_tasks_status_dict: dict = {}\n\n    # Lowest priority: read status from DB, which corresponds to jobs that are\n    # not running\n    history = dataset.history\n    for history_item in history:\n        wftask_id = history_item[\"workflowtask\"][\"id\"]\n        wftask_status = history_item[\"status\"]\n        workflow_tasks_status_dict[wftask_id] = wftask_status\n\n    # If a job is running, then gather more up-to-date information\n    if running_job is not None:\n        # Get the workflow DB entry\n        running_workflow = await _get_workflow_check_owner(\n            project_id=project_id,\n            workflow_id=running_job.workflow_id,\n            user_id=user.id,\n            db=db,\n        )\n        # Mid priority: Set all WorkflowTask's that are part of the running job\n        # as \"submitted\"\n        start = running_job.first_task_index\n        end = running_job.last_task_index + 1\n        for wftask in running_workflow.task_list[start:end]:\n            workflow_tasks_status_dict[wftask.id] = \"submitted\"\n\n        # Highest priority: Read status updates coming from the running-job\n        # temporary file. Note: this file only contains information on\n        # WorkflowTask's that ran through successfully\n        tmp_file = Path(running_job.working_dir) / HISTORY_FILENAME\n        try:\n            with tmp_file.open(\"r\") as f:\n                history = json.load(f)\n        except FileNotFoundError:\n            history = []\n        except JSONDecodeError:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=\"History file does not include a valid JSON.\",\n            )\n\n        for history_item in history:\n            wftask_id = history_item[\"workflowtask\"][\"id\"]\n            wftask_status = history_item[\"status\"]\n            workflow_tasks_status_dict[wftask_id] = wftask_status\n\n    response_body = DatasetStatusReadV1(status=workflow_tasks_status_dict)\n    return response_body\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.read_dataset","title":"<code>read_dataset(project_id, dataset_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    response_model=DatasetReadV1,\n)\nasync def read_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[DatasetReadV1]:\n    \"\"\"\n    Get info on a dataset associated to the current project\n    \"\"\"\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n    await db.close()\n    return dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.read_dataset_list","title":"<code>read_dataset_list(project_id, history=True, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get dataset list for given project</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/\",\n    response_model=list[DatasetReadV1],\n)\nasync def read_dataset_list(\n    project_id: int,\n    history: bool = True,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[list[DatasetReadV1]]:\n    \"\"\"\n    Get dataset list for given project\n    \"\"\"\n    # Access control\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    # Find datasets of the current project. Note: this select/where approach\n    # has much better scaling than refreshing all elements of\n    # `project.dataset_list` - ref\n    # https://github.com/fractal-analytics-platform/fractal-server/pull/1082#issuecomment-1856676097.\n    stm = select(Dataset).where(Dataset.project_id == project.id)\n    res = await db.execute(stm)\n    dataset_list = res.scalars().all()\n    await db.close()\n    if not history:\n        for ds in dataset_list:\n            setattr(ds, \"history\", [])\n    return dataset_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.update_dataset","title":"<code>update_dataset(project_id, dataset_id, dataset_update, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    response_model=DatasetReadV1,\n)\nasync def update_dataset(\n    project_id: int,\n    dataset_id: int,\n    dataset_update: DatasetUpdateV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[DatasetReadV1]:\n    \"\"\"\n    Edit a dataset associated to the current project\n    \"\"\"\n\n    if dataset_update.history is not None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"Cannot modify dataset history.\",\n        )\n\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    db_dataset = output[\"dataset\"]\n\n    for key, value in dataset_update.dict(exclude_unset=True).items():\n        setattr(db_dataset, key, value)\n\n    await db.commit()\n    await db.refresh(db_dataset)\n    await db.close()\n    return db_dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/dataset/#fractal_server.app.routes.api.v1.dataset.update_resource","title":"<code>update_resource(project_id, dataset_id, resource_id, resource_update, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a resource of a dataset</p> Source code in <code>fractal_server/app/routes/api/v1/dataset.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/dataset/{dataset_id}/resource/{resource_id}/\",\n    response_model=ResourceReadV1,\n)\nasync def update_resource(\n    project_id: int,\n    dataset_id: int,\n    resource_id: int,\n    resource_update: ResourceUpdateV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[ResourceReadV1]:\n    \"\"\"\n    Edit a resource of a dataset\n    \"\"\"\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n    orig_resource = await db.get(Resource, resource_id)\n\n    if orig_resource not in dataset.resource_list:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Resource {resource_id} is not part of \"\n                f\"dataset {dataset_id}\"\n            ),\n        )\n\n    for key, value in resource_update.dict(exclude_unset=True).items():\n        setattr(orig_resource, key, value)\n    await db.commit()\n    await db.refresh(orig_resource)\n    await db.close()\n    return orig_resource\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/job/","title":"job","text":""},{"location":"reference/fractal_server/app/routes/api/v1/job/#fractal_server.app.routes.api.v1.job.download_job_logs","title":"<code>download_job_logs(project_id, job_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Download job folder</p> Source code in <code>fractal_server/app/routes/api/v1/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/download/\",\n    response_class=StreamingResponse,\n)\nasync def download_job_logs(\n    project_id: int,\n    job_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Download job folder\n    \"\"\"\n    output = await _get_job_check_owner(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        db=db,\n    )\n    job = output[\"job\"]\n\n    # Create and return byte stream for zipped log folder\n    PREFIX_ZIP = Path(job.working_dir).name\n    zip_filename = f\"{PREFIX_ZIP}_archive.zip\"\n    byte_stream = _zip_folder_to_byte_stream(\n        folder=job.working_dir, zip_filename=zip_filename\n    )\n    return StreamingResponse(\n        iter([byte_stream.getvalue()]),\n        media_type=\"application/x-zip-compressed\",\n        headers={\"Content-Disposition\": f\"attachment;filename={zip_filename}\"},\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/job/#fractal_server.app.routes.api.v1.job.get_job_list","title":"<code>get_job_list(project_id, user=Depends(current_active_user), log=True, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get job list for given project</p> Source code in <code>fractal_server/app/routes/api/v1/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/\",\n    response_model=list[ApplyWorkflowReadV1],\n)\nasync def get_job_list(\n    project_id: int,\n    user: User = Depends(current_active_user),\n    log: bool = True,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[list[ApplyWorkflowReadV1]]:\n    \"\"\"\n    Get job list for given project\n    \"\"\"\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n\n    stm = select(ApplyWorkflow).where(ApplyWorkflow.project_id == project.id)\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    await db.close()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/job/#fractal_server.app.routes.api.v1.job.get_user_jobs","title":"<code>get_user_jobs(user=Depends(current_active_user), log=True, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the jobs of the current user</p> Source code in <code>fractal_server/app/routes/api/v1/job.py</code> <pre><code>@router.get(\"/job/\", response_model=list[ApplyWorkflowReadV1])\nasync def get_user_jobs(\n    user: User = Depends(current_active_user),\n    log: bool = True,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ApplyWorkflowReadV1]:\n    \"\"\"\n    Returns all the jobs of the current user\n    \"\"\"\n    stm = select(ApplyWorkflow)\n    stm = stm.join(Project).where(Project.user_list.any(User.id == user.id))\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    await db.close()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/job/#fractal_server.app.routes.api.v1.job.get_workflow_jobs","title":"<code>get_workflow_jobs(project_id, workflow_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the jobs related to a specific workflow</p> Source code in <code>fractal_server/app/routes/api/v1/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/job/\",\n    response_model=list[ApplyWorkflowReadV1],\n)\nasync def get_workflow_jobs(\n    project_id: int,\n    workflow_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[list[ApplyWorkflowReadV1]]:\n    \"\"\"\n    Returns all the jobs related to a specific workflow\n    \"\"\"\n    await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n    stm = select(ApplyWorkflow).where(ApplyWorkflow.workflow_id == workflow_id)\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/job/#fractal_server.app.routes.api.v1.job.read_job","title":"<code>read_job(project_id, job_id, show_tmp_logs=False, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return info on an existing job</p> Source code in <code>fractal_server/app/routes/api/v1/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/\",\n    response_model=ApplyWorkflowReadV1,\n)\nasync def read_job(\n    project_id: int,\n    job_id: int,\n    show_tmp_logs: bool = False,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[ApplyWorkflowReadV1]:\n    \"\"\"\n    Return info on an existing job\n    \"\"\"\n\n    output = await _get_job_check_owner(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        db=db,\n    )\n    job = output[\"job\"]\n    await db.close()\n\n    if show_tmp_logs and (job.status == JobStatusTypeV1.SUBMITTED):\n        try:\n            with open(f\"{job.working_dir}/{WORKFLOW_LOG_FILENAME}\", \"r\") as f:\n                job.log = f.read()\n        except FileNotFoundError:\n            pass\n\n    return job\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/job/#fractal_server.app.routes.api.v1.job.stop_job","title":"<code>stop_job(project_id, job_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Stop execution of a workflow job (only available for slurm backend)</p> Source code in <code>fractal_server/app/routes/api/v1/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/stop/\",\n    status_code=202,\n)\nasync def stop_job(\n    project_id: int,\n    job_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Stop execution of a workflow job (only available for slurm backend)\n    \"\"\"\n\n    # This endpoint is only implemented for SLURM backend\n    _check_backend_is_slurm()\n\n    # Get job from DB\n    output = await _get_job_check_owner(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        db=db,\n    )\n    job = output[\"job\"]\n\n    _write_shutdown_file(job=job)\n\n    return Response(status_code=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/project/","title":"project","text":""},{"location":"reference/fractal_server/app/routes/api/v1/project/#fractal_server.app.routes.api.v1.project.create_project","title":"<code>create_project(project, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create new poject</p> Source code in <code>fractal_server/app/routes/api/v1/project.py</code> <pre><code>@router.post(\"/\", response_model=ProjectReadV1, status_code=201)\nasync def create_project(\n    project: ProjectCreateV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[ProjectReadV1]:\n    \"\"\"\n    Create new poject\n    \"\"\"\n\n    # Check that there is no project with the same user and name\n    await _check_project_exists(\n        project_name=project.name, user_id=user.id, db=db\n    )\n\n    db_project = Project(**project.dict())\n    db_project.user_list.append(user)\n    try:\n        db.add(db_project)\n        await db.commit()\n        await db.refresh(db_project)\n        await db.close()\n    except IntegrityError as e:\n        await db.rollback()\n        logger = set_logger(\"create_project\")\n        logger.error(str(e))\n        close_logger(logger)\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=str(e),\n        )\n\n    return db_project\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/project/#fractal_server.app.routes.api.v1.project.delete_project","title":"<code>delete_project(project_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete project</p> Source code in <code>fractal_server/app/routes/api/v1/project.py</code> <pre><code>@router.delete(\"/{project_id}/\", status_code=204)\nasync def delete_project(\n    project_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete project\n    \"\"\"\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current project.\n    stm = _get_submitted_jobs_statement().where(\n        ApplyWorkflow.project_id == project_id\n    )\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot delete project {project.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Cascade operations\n\n    # Workflows\n    stm = select(Workflow).where(Workflow.project_id == project_id)\n    res = await db.execute(stm)\n    workflows = res.scalars().all()\n    for wf in workflows:\n        # Cascade operations: set foreign-keys to null for jobs which are in\n        # relationship with the current workflow\n        stm = select(ApplyWorkflow).where(ApplyWorkflow.workflow_id == wf.id)\n        res = await db.execute(stm)\n        jobs = res.scalars().all()\n        for job in jobs:\n            job.workflow_id = None\n            await db.merge(job)\n        await db.commit()\n        # Delete workflow\n        await db.delete(wf)\n\n    # Dataset\n    stm = select(Dataset).where(Dataset.project_id == project_id)\n    res = await db.execute(stm)\n    datasets = res.scalars().all()\n    for ds in datasets:\n        # Cascade operations: set foreign-keys to null for jobs which are in\n        # relationship with the current dataset\n        # input_dataset\n        stm = select(ApplyWorkflow).where(\n            ApplyWorkflow.input_dataset_id == ds.id\n        )\n        res = await db.execute(stm)\n        jobs = res.scalars().all()\n        for job in jobs:\n            job.input_dataset_id = None\n            await db.merge(job)\n        await db.commit()\n        # output_dataset\n        stm = select(ApplyWorkflow).where(\n            ApplyWorkflow.output_dataset_id == ds.id\n        )\n        res = await db.execute(stm)\n        jobs = res.scalars().all()\n        for job in jobs:\n            job.output_dataset_id = None\n            await db.merge(job)\n        await db.commit()\n        await db.delete(ds)\n\n    # Job\n    stm = select(ApplyWorkflow).where(ApplyWorkflow.project_id == project_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    for job in jobs:\n        job.project_id = None\n        await db.merge(job)\n\n    await db.commit()\n\n    await db.delete(project)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/project/#fractal_server.app.routes.api.v1.project.get_list_project","title":"<code>get_list_project(user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return list of projects user is member of</p> Source code in <code>fractal_server/app/routes/api/v1/project.py</code> <pre><code>@router.get(\"/\", response_model=list[ProjectReadV1])\nasync def get_list_project(\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[Project]:\n    \"\"\"\n    Return list of projects user is member of\n    \"\"\"\n    stm = (\n        select(Project)\n        .join(LinkUserProject)\n        .where(LinkUserProject.user_id == user.id)\n    )\n    res = await db.execute(stm)\n    project_list = res.scalars().all()\n    await db.close()\n    return project_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/project/#fractal_server.app.routes.api.v1.project.read_project","title":"<code>read_project(project_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return info on an existing project</p> Source code in <code>fractal_server/app/routes/api/v1/project.py</code> <pre><code>@router.get(\"/{project_id}/\", response_model=ProjectReadV1)\nasync def read_project(\n    project_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[ProjectReadV1]:\n    \"\"\"\n    Return info on an existing project\n    \"\"\"\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    await db.close()\n    return project\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/task/","title":"task","text":""},{"location":"reference/fractal_server/app/routes/api/v1/task/#fractal_server.app.routes.api.v1.task.create_task","title":"<code>create_task(task, user=Depends(current_active_verified_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create a new task</p> Source code in <code>fractal_server/app/routes/api/v1/task.py</code> <pre><code>@router.post(\n    \"/\", response_model=TaskReadV1, status_code=status.HTTP_201_CREATED\n)\nasync def create_task(\n    task: TaskCreateV1,\n    user: User = Depends(current_active_verified_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[TaskReadV1]:\n    \"\"\"\n    Create a new task\n    \"\"\"\n    # Set task.owner attribute\n    if user.username:\n        owner = user.username\n    elif user.slurm_user:\n        owner = user.slurm_user\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                \"Cannot add a new task because current user does not \"\n                \"have `username` or `slurm_user` attributes.\"\n            ),\n        )\n\n    # Prepend owner to task.source\n    task.source = f\"{owner}:{task.source}\"\n\n    # Verify that source is not already in use (note: this check is only useful\n    # to provide a user-friendly error message, but `task.source` uniqueness is\n    # already guaranteed by a constraint in the table definition).\n    stm = select(Task).where(Task.source == task.source)\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Source '{task.source}' already used by some TaskV1\",\n        )\n    stm = select(TaskV2).where(TaskV2.source == task.source)\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Source '{task.source}' already used by some TaskV2\",\n        )\n\n    # Add task\n    db_task = Task(**task.dict(), owner=owner)\n    db.add(db_task)\n    await db.commit()\n    await db.refresh(db_task)\n    await db.close()\n    return db_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/task/#fractal_server.app.routes.api.v1.task.delete_task","title":"<code>delete_task(task_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a task</p> Source code in <code>fractal_server/app/routes/api/v1/task.py</code> <pre><code>@router.delete(\"/{task_id}/\", status_code=204)\nasync def delete_task(\n    task_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a task\n    \"\"\"\n\n    db_task = await _get_task_check_owner(task_id=task_id, user=user, db=db)\n\n    # Check that the Task is not in relationship with some WorkflowTask\n    stm = select(WorkflowTask).filter(WorkflowTask.task_id == task_id)\n    res = await db.execute(stm)\n    workflowtask_list = res.scalars().all()\n    if workflowtask_list:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot remove Task {task_id} because it is currently \"\n                \"imported in Workflows \"\n                f\"{[x.workflow_id for x in workflowtask_list]}. \"\n                \"If you want to remove this task, then you should first remove\"\n                \" the workflows.\",\n            ),\n        )\n\n    await db.delete(db_task)\n    await db.commit()\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/task/#fractal_server.app.routes.api.v1.task.get_list_task","title":"<code>get_list_task(user=Depends(current_active_user), args_schema=True, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get list of available tasks</p> Source code in <code>fractal_server/app/routes/api/v1/task.py</code> <pre><code>@router.get(\"/\", response_model=list[TaskReadV1])\nasync def get_list_task(\n    user: User = Depends(current_active_user),\n    args_schema: bool = True,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[TaskReadV1]:\n    \"\"\"\n    Get list of available tasks\n    \"\"\"\n    stm = select(Task)\n    res = await db.execute(stm)\n    task_list = res.scalars().all()\n    await db.close()\n    if not args_schema:\n        for task in task_list:\n            setattr(task, \"args_schema\", None)\n\n    return task_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/task/#fractal_server.app.routes.api.v1.task.get_task","title":"<code>get_task(task_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on a specific task</p> Source code in <code>fractal_server/app/routes/api/v1/task.py</code> <pre><code>@router.get(\"/{task_id}/\", response_model=TaskReadV1)\nasync def get_task(\n    task_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskReadV1:\n    \"\"\"\n    Get info on a specific task\n    \"\"\"\n    task = await db.get(Task, task_id)\n    await db.close()\n    if not task:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Task not found\"\n        )\n    return task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/task/#fractal_server.app.routes.api.v1.task.patch_task","title":"<code>patch_task(task_id, task_update, user=Depends(current_active_verified_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a specific task (restricted to superusers and task owner)</p> Source code in <code>fractal_server/app/routes/api/v1/task.py</code> <pre><code>@router.patch(\"/{task_id}/\", response_model=TaskReadV1)\nasync def patch_task(\n    task_id: int,\n    task_update: TaskUpdateV1,\n    user: User = Depends(current_active_verified_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[TaskReadV1]:\n    \"\"\"\n    Edit a specific task (restricted to superusers and task owner)\n    \"\"\"\n\n    if task_update.source:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"patch_task endpoint cannot set `source`\",\n        )\n\n    # Retrieve task from database\n    db_task = await _get_task_check_owner(task_id=task_id, user=user, db=db)\n\n    update = task_update.dict(exclude_unset=True)\n    for key, value in update.items():\n        if isinstance(value, str) or (\n            key == \"version\" and value is None\n        ):  # special case (issue 817)\n            setattr(db_task, key, value)\n        elif isinstance(value, dict):\n            if key == \"args_schema\":\n                setattr(db_task, key, value)\n            else:\n                current_dict = deepcopy(getattr(db_task, key))\n                current_dict.update(value)\n                setattr(db_task, key, current_dict)\n        else:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=f\"Invalid {type(value)=} for {key=}\",\n            )\n\n    await db.commit()\n    await db.refresh(db_task)\n    await db.close()\n    return db_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/task_collection/","title":"task_collection","text":""},{"location":"reference/fractal_server/app/routes/api/v1/task_collection/#fractal_server.app.routes.api.v1.task_collection.check_collection_status","title":"<code>check_collection_status(state_id, user=Depends(current_active_user), verbose=False, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Check status of background task collection</p> Source code in <code>fractal_server/app/routes/api/v1/task_collection.py</code> <pre><code>@router.get(\"/collect/{state_id}/\", response_model=StateRead)\nasync def check_collection_status(\n    state_id: int,\n    user: User = Depends(current_active_user),\n    verbose: bool = False,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StateRead:  # State[TaskCollectStatus]\n    \"\"\"\n    Check status of background task collection\n    \"\"\"\n    logger = set_logger(logger_name=\"check_collection_status\")\n    logger.debug(f\"Querying state for state.id={state_id}\")\n    state = await db.get(State, state_id)\n    if not state:\n        await db.close()\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"No task collection info with id={state_id}\",\n        )\n    data = TaskCollectStatusV1(**state.data)\n\n    # In some cases (i.e. a successful or ongoing task collection), data.log is\n    # not set; if so, we collect the current logs\n    if verbose and not data.log:\n        data.log = get_collection_log(data.venv_path)\n        state.data = data.sanitised_dict()\n    close_logger(logger)\n    await db.close()\n    return state\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/task_collection/#fractal_server.app.routes.api.v1.task_collection.collect_tasks_pip","title":"<code>collect_tasks_pip(task_collect, background_tasks, response, user=Depends(current_active_verified_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Task collection endpoint</p> <p>Trigger the creation of a dedicated virtual environment, the installation of a package and the collection of tasks as advertised in the manifest.</p> Source code in <code>fractal_server/app/routes/api/v1/task_collection.py</code> <pre><code>@router.post(\n    \"/collect/pip/\",\n    response_model=StateRead,\n    responses={\n        201: dict(\n            description=(\n                \"Task collection successfully started in the background\"\n            )\n        ),\n        200: dict(\n            description=(\n                \"Package already collected. Returning info on already \"\n                \"available tasks\"\n            )\n        ),\n    },\n)\nasync def collect_tasks_pip(\n    task_collect: TaskCollectPipV1,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    user: User = Depends(current_active_verified_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StateRead:  # State[TaskCollectStatus]\n    \"\"\"\n    Task collection endpoint\n\n    Trigger the creation of a dedicated virtual environment, the installation\n    of a package and the collection of tasks as advertised in the manifest.\n    \"\"\"\n\n    logger = set_logger(logger_name=\"collect_tasks_pip\")\n\n    # Validate payload as _TaskCollectPip, which has more strict checks than\n    # TaskCollectPip\n    try:\n        task_pkg = _TaskCollectPip(**task_collect.dict(exclude_unset=True))\n    except ValidationError as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Invalid task-collection object. Original error: {e}\",\n        )\n\n    with TemporaryDirectory() as tmpdir:\n        try:\n            # Copy or download the package wheel file to tmpdir\n            if task_pkg.is_local_package:\n                shell_copy(task_pkg.package_path.as_posix(), tmpdir)\n                pkg_path = Path(tmpdir) / task_pkg.package_path.name\n            else:\n                pkg_path = await download_package(\n                    task_pkg=task_pkg, dest=tmpdir\n                )\n            # Read package info from wheel file, and override the ones coming\n            # from the request body\n            pkg_info = inspect_package(pkg_path)\n            task_pkg.package_name = pkg_info[\"pkg_name\"]\n            task_pkg.package_version = pkg_info[\"pkg_version\"]\n            task_pkg.package_manifest = pkg_info[\"pkg_manifest\"]\n            task_pkg.check()\n        except Exception as e:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=f\"Invalid package or manifest. Original error: {e}\",\n            )\n\n    try:\n        venv_path = create_package_dir_pip(task_pkg=task_pkg)\n    except FileExistsError:\n        venv_path = create_package_dir_pip(task_pkg=task_pkg, create=False)\n        try:\n            task_collect_status = get_collection_data(venv_path)\n            for task in task_collect_status.task_list:\n                db_task = await db.get(Task, task.id)\n                if (\n                    (not db_task)\n                    or db_task.source != task.source\n                    or db_task.name != task.name\n                ):\n                    await db.close()\n                    raise HTTPException(\n                        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                        detail=(\n                            \"Cannot collect package. Folder already exists, \"\n                            f\"but task {task.id} does not exists or it does \"\n                            f\"not have the expected source ({task.source}) or \"\n                            f\"name ({task.name}).\"\n                        ),\n                    )\n        except FileNotFoundError as e:\n            await db.close()\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=(\n                    \"Cannot collect package. Possible reason: another \"\n                    \"collection of the same package is in progress. \"\n                    f\"Original error: {e}\"\n                ),\n            )\n        except ValidationError as e:\n            await db.close()\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=(\n                    \"Cannot collect package. Possible reason: an old version \"\n                    \"of the same package has already been collected. \"\n                    f\"Original error: {e}\"\n                ),\n            )\n        task_collect_status.info = \"Already installed\"\n        state = State(data=task_collect_status.sanitised_dict())\n        response.status_code == status.HTTP_200_OK\n        await db.close()\n        return state\n    settings = Inject(get_settings)\n\n    # Check that tasks are not already in the DB\n    for new_task in task_pkg.package_manifest.task_list:\n        new_task_name_slug = slugify_task_name(new_task.name)\n        new_task_source = f\"{task_pkg.package_source}:{new_task_name_slug}\"\n        stm = select(Task).where(Task.source == new_task_source)\n        res = await db.execute(stm)\n        if res.scalars().all():\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=(\n                    \"Cannot collect package. Task with source \"\n                    f'\"{new_task_source}\" already exists in the database.'\n                ),\n            )\n\n    # All checks are OK, proceed with task collection\n    full_venv_path = venv_path.relative_to(settings.FRACTAL_TASKS_DIR)\n    collection_status = TaskCollectStatusV1(\n        status=\"pending\", venv_path=full_venv_path, package=task_pkg.package\n    )\n\n    # Create State object (after casting venv_path to string)\n    collection_status_dict = collection_status.dict()\n    collection_status_dict[\"venv_path\"] = str(collection_status.venv_path)\n    state = State(data=collection_status_dict)\n    db.add(state)\n    await db.commit()\n    await db.refresh(state)\n\n    background_tasks.add_task(\n        background_collect_pip,\n        state_id=state.id,\n        venv_path=venv_path,\n        task_pkg=task_pkg,\n    )\n    logger.debug(\n        \"Task-collection endpoint: start background collection \"\n        \"and return state\"\n    )\n    close_logger(logger)\n    info = (\n        \"Collecting tasks in the background. \"\n        f\"GET /task/collect/{state.id} to query collection status\"\n    )\n    state.data[\"info\"] = info\n    response.status_code = status.HTTP_201_CREATED\n    await db.close()\n    return state\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflow/","title":"workflow","text":""},{"location":"reference/fractal_server/app/routes/api/v1/workflow/#fractal_server.app.routes.api.v1.workflow.create_workflow","title":"<code>create_workflow(project_id, workflow, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create a workflow, associate to a project</p> Source code in <code>fractal_server/app/routes/api/v1/workflow.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/\",\n    response_model=WorkflowReadV1,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_workflow(\n    project_id: int,\n    workflow: WorkflowCreateV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowReadV1]:\n    \"\"\"\n    Create a workflow, associate to a project\n    \"\"\"\n    await _get_project_check_owner(\n        project_id=project_id,\n        user_id=user.id,\n        db=db,\n    )\n    await _check_workflow_exists(\n        name=workflow.name, project_id=project_id, db=db\n    )\n\n    db_workflow = Workflow(project_id=project_id, **workflow.dict())\n    db.add(db_workflow)\n    await db.commit()\n    await db.refresh(db_workflow)\n    await db.close()\n    return db_workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflow/#fractal_server.app.routes.api.v1.workflow.delete_workflow","title":"<code>delete_workflow(project_id, workflow_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a workflow</p> Source code in <code>fractal_server/app/routes/api/v1/workflow.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    status_code=status.HTTP_204_NO_CONTENT,\n)\nasync def delete_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current workflow.\n    stm = _get_submitted_jobs_statement().where(\n        ApplyWorkflow.workflow_id == workflow.id\n    )\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot delete workflow {workflow.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Cascade operations: set foreign-keys to null for jobs which are in\n    # relationship with the current workflow\n    stm = select(ApplyWorkflow).where(ApplyWorkflow.workflow_id == workflow_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    for job in jobs:\n        job.workflow_id = None\n        await db.merge(job)\n    await db.commit()\n\n    # Delete workflow\n    await db.delete(workflow)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflow/#fractal_server.app.routes.api.v1.workflow.export_worfklow","title":"<code>export_worfklow(project_id, workflow_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Export an existing workflow, after stripping all IDs</p> Source code in <code>fractal_server/app/routes/api/v1/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/export/\",\n    response_model=WorkflowExportV1,\n)\nasync def export_worfklow(\n    project_id: int,\n    workflow_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowExportV1]:\n    \"\"\"\n    Export an existing workflow, after stripping all IDs\n    \"\"\"\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n    # Emit a warning when exporting a workflow with custom tasks\n    logger = set_logger(None)\n    for wftask in workflow.task_list:\n        if wftask.task.owner is not None:\n            logger.warning(\n                f\"Custom tasks (like the one with id={wftask.task.id} and \"\n                f'source=\"{wftask.task.source}\") are not meant to be '\n                \"portable; re-importing this workflow may not work as \"\n                \"expected.\"\n            )\n    close_logger(logger)\n\n    await db.close()\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflow/#fractal_server.app.routes.api.v1.workflow.get_user_workflows","title":"<code>get_user_workflows(user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the workflows of the current user</p> Source code in <code>fractal_server/app/routes/api/v1/workflow.py</code> <pre><code>@router.get(\"/workflow/\", response_model=list[WorkflowReadV1])\nasync def get_user_workflows(\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[WorkflowReadV1]:\n    \"\"\"\n    Returns all the workflows of the current user\n    \"\"\"\n    stm = select(Workflow)\n    stm = stm.join(Project).where(Project.user_list.any(User.id == user.id))\n    res = await db.execute(stm)\n    workflow_list = res.scalars().all()\n    return workflow_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflow/#fractal_server.app.routes.api.v1.workflow.get_workflow_list","title":"<code>get_workflow_list(project_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get workflow list for given project</p> Source code in <code>fractal_server/app/routes/api/v1/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/\",\n    response_model=list[WorkflowReadV1],\n)\nasync def get_workflow_list(\n    project_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[list[WorkflowReadV1]]:\n    \"\"\"\n    Get workflow list for given project\n    \"\"\"\n    # Access control\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    # Find workflows of the current project. Note: this select/where approach\n    # has much better scaling than refreshing all elements of\n    # `project.workflow_list` - ref\n    # https://github.com/fractal-analytics-platform/fractal-server/pull/1082#issuecomment-1856676097.\n    stm = select(Workflow).where(Workflow.project_id == project.id)\n    workflow_list = (await db.execute(stm)).scalars().all()\n    return workflow_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflow/#fractal_server.app.routes.api.v1.workflow.import_workflow","title":"<code>import_workflow(project_id, workflow, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Import an existing workflow into a project</p> <p>Also create all required objects (i.e. Workflow and WorkflowTask's) along the way.</p> Source code in <code>fractal_server/app/routes/api/v1/workflow.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/import/\",\n    response_model=WorkflowReadV1,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def import_workflow(\n    project_id: int,\n    workflow: WorkflowImportV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowReadV1]:\n    \"\"\"\n    Import an existing workflow into a project\n\n    Also create all required objects (i.e. Workflow and WorkflowTask's) along\n    the way.\n    \"\"\"\n\n    # Preliminary checks\n    await _get_project_check_owner(\n        project_id=project_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    await _check_workflow_exists(\n        name=workflow.name, project_id=project_id, db=db\n    )\n\n    # Check that all required tasks are available\n    tasks = [wf_task.task for wf_task in workflow.task_list]\n    source_to_id = {}\n    for task in tasks:\n        source = task.source\n        if source not in source_to_id.keys():\n            stm = select(Task).where(Task.source == source)\n            tasks_by_source = (await db.execute(stm)).scalars().all()\n            if len(tasks_by_source) != 1:\n                raise HTTPException(\n                    status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                    detail=(\n                        f\"Found {len(tasks_by_source)} tasks with {source=}.\"\n                    ),\n                )\n            source_to_id[source] = tasks_by_source[0].id\n\n    # Create new Workflow (with empty task_list)\n    db_workflow = Workflow(\n        project_id=project_id,\n        **workflow.dict(exclude_none=True, exclude={\"task_list\"}),\n    )\n    db.add(db_workflow)\n    await db.commit()\n    await db.refresh(db_workflow)\n\n    # Insert tasks\n    async with db:\n        for _, wf_task in enumerate(workflow.task_list):\n            # Identify task_id\n            source = wf_task.task.source\n            task_id = source_to_id[source]\n            # Prepare new_wf_task\n            new_wf_task = WorkflowTaskCreateV1(\n                **wf_task.dict(exclude_none=True),\n            )\n            # Insert task\n            await _workflow_insert_task(\n                **new_wf_task.dict(),\n                workflow_id=db_workflow.id,\n                task_id=task_id,\n                db=db,\n            )\n\n    await db.close()\n    return db_workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflow/#fractal_server.app.routes.api.v1.workflow.read_workflow","title":"<code>read_workflow(project_id, workflow_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on an existing workflow</p> Source code in <code>fractal_server/app/routes/api/v1/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    response_model=WorkflowReadV1,\n)\nasync def read_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowReadV1]:\n    \"\"\"\n    Get info on an existing workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflow/#fractal_server.app.routes.api.v1.workflow.update_workflow","title":"<code>update_workflow(project_id, workflow_id, patch, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a workflow</p> Source code in <code>fractal_server/app/routes/api/v1/workflow.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    response_model=WorkflowReadV1,\n)\nasync def update_workflow(\n    project_id: int,\n    workflow_id: int,\n    patch: WorkflowUpdateV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowReadV1]:\n    \"\"\"\n    Edit a workflow\n    \"\"\"\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n\n    if patch.name:\n        await _check_workflow_exists(\n            name=patch.name, project_id=project_id, db=db\n        )\n\n    for key, value in patch.dict(exclude_unset=True).items():\n        if key == \"reordered_workflowtask_ids\":\n            current_workflowtask_ids = [\n                wftask.id for wftask in workflow.task_list\n            ]\n            num_tasks = len(workflow.task_list)\n            if len(value) != num_tasks or set(value) != set(\n                current_workflowtask_ids\n            ):\n                raise HTTPException(\n                    status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                    detail=(\n                        \"`reordered_workflowtask_ids` must be a permutation of\"\n                        f\" {current_workflowtask_ids} (given {value})\"\n                    ),\n                )\n            for ind_wftask in range(num_tasks):\n                new_order = value.index(workflow.task_list[ind_wftask].id)\n                workflow.task_list[ind_wftask].order = new_order\n        else:\n            setattr(workflow, key, value)\n\n    await db.commit()\n    await db.refresh(workflow)\n    await db.close()\n\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflowtask/","title":"workflowtask","text":""},{"location":"reference/fractal_server/app/routes/api/v1/workflowtask/#fractal_server.app.routes.api.v1.workflowtask.create_workflowtask","title":"<code>create_workflowtask(project_id, workflow_id, task_id, new_task, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add a WorkflowTask to a Workflow</p> Source code in <code>fractal_server/app/routes/api/v1/workflowtask.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/\",\n    response_model=WorkflowTaskReadV1,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    task_id: int,\n    new_task: WorkflowTaskCreateV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowTaskReadV1]:\n    \"\"\"\n    Add a WorkflowTask to a Workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n\n    # Check that task exists\n    task = await db.get(Task, task_id)\n    if not task:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Task {task_id} not found.\",\n        )\n\n    async with db:\n        workflow_task = await _workflow_insert_task(\n            **new_task.dict(),\n            workflow_id=workflow.id,\n            task_id=task_id,\n            db=db,\n        )\n\n    await db.close()\n    return workflow_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflowtask/#fractal_server.app.routes.api.v1.workflowtask.delete_workflowtask","title":"<code>delete_workflowtask(project_id, workflow_id, workflow_task_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a WorkflowTask of a Workflow</p> Source code in <code>fractal_server/app/routes/api/v1/workflowtask.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}/\",\n    status_code=status.HTTP_204_NO_CONTENT,\n)\nasync def delete_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a WorkflowTask of a Workflow\n    \"\"\"\n\n    db_workflow_task, db_workflow = await _get_workflow_task_check_owner(\n        project_id=project_id,\n        workflow_task_id=workflow_task_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    await db.delete(db_workflow_task)\n    await db.commit()\n\n    await db.refresh(db_workflow)\n    db_workflow.task_list.reorder()\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v1/workflowtask/#fractal_server.app.routes.api.v1.workflowtask.update_workflowtask","title":"<code>update_workflowtask(project_id, workflow_id, workflow_task_id, workflow_task_update, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a WorkflowTask of a Workflow</p> Source code in <code>fractal_server/app/routes/api/v1/workflowtask.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}/\",\n    response_model=WorkflowTaskReadV1,\n)\nasync def update_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    workflow_task_update: WorkflowTaskUpdateV1,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowTaskReadV1]:\n    \"\"\"\n    Edit a WorkflowTask of a Workflow\n    \"\"\"\n\n    db_workflow_task, db_workflow = await _get_workflow_task_check_owner(\n        project_id=project_id,\n        workflow_task_id=workflow_task_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    for key, value in workflow_task_update.dict(exclude_unset=True).items():\n        if key == \"args\":\n\n            # Get default arguments via a Task property method\n            default_args = deepcopy(\n                db_workflow_task.task.default_args_from_args_schema\n            )\n            # Override default_args with args value items\n            actual_args = default_args.copy()\n            if value is not None:\n                for k, v in value.items():\n                    actual_args[k] = v\n            if not actual_args:\n                actual_args = None\n            setattr(db_workflow_task, key, actual_args)\n        elif key == \"meta\":\n            current_meta = deepcopy(db_workflow_task.meta) or {}\n            current_meta.update(value)\n            setattr(db_workflow_task, key, current_meta)\n        else:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=f\"patch_workflow_task endpoint cannot set {key=}\",\n            )\n\n    await db.commit()\n    await db.refresh(db_workflow_task)\n    await db.close()\n\n    return db_workflow_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/","title":"v2","text":"<p><code>api/v2</code> module</p>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/","title":"_aux_functions","text":"<p>Auxiliary functions to get object from the database or perform simple checks</p>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._check_project_exists","title":"<code>_check_project_exists(*, project_name, user_id, db)</code>  <code>async</code>","text":"<p>Check that no other project with this name exists for this user.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Project name</p> required <code>user_id</code> <code>int</code> <p>User ID</p> required <code>db</code> <code>AsyncSession</code> required <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If such a project already exists</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _check_project_exists(\n    *,\n    project_name: str,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Check that no other project with this name exists for this user.\n\n    Args:\n        project_name: Project name\n        user_id: User ID\n        db:\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If such a project already exists\n    \"\"\"\n    stm = (\n        select(ProjectV2)\n        .join(LinkUserProjectV2)\n        .where(ProjectV2.name == project_name)\n        .where(LinkUserProjectV2.user_id == user_id)\n    )\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Project name ({project_name}) already in use\",\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._check_workflow_exists","title":"<code>_check_workflow_exists(*, name, project_id, db)</code>  <code>async</code>","text":"<p>Check that no other workflow of this project has the same name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Workflow name</p> required <code>project_id</code> <code>int</code> <p>Project ID</p> required <code>db</code> <code>AsyncSession</code> required <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If such a workflow already exists</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _check_workflow_exists(\n    *,\n    name: str,\n    project_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Check that no other workflow of this project has the same name.\n\n    Args:\n        name: Workflow name\n        project_id: Project ID\n        db:\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If such a workflow already exists\n    \"\"\"\n    stm = (\n        select(WorkflowV2)\n        .where(WorkflowV2.name == name)\n        .where(WorkflowV2.project_id == project_id)\n    )\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Workflow with {name=} and {project_id=} already exists.\",\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_dataset_check_owner","title":"<code>_get_dataset_check_owner(*, project_id, dataset_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a dataset and a project, after access control on the project</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>dataset_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>dict[Literal['dataset', 'project'], Union[DatasetV2, ProjectV2]]</code> <p>Dictionary with the dataset and project objects (keys: <code>dataset</code>, <code>project</code>).</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the dataset is not associated to the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_dataset_check_owner(\n    *,\n    project_id: int,\n    dataset_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; dict[Literal[\"dataset\", \"project\"], Union[DatasetV2, ProjectV2]]:\n    \"\"\"\n    Get a dataset and a project, after access control on the project\n\n    Args:\n        project_id:\n        dataset_id:\n        user_id:\n        db:\n\n    Returns:\n        Dictionary with the dataset and project objects (keys: `dataset`,\n            `project`).\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the dataset is not associated to the project\n    \"\"\"\n\n    # Access control for project\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user_id, db=db\n    )\n    # Get dataset\n    dataset = await db.get(DatasetV2, dataset_id)\n    if not dataset:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Dataset not found\"\n        )\n    if dataset.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Invalid {project_id=} for {dataset_id=}\",\n        )\n\n    # Refresh so that dataset.project relationship is loaded (see discussion\n    # in issue #1063)\n    await db.refresh(dataset)\n\n    return dict(dataset=dataset, project=project)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_job_check_owner","title":"<code>_get_job_check_owner(*, project_id, job_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a job and a project, after access control on the project</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>job_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>dict[Literal['job', 'project'], Union[JobV2, ProjectV2]]</code> <p>Dictionary with the job and project objects (keys: <code>job</code>, <code>project</code>).</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the job is not associated to the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_job_check_owner(\n    *,\n    project_id: int,\n    job_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; dict[Literal[\"job\", \"project\"], Union[JobV2, ProjectV2]]:\n    \"\"\"\n    Get a job and a project, after access control on the project\n\n    Args:\n        project_id:\n        job_id:\n        user_id:\n        db:\n\n    Returns:\n        Dictionary with the job and project objects (keys: `job`,\n            `project`).\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the job is not associated to the project\n    \"\"\"\n    # Access control for project\n    project = await _get_project_check_owner(\n        project_id=project_id,\n        user_id=user_id,\n        db=db,\n    )\n    # Get dataset\n    job = await db.get(JobV2, job_id)\n    if not job:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Job not found\"\n        )\n    if job.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Invalid {project_id=} for {job_id=}\",\n        )\n    return dict(job=job, project=project)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_project_check_owner","title":"<code>_get_project_check_owner(*, project_id, user_id, db)</code>  <code>async</code>","text":"<p>Check that user is a member of project and return the project.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>ProjectV2</code> <p>The project object</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=403_FORBIDDEN)</code> <p>If the user is not a member of the project</p> <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the project does not exist</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_project_check_owner(\n    *,\n    project_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; ProjectV2:\n    \"\"\"\n    Check that user is a member of project and return the project.\n\n    Args:\n        project_id:\n        user_id:\n        db:\n\n    Returns:\n        The project object\n\n    Raises:\n        HTTPException(status_code=403_FORBIDDEN):\n            If the user is not a member of the project\n        HTTPException(status_code=404_NOT_FOUND):\n            If the project does not exist\n    \"\"\"\n    project = await db.get(ProjectV2, project_id)\n\n    link_user_project = await db.get(LinkUserProjectV2, (project_id, user_id))\n    if not project:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Project not found\"\n        )\n    if not link_user_project:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=f\"Not allowed on project {project_id}\",\n        )\n\n    return project\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_submitted_jobs_statement","title":"<code>_get_submitted_jobs_statement()</code>","text":"<p>Returns:</p> Type Description <code>SelectOfScalar</code> <p>A sqlmodel statement that selects all <code>Job</code>s with</p> <code>SelectOfScalar</code> <p><code>Job.status</code> equal to <code>submitted</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>def _get_submitted_jobs_statement() -&gt; SelectOfScalar:\n    \"\"\"\n    Returns:\n        A sqlmodel statement that selects all `Job`s with\n        `Job.status` equal to `submitted`.\n    \"\"\"\n    stm = select(JobV2).where(JobV2.status == JobStatusTypeV2.SUBMITTED)\n    return stm\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_task_check_owner","title":"<code>_get_task_check_owner(*, task_id, user, db)</code>  <code>async</code>","text":"<p>Get a task, after access control.</p> <p>This check constitutes a preliminary version of access control: if the current user is not a superuser and differs from the task owner (including when <code>owner is None</code>), we raise an 403 HTTP Exception.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> required <code>user</code> <code>User</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>TaskV2</code> <p>The task object.</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the task does not exist</p> <code>HTTPException(status_code=403_FORBIDDEN)</code> <p>If the user does not have rights to edit this task.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_task_check_owner(\n    *,\n    task_id: int,\n    user: User,\n    db: AsyncSession,\n) -&gt; TaskV2:\n    \"\"\"\n    Get a task, after access control.\n\n    This check constitutes a preliminary version of access control:\n    if the current user is not a superuser and differs from the task owner\n    (including when `owner is None`), we raise an 403 HTTP Exception.\n\n    Args:\n        task_id:\n        user:\n        db:\n\n    Returns:\n        The task object.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the task does not exist\n        HTTPException(status_code=403_FORBIDDEN):\n            If the user does not have rights to edit this task.\n    \"\"\"\n    task = await db.get(TaskV2, task_id)\n    if not task:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"TaskV2 {task_id} not found.\",\n        )\n\n    if not user.is_superuser:\n        if task.owner is None:\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN,\n                detail=(\n                    \"Only a superuser can modify a TaskV2 with `owner=None`.\"\n                ),\n            )\n        else:\n            owner = user.username or user.slurm_user\n            if owner != task.owner:\n                raise HTTPException(\n                    status_code=status.HTTP_403_FORBIDDEN,\n                    detail=(\n                        f\"Current user ({owner}) cannot modify TaskV2 \"\n                        f\"{task.id} with different owner ({task.owner}).\"\n                    ),\n                )\n    return task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflow_check_owner","title":"<code>_get_workflow_check_owner(*, workflow_id, project_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a workflow and a project, after access control on the project.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>int</code> required <code>project_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>WorkflowV2</code> <p>The workflow object.</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the workflow does not exist</p> <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the workflow is not associated to the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflow_check_owner(\n    *,\n    workflow_id: int,\n    project_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; WorkflowV2:\n    \"\"\"\n    Get a workflow and a project, after access control on the project.\n\n    Args:\n        workflow_id:\n        project_id:\n        user_id:\n        db:\n\n    Returns:\n        The workflow object.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the workflow does not exist\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the workflow is not associated to the project\n    \"\"\"\n\n    # Access control for project\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user_id, db=db\n    )\n    # Get workflow\n    workflow = await db.get(WorkflowV2, workflow_id)\n    if not workflow:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Workflow not found\"\n        )\n    if workflow.project_id != project.id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(f\"Invalid {project_id=} for {workflow_id=}.\"),\n        )\n\n    # Refresh so that workflow.project relationship is loaded (see discussion\n    # in issue #1063)\n    await db.refresh(workflow)\n\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflow_task_check_owner","title":"<code>_get_workflow_task_check_owner(*, project_id, workflow_id, workflow_task_id, user_id, db)</code>  <code>async</code>","text":"<p>Check that user has access to Workflow and WorkflowTask.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>workflow_id</code> <code>int</code> required <code>workflow_task_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>tuple[WorkflowTaskV2, WorkflowV2]</code> <p>Tuple of WorkflowTask and Workflow objects.</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the WorkflowTask does not exist</p> <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the WorkflowTask is not associated to the Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflow_task_check_owner(\n    *,\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; tuple[WorkflowTaskV2, WorkflowV2]:\n    \"\"\"\n    Check that user has access to Workflow and WorkflowTask.\n\n    Args:\n        project_id:\n        workflow_id:\n        workflow_task_id:\n        user_id:\n        db:\n\n    Returns:\n        Tuple of WorkflowTask and Workflow objects.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the WorkflowTask does not exist\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the WorkflowTask is not associated to the Workflow\n    \"\"\"\n\n    # Access control for workflow\n    workflow = await _get_workflow_check_owner(\n        workflow_id=workflow_id,\n        project_id=project_id,\n        user_id=user_id,\n        db=db,\n    )\n\n    # If WorkflowTask is not in the db, exit\n    workflow_task = await db.get(WorkflowTaskV2, workflow_task_id)\n    if not workflow_task:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"WorkflowTask not found\",\n        )\n\n    # If WorkflowTask is not part of the expected Workflow, exit\n    if workflow_id != workflow_task.workflow_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Invalid {workflow_id=} for {workflow_task_id=}\",\n        )\n\n    return workflow_task, workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._workflow_insert_task","title":"<code>_workflow_insert_task(*, workflow_id, task_id, is_legacy_task=False, order=None, meta_parallel=None, meta_non_parallel=None, args_non_parallel=None, args_parallel=None, input_filters=None, db)</code>  <code>async</code>","text":"<p>Insert a new WorkflowTask into Workflow.task_list</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>int</code> required <code>task_id</code> <code>int</code> required <code>is_legacy_task</code> <code>bool</code> <code>False</code> <code>order</code> <code>Optional[int]</code> <code>None</code> <code>meta_parallel</code> <code>Optional[dict[str, Any]]</code> <code>None</code> <code>meta_non_parallel</code> <code>Optional[dict[str, Any]]</code> <code>None</code> <code>args_non_parallel</code> <code>Optional[dict[str, Any]]</code> <code>None</code> <code>args_parallel</code> <code>Optional[dict[str, Any]]</code> <code>None</code> <code>input_filters</code> <code>Optional[Filters]</code> <code>None</code> <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _workflow_insert_task(\n    *,\n    workflow_id: int,\n    task_id: int,\n    is_legacy_task: bool = False,\n    order: Optional[int] = None,\n    meta_parallel: Optional[dict[str, Any]] = None,\n    meta_non_parallel: Optional[dict[str, Any]] = None,\n    args_non_parallel: Optional[dict[str, Any]] = None,\n    args_parallel: Optional[dict[str, Any]] = None,\n    input_filters: Optional[Filters] = None,\n    db: AsyncSession,\n) -&gt; WorkflowTaskV2:\n    \"\"\"\n    Insert a new WorkflowTask into Workflow.task_list\n\n    Args:\n        workflow_id:\n        task_id:\n        is_legacy_task:\n        order:\n        meta_parallel:\n        meta_non_parallel:\n        args_non_parallel:\n        args_parallel:\n        input_filters:\n        db:\n    \"\"\"\n    db_workflow = await db.get(WorkflowV2, workflow_id)\n    if db_workflow is None:\n        raise ValueError(f\"Workflow {workflow_id} does not exist\")\n\n    if order is None:\n        order = len(db_workflow.task_list)\n\n    # Get task from db, and extract default arguments via a Task property\n    # method\n    if is_legacy_task is True:\n        db_task = await db.get(Task, task_id)\n        if db_task is None:\n            raise ValueError(f\"Task {task_id} not found.\")\n        task_type = \"parallel\"\n\n        final_args_parallel = db_task.default_args_from_args_schema.copy()\n        final_args_non_parallel = {}\n        final_meta_parallel = (db_task.meta or {}).copy()\n        final_meta_non_parallel = {}\n\n    else:\n        db_task = await db.get(TaskV2, task_id)\n        if db_task is None:\n            raise ValueError(f\"TaskV2 {task_id} not found.\")\n        task_type = db_task.type\n\n        final_args_non_parallel = (\n            db_task.default_args_non_parallel_from_args_schema.copy()\n        )\n        final_args_parallel = (\n            db_task.default_args_parallel_from_args_schema.copy()\n        )\n        final_meta_parallel = (db_task.meta_parallel or {}).copy()\n        final_meta_non_parallel = (db_task.meta_non_parallel or {}).copy()\n\n    # Combine arg_parallel\n    if args_parallel is not None:\n        for k, v in args_parallel.items():\n            final_args_parallel[k] = v\n    if final_args_parallel == {}:\n        final_args_parallel = None\n    # Combine arg_non_parallel\n    if args_non_parallel is not None:\n        for k, v in args_non_parallel.items():\n            final_args_non_parallel[k] = v\n    if final_args_non_parallel == {}:\n        final_args_non_parallel = None\n\n    # Combine meta_parallel (higher priority)\n    # and db_task.meta_parallel (lower priority)\n    final_meta_parallel.update(meta_parallel or {})\n    if final_meta_parallel == {}:\n        final_meta_parallel = None\n    # Combine meta_non_parallel (higher priority)\n    # and db_task.meta_non_parallel (lower priority)\n    final_meta_non_parallel.update(meta_non_parallel or {})\n    if final_meta_non_parallel == {}:\n        final_meta_non_parallel = None\n\n    # Prepare input_filters attribute\n    if input_filters is None:\n        input_filters_kwarg = {}\n    else:\n        input_filters_kwarg = dict(input_filters=input_filters)\n\n    # Create DB entry\n    wf_task = WorkflowTaskV2(\n        task_type=task_type,\n        is_legacy_task=is_legacy_task,\n        task_id=(task_id if not is_legacy_task else None),\n        task_legacy_id=(task_id if is_legacy_task else None),\n        args_non_parallel=final_args_non_parallel,\n        args_parallel=final_args_parallel,\n        meta_parallel=final_meta_parallel,\n        meta_non_parallel=final_meta_non_parallel,\n        **input_filters_kwarg,\n    )\n    db_workflow.task_list.insert(order, wf_task)\n    db_workflow.task_list.reorder()  # type: ignore\n    flag_modified(db_workflow, \"task_list\")\n    await db.commit()\n    await db.refresh(wf_task)\n\n    return wf_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/","title":"dataset","text":""},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.create_dataset","title":"<code>create_dataset(project_id, dataset, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add new dataset to current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/dataset/\",\n    response_model=DatasetReadV2,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_dataset(\n    project_id: int,\n    dataset: DatasetCreateV2,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[DatasetReadV2]:\n    \"\"\"\n    Add new dataset to current project\n    \"\"\"\n    await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    db_dataset = DatasetV2(project_id=project_id, **dataset.dict())\n    db.add(db_dataset)\n    await db.commit()\n    await db.refresh(db_dataset)\n    await db.close()\n\n    return db_dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.delete_dataset","title":"<code>delete_dataset(project_id, dataset_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    status_code=204,\n)\nasync def delete_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a dataset associated to the current project\n    \"\"\"\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current dataset.\n    stm = _get_submitted_jobs_statement().where(JobV2.dataset_id == dataset_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot delete dataset {dataset.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Cascade operations: set foreign-keys to null for jobs which are in\n    # relationship with the current dataset\n    stm = select(JobV2).where(JobV2.dataset_id == dataset_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    for job in jobs:\n        job.dataset_id = None\n\n    # Delete dataset\n    await db.delete(dataset)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.export_dataset","title":"<code>export_dataset(project_id, dataset_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Export an existing dataset</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/export/\",\n    response_model=DatasetExportV2,\n)\nasync def export_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[DatasetExportV2]:\n    \"\"\"\n    Export an existing dataset\n    \"\"\"\n    dict_dataset_project = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    await db.close()\n\n    dataset = dict_dataset_project[\"dataset\"]\n\n    return dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.get_user_datasets","title":"<code>get_user_datasets(history=True, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the datasets of the current user</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\"/dataset/\", response_model=list[DatasetReadV2])\nasync def get_user_datasets(\n    history: bool = True,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[DatasetReadV2]:\n    \"\"\"\n    Returns all the datasets of the current user\n    \"\"\"\n    stm = select(DatasetV2)\n    stm = stm.join(ProjectV2).where(\n        ProjectV2.user_list.any(User.id == user.id)\n    )\n\n    res = await db.execute(stm)\n    dataset_list = res.scalars().all()\n    await db.close()\n    if not history:\n        for ds in dataset_list:\n            setattr(ds, \"history\", [])\n    return dataset_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.import_dataset","title":"<code>import_dataset(project_id, dataset, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Import an existing dataset into a project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/dataset/import/\",\n    response_model=DatasetReadV2,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def import_dataset(\n    project_id: int,\n    dataset: DatasetImportV2,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[DatasetReadV2]:\n    \"\"\"\n    Import an existing dataset into a project\n    \"\"\"\n\n    # Preliminary checks\n    await _get_project_check_owner(\n        project_id=project_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Create new Dataset\n    db_dataset = DatasetV2(\n        project_id=project_id,\n        **dataset.dict(exclude_none=True),\n    )\n    db.add(db_dataset)\n    await db.commit()\n    await db.refresh(db_dataset)\n    await db.close()\n\n    return db_dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.read_dataset","title":"<code>read_dataset(project_id, dataset_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    response_model=DatasetReadV2,\n)\nasync def read_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[DatasetReadV2]:\n    \"\"\"\n    Get info on a dataset associated to the current project\n    \"\"\"\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n    await db.close()\n    return dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.read_dataset_list","title":"<code>read_dataset_list(project_id, history=True, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get dataset list for given project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/\",\n    response_model=list[DatasetReadV2],\n)\nasync def read_dataset_list(\n    project_id: int,\n    history: bool = True,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[list[DatasetReadV2]]:\n    \"\"\"\n    Get dataset list for given project\n    \"\"\"\n    # Access control\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    # Find datasets of the current project. Note: this select/where approach\n    # has much better scaling than refreshing all elements of\n    # `project.dataset_list` - ref\n    # https://github.com/fractal-analytics-platform/fractal-server/pull/1082#issuecomment-1856676097.\n    stm = select(DatasetV2).where(DatasetV2.project_id == project.id)\n    res = await db.execute(stm)\n    dataset_list = res.scalars().all()\n    await db.close()\n    if not history:\n        for ds in dataset_list:\n            setattr(ds, \"history\", [])\n    return dataset_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.update_dataset","title":"<code>update_dataset(project_id, dataset_id, dataset_update, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    response_model=DatasetReadV2,\n)\nasync def update_dataset(\n    project_id: int,\n    dataset_id: int,\n    dataset_update: DatasetUpdateV2,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[DatasetReadV2]:\n    \"\"\"\n    Edit a dataset associated to the current project\n    \"\"\"\n\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    db_dataset = output[\"dataset\"]\n\n    if (dataset_update.zarr_dir is not None) and (len(db_dataset.images) != 0):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                \"Cannot modify `zarr_dir` because the dataset has a non-empty \"\n                \"image list.\"\n            ),\n        )\n\n    for key, value in dataset_update.dict(exclude_unset=True).items():\n        setattr(db_dataset, key, value)\n\n    await db.commit()\n    await db.refresh(db_dataset)\n    await db.close()\n    return db_dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/images/","title":"images","text":""},{"location":"reference/fractal_server/app/routes/api/v2/job/","title":"job","text":""},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.download_job_logs","title":"<code>download_job_logs(project_id, job_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Download job folder</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/download/\",\n    response_class=StreamingResponse,\n)\nasync def download_job_logs(\n    project_id: int,\n    job_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Download job folder\n    \"\"\"\n    output = await _get_job_check_owner(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        db=db,\n    )\n    job = output[\"job\"]\n\n    # Create and return byte stream for zipped log folder\n    PREFIX_ZIP = Path(job.working_dir).name\n    zip_filename = f\"{PREFIX_ZIP}_archive.zip\"\n    byte_stream = _zip_folder_to_byte_stream(\n        folder=job.working_dir, zip_filename=zip_filename\n    )\n    return StreamingResponse(\n        iter([byte_stream.getvalue()]),\n        media_type=\"application/x-zip-compressed\",\n        headers={\"Content-Disposition\": f\"attachment;filename={zip_filename}\"},\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.get_job_list","title":"<code>get_job_list(project_id, user=Depends(current_active_user), log=True, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get job list for given project</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/\",\n    response_model=list[JobReadV2],\n)\nasync def get_job_list(\n    project_id: int,\n    user: User = Depends(current_active_user),\n    log: bool = True,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[list[JobReadV2]]:\n    \"\"\"\n    Get job list for given project\n    \"\"\"\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n\n    stm = select(JobV2).where(JobV2.project_id == project.id)\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    await db.close()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.get_user_jobs","title":"<code>get_user_jobs(user=Depends(current_active_user), log=True, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the jobs of the current user</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\"/job/\", response_model=list[JobReadV2])\nasync def get_user_jobs(\n    user: User = Depends(current_active_user),\n    log: bool = True,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[JobReadV2]:\n    \"\"\"\n    Returns all the jobs of the current user\n    \"\"\"\n    stm = (\n        select(JobV2)\n        .join(ProjectV2)\n        .where(ProjectV2.user_list.any(User.id == user.id))\n    )\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    await db.close()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.get_workflow_jobs","title":"<code>get_workflow_jobs(project_id, workflow_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the jobs related to a specific workflow</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/job/\",\n    response_model=list[JobReadV2],\n)\nasync def get_workflow_jobs(\n    project_id: int,\n    workflow_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[list[JobReadV2]]:\n    \"\"\"\n    Returns all the jobs related to a specific workflow\n    \"\"\"\n    await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n    stm = select(JobV2).where(JobV2.workflow_id == workflow_id)\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.read_job","title":"<code>read_job(project_id, job_id, show_tmp_logs=False, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return info on an existing job</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/\",\n    response_model=JobReadV2,\n)\nasync def read_job(\n    project_id: int,\n    job_id: int,\n    show_tmp_logs: bool = False,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[JobReadV2]:\n    \"\"\"\n    Return info on an existing job\n    \"\"\"\n\n    output = await _get_job_check_owner(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        db=db,\n    )\n    job = output[\"job\"]\n    await db.close()\n\n    if show_tmp_logs and (job.status == JobStatusTypeV2.SUBMITTED):\n        try:\n            with open(f\"{job.working_dir}/{WORKFLOW_LOG_FILENAME}\", \"r\") as f:\n                job.log = f.read()\n        except FileNotFoundError:\n            pass\n\n    return job\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.stop_job","title":"<code>stop_job(project_id, job_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Stop execution of a workflow job (only available for slurm backend)</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/stop/\",\n    status_code=202,\n)\nasync def stop_job(\n    project_id: int,\n    job_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Stop execution of a workflow job (only available for slurm backend)\n    \"\"\"\n\n    # This endpoint is only implemented for SLURM backend\n    _check_backend_is_slurm()\n\n    # Get job from DB\n    output = await _get_job_check_owner(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        db=db,\n    )\n    job = output[\"job\"]\n\n    _write_shutdown_file(job=job)\n\n    return Response(status_code=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/project/","title":"project","text":""},{"location":"reference/fractal_server/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.create_project","title":"<code>create_project(project, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create new poject</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.post(\"/project/\", response_model=ProjectReadV2, status_code=201)\nasync def create_project(\n    project: ProjectCreateV2,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[ProjectReadV2]:\n    \"\"\"\n    Create new poject\n    \"\"\"\n\n    # Check that there is no project with the same user and name\n    await _check_project_exists(\n        project_name=project.name, user_id=user.id, db=db\n    )\n\n    db_project = ProjectV2(**project.dict())\n    db_project.user_list.append(user)\n\n    db.add(db_project)\n    await db.commit()\n    await db.refresh(db_project)\n    await db.close()\n\n    return db_project\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.delete_project","title":"<code>delete_project(project_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete project</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.delete(\"/project/{project_id}/\", status_code=204)\nasync def delete_project(\n    project_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete project\n    \"\"\"\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current project.\n    stm = _get_submitted_jobs_statement().where(JobV2.project_id == project_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot delete project {project.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Cascade operations\n\n    # Workflows\n    stm = select(WorkflowV2).where(WorkflowV2.project_id == project_id)\n    res = await db.execute(stm)\n    workflows = res.scalars().all()\n    for wf in workflows:\n        # Cascade operations: set foreign-keys to null for jobs which are in\n        # relationship with the current workflow\n        stm = select(JobV2).where(JobV2.workflow_id == wf.id)\n        res = await db.execute(stm)\n        jobs = res.scalars().all()\n        for job in jobs:\n            job.workflow_id = None\n        # Delete workflow\n        await db.delete(wf)\n\n    # Dataset\n    stm = select(DatasetV2).where(DatasetV2.project_id == project_id)\n    res = await db.execute(stm)\n    datasets = res.scalars().all()\n    for ds in datasets:\n        # Cascade operations: set foreign-keys to null for jobs which are in\n        # relationship with the current dataset\n        stm = select(JobV2).where(JobV2.dataset_id == ds.id)\n        res = await db.execute(stm)\n        jobs = res.scalars().all()\n        for job in jobs:\n            job.dataset_id = None\n        # Delete dataset\n        await db.delete(ds)\n\n    # Job\n    stm = select(JobV2).where(JobV2.project_id == project_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    for job in jobs:\n        job.project_id = None\n\n    await db.delete(project)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.get_list_project","title":"<code>get_list_project(user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return list of projects user is member of</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.get(\"/project/\", response_model=list[ProjectReadV2])\nasync def get_list_project(\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProjectV2]:\n    \"\"\"\n    Return list of projects user is member of\n    \"\"\"\n    stm = (\n        select(ProjectV2)\n        .join(LinkUserProjectV2)\n        .where(LinkUserProjectV2.user_id == user.id)\n    )\n    res = await db.execute(stm)\n    project_list = res.scalars().all()\n    await db.close()\n    return project_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.read_project","title":"<code>read_project(project_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return info on an existing project</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.get(\"/project/{project_id}/\", response_model=ProjectReadV2)\nasync def read_project(\n    project_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[ProjectReadV2]:\n    \"\"\"\n    Return info on an existing project\n    \"\"\"\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    await db.close()\n    return project\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/status/","title":"status","text":""},{"location":"reference/fractal_server/app/routes/api/v2/status/#fractal_server.app.routes.api.v2.status.get_workflowtask_status","title":"<code>get_workflowtask_status(project_id, dataset_id, workflow_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Extract the status of all <code>WorkflowTaskV2</code> of a given <code>WorkflowV2</code> that ran on a given <code>DatasetV2</code>.</p> <p>NOTE: the current endpoint is not guaranteed to provide consistent results if the workflow task list is modified in a non-trivial way (that is, by adding intermediate tasks, removing tasks, or changing their order). See fractal-server GitHub issues: 793, 1083.</p> Source code in <code>fractal_server/app/routes/api/v2/status.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/status/\",\n    response_model=StatusReadV2,\n)\nasync def get_workflowtask_status(\n    project_id: int,\n    dataset_id: int,\n    workflow_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[StatusReadV2]:\n    \"\"\"\n    Extract the status of all `WorkflowTaskV2` of a given `WorkflowV2` that ran\n    on a given `DatasetV2`.\n\n    *NOTE*: the current endpoint is not guaranteed to provide consistent\n    results if the workflow task list is modified in a non-trivial way\n    (that is, by adding intermediate tasks, removing tasks, or changing their\n    order). See fractal-server GitHub issues: 793, 1083.\n    \"\"\"\n    # Get the dataset DB entry\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n\n    # Get the workflow DB entry\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Check whether there exists a submitted job associated to this\n    # workflow/dataset pair. If it does exist, it will be used later.\n    # If there are multiple jobs, raise an error.\n    stm = _get_submitted_jobs_statement()\n    stm = stm.where(JobV2.dataset_id == dataset_id)\n    stm = stm.where(JobV2.workflow_id == workflow_id)\n    res = await db.execute(stm)\n    running_jobs = res.scalars().all()\n    if len(running_jobs) == 0:\n        running_job = None\n    elif len(running_jobs) == 1:\n        running_job = running_jobs[0]\n    else:\n        string_ids = str([job.id for job in running_jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot get WorkflowTaskV2 statuses as DatasetV2 {dataset.id}\"\n                f\" is linked to multiple active jobs: {string_ids}.\"\n            ),\n        )\n\n    # Initialize empty dictionary for WorkflowTaskV2 status\n    workflow_tasks_status_dict: dict = {}\n\n    # Lowest priority: read status from DB, which corresponds to jobs that are\n    # not running\n    history = dataset.history\n    for history_item in history:\n        wftask_id = history_item[\"workflowtask\"][\"id\"]\n        wftask_status = history_item[\"status\"]\n        workflow_tasks_status_dict[wftask_id] = wftask_status\n\n    if running_job is None:\n        # If no job is running, the chronological-last history item is also the\n        # positional-last workflow task to be included in the response.\n        if len(dataset.history) &gt; 0:\n            last_valid_wftask_id = dataset.history[-1][\"workflowtask\"][\"id\"]\n        else:\n            last_valid_wftask_id = None\n    else:\n        # If a job is running, then gather more up-to-date information\n\n        # Mid priority: Set all WorkflowTask's that are part of the running job\n        # as \"submitted\"\n        start = running_job.first_task_index\n        end = running_job.last_task_index + 1\n        for wftask in workflow.task_list[start:end]:\n            workflow_tasks_status_dict[\n                wftask.id\n            ] = WorkflowTaskStatusTypeV2.SUBMITTED\n\n        # The last workflow task that is included in the submitted job is also\n        # the positional-last workflow task to be included in the response.\n        last_valid_wftask_id = workflow.task_list[end - 1]\n\n        # Highest priority: Read status updates coming from the running-job\n        # temporary file. Note: this file only contains information on\n        # WorkflowTask's that ran through successfully.\n        tmp_file = Path(running_job.working_dir) / HISTORY_FILENAME\n        try:\n            with tmp_file.open(\"r\") as f:\n                history = json.load(f)\n        except FileNotFoundError:\n            history = []\n        for history_item in history:\n            wftask_id = history_item[\"workflowtask\"][\"id\"]\n            wftask_status = history_item[\"status\"]\n            workflow_tasks_status_dict[wftask_id] = wftask_status\n\n    # Based on previously-gathered information, clean up the response body\n    clean_workflow_tasks_status_dict = {}\n    for wf_task in workflow.task_list:\n        wf_task_status = workflow_tasks_status_dict.get(wf_task.id)\n        if wf_task_status is None:\n            # If a wftask ID was not found, ignore it and continue\n            continue\n        clean_workflow_tasks_status_dict[wf_task.id] = wf_task_status\n        if wf_task_status == WorkflowTaskStatusTypeV2.FAILED:\n            # Starting from the beginning of `workflow.task_list`, stop the\n            # first time that you hit a failed job\n            break\n        if wf_task.id == last_valid_wftask_id:\n            # Starting from the beginning of `workflow.task_list`, stop the\n            # first time that you hit `last_valid_wftask_id``\n            break\n\n    response_body = StatusReadV2(status=clean_workflow_tasks_status_dict)\n    return response_body\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/submit/","title":"submit","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task/","title":"task","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.create_task","title":"<code>create_task(task, user=Depends(current_active_verified_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create a new task</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.post(\n    \"/\", response_model=TaskReadV2, status_code=status.HTTP_201_CREATED\n)\nasync def create_task(\n    task: TaskCreateV2,\n    user: User = Depends(current_active_verified_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[TaskReadV2]:\n    \"\"\"\n    Create a new task\n    \"\"\"\n\n    if task.command_non_parallel is None:\n        task_type = \"parallel\"\n    elif task.command_parallel is None:\n        task_type = \"non_parallel\"\n    else:\n        task_type = \"compound\"\n\n    if task_type == \"parallel\" and (\n        task.args_schema_non_parallel is not None\n        or task.meta_non_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                \"Cannot set `TaskV2.args_schema_non_parallel` or \"\n                \"`TaskV2.args_schema_non_parallel` if TaskV2 is parallel\"\n            ),\n        )\n    elif task_type == \"non_parallel\" and (\n        task.args_schema_parallel is not None or task.meta_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                \"Cannot set `TaskV2.args_schema_parallel` or \"\n                \"`TaskV2.args_schema_parallel` if TaskV2 is non_parallel\"\n            ),\n        )\n\n    # Set task.owner attribute\n    if user.username:\n        owner = user.username\n    elif user.slurm_user:\n        owner = user.slurm_user\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                \"Cannot add a new task because current user does not \"\n                \"have `username` or `slurm_user` attributes.\"\n            ),\n        )\n\n    # Prepend owner to task.source\n    task.source = f\"{owner}:{task.source}\"\n\n    # Verify that source is not already in use (note: this check is only useful\n    # to provide a user-friendly error message, but `task.source` uniqueness is\n    # already guaranteed by a constraint in the table definition).\n    stm = select(TaskV2).where(TaskV2.source == task.source)\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Source '{task.source}' already used by some TaskV2\",\n        )\n    stm = select(TaskV1).where(TaskV1.source == task.source)\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Source '{task.source}' already used by some TaskV1\",\n        )\n    # Add task\n    db_task = TaskV2(**task.dict(), owner=owner, type=task_type)\n    db.add(db_task)\n    await db.commit()\n    await db.refresh(db_task)\n    await db.close()\n    return db_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.delete_task","title":"<code>delete_task(task_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a task</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.delete(\"/{task_id}/\", status_code=204)\nasync def delete_task(\n    task_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a task\n    \"\"\"\n\n    db_task = await _get_task_check_owner(task_id=task_id, user=user, db=db)\n\n    # Check that the TaskV2 is not in relationship with some WorkflowTaskV2\n    stm = select(WorkflowTaskV2).filter(WorkflowTaskV2.task_id == task_id)\n    res = await db.execute(stm)\n    workflowtask_list = res.scalars().all()\n    if workflowtask_list:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot remove TaskV2 {task_id} because it is currently \"\n                \"imported in WorkflowsV2 \"\n                f\"{[x.workflow_id for x in workflowtask_list]}. \"\n                \"If you want to remove this task, then you should first remove\"\n                \" the workflows.\",\n            ),\n        )\n\n    await db.delete(db_task)\n    await db.commit()\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.get_list_task","title":"<code>get_list_task(args_schema_parallel=True, args_schema_non_parallel=True, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get list of available tasks</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.get(\"/\", response_model=list[TaskReadV2])\nasync def get_list_task(\n    args_schema_parallel: bool = True,\n    args_schema_non_parallel: bool = True,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[TaskReadV2]:\n    \"\"\"\n    Get list of available tasks\n    \"\"\"\n    stm = select(TaskV2)\n    res = await db.execute(stm)\n    task_list = res.scalars().all()\n    await db.close()\n    if args_schema_parallel is False:\n        for task in task_list:\n            setattr(task, \"args_schema_parallel\", None)\n    if args_schema_non_parallel is False:\n        for task in task_list:\n            setattr(task, \"args_schema_non_parallel\", None)\n\n    return task_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.get_task","title":"<code>get_task(task_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on a specific task</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.get(\"/{task_id}/\", response_model=TaskReadV2)\nasync def get_task(\n    task_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskReadV2:\n    \"\"\"\n    Get info on a specific task\n    \"\"\"\n    task = await db.get(TaskV2, task_id)\n    await db.close()\n    if not task:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"TaskV2 not found\"\n        )\n    return task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.patch_task","title":"<code>patch_task(task_id, task_update, user=Depends(current_active_verified_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a specific task (restricted to superusers and task owner)</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.patch(\"/{task_id}/\", response_model=TaskReadV2)\nasync def patch_task(\n    task_id: int,\n    task_update: TaskUpdateV2,\n    user: User = Depends(current_active_verified_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[TaskReadV2]:\n    \"\"\"\n    Edit a specific task (restricted to superusers and task owner)\n    \"\"\"\n\n    # Retrieve task from database\n    db_task = await _get_task_check_owner(task_id=task_id, user=user, db=db)\n    update = task_update.dict(exclude_unset=True)\n\n    # Forbid changes that set a previously unset command\n    if db_task.type == \"non_parallel\" and \"command_parallel\" in update:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"Cannot set an unset `command_parallel`.\",\n        )\n    if db_task.type == \"parallel\" and \"command_non_parallel\" in update:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"Cannot set an unset `command_non_parallel`.\",\n        )\n\n    for key, value in update.items():\n        setattr(db_task, key, value)\n\n    await db.commit()\n    await db.refresh(db_task)\n    await db.close()\n    return db_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_collection/","title":"task_collection","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task_collection/#fractal_server.app.routes.api.v2.task_collection.check_collection_status","title":"<code>check_collection_status(state_id, user=Depends(current_active_user), verbose=False, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Check status of background task collection</p> Source code in <code>fractal_server/app/routes/api/v2/task_collection.py</code> <pre><code>@router.get(\"/collect/{state_id}/\", response_model=StateRead)\nasync def check_collection_status(\n    state_id: int,\n    user: User = Depends(current_active_user),\n    verbose: bool = False,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StateRead:  # State[TaskCollectStatus]\n    \"\"\"\n    Check status of background task collection\n    \"\"\"\n    logger = set_logger(logger_name=\"check_collection_status\")\n    logger.debug(f\"Querying state for state.id={state_id}\")\n    state = await db.get(CollectionStateV2, state_id)\n    if not state:\n        await db.close()\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"No task collection info with id={state_id}\",\n        )\n    data = TaskCollectStatusV2(**state.data)\n\n    # In some cases (i.e. a successful or ongoing task collection), data.log is\n    # not set; if so, we collect the current logs\n    if verbose and not data.log:\n        data.log = get_collection_log(data.venv_path)\n        state.data = data.sanitised_dict()\n    close_logger(logger)\n    await db.close()\n    return state\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_collection/#fractal_server.app.routes.api.v2.task_collection.collect_tasks_pip","title":"<code>collect_tasks_pip(task_collect, background_tasks, response, user=Depends(current_active_verified_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Task collection endpoint</p> <p>Trigger the creation of a dedicated virtual environment, the installation of a package and the collection of tasks as advertised in the manifest.</p> Source code in <code>fractal_server/app/routes/api/v2/task_collection.py</code> <pre><code>@router.post(\n    \"/collect/pip/\",\n    response_model=StateRead,\n    responses={\n        201: dict(\n            description=(\n                \"Task collection successfully started in the background\"\n            )\n        ),\n        200: dict(\n            description=(\n                \"Package already collected. Returning info on already \"\n                \"available tasks\"\n            )\n        ),\n    },\n)\nasync def collect_tasks_pip(\n    task_collect: TaskCollectPipV2,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    user: User = Depends(current_active_verified_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StateRead:  # State[TaskCollectStatus]\n    \"\"\"\n    Task collection endpoint\n\n    Trigger the creation of a dedicated virtual environment, the installation\n    of a package and the collection of tasks as advertised in the manifest.\n    \"\"\"\n\n    logger = set_logger(logger_name=\"collect_tasks_pip\")\n\n    # Validate payload as _TaskCollectPip, which has more strict checks than\n    # TaskCollectPip\n    try:\n        task_pkg = _TaskCollectPip(**task_collect.dict(exclude_unset=True))\n    except ValidationError as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Invalid task-collection object. Original error: {e}\",\n        )\n\n    with TemporaryDirectory() as tmpdir:\n        try:\n            # Copy or download the package wheel file to tmpdir\n            if task_pkg.is_local_package:\n                shell_copy(task_pkg.package_path.as_posix(), tmpdir)\n                pkg_path = Path(tmpdir) / task_pkg.package_path.name\n            else:\n                pkg_path = await download_package(\n                    task_pkg=task_pkg, dest=tmpdir\n                )\n            # Read package info from wheel file, and override the ones coming\n            # from the request body\n            pkg_info = inspect_package(pkg_path)\n            task_pkg.package_name = pkg_info[\"pkg_name\"]\n            task_pkg.package_version = pkg_info[\"pkg_version\"]\n            task_pkg.package_manifest = pkg_info[\"pkg_manifest\"]\n            task_pkg.check()\n        except Exception as e:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=f\"Invalid package or manifest. Original error: {e}\",\n            )\n\n    try:\n        venv_path = create_package_dir_pip(task_pkg=task_pkg)\n    except FileExistsError:\n        venv_path = create_package_dir_pip(task_pkg=task_pkg, create=False)\n        try:\n            task_collect_status = get_collection_data(venv_path)\n            for task in task_collect_status.task_list:\n                db_task = await db.get(TaskV2, task.id)\n                if (\n                    (not db_task)\n                    or db_task.source != task.source\n                    or db_task.name != task.name\n                ):\n                    await db.close()\n                    raise HTTPException(\n                        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                        detail=(\n                            \"Cannot collect package. Folder already exists, \"\n                            f\"but task {task.id} does not exists or it does \"\n                            f\"not have the expected source ({task.source}) or \"\n                            f\"name ({task.name}).\"\n                        ),\n                    )\n        except FileNotFoundError as e:\n            await db.close()\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=(\n                    \"Cannot collect package. Possible reason: another \"\n                    \"collection of the same package is in progress. \"\n                    f\"Original FileNotFoundError: {e}\"\n                ),\n            )\n        except ValidationError as e:\n            await db.close()\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=(\n                    \"Cannot collect package. Possible reason: an old version \"\n                    \"of the same package has already been collected. \"\n                    f\"Original ValidationError: {e}\"\n                ),\n            )\n        task_collect_status.info = \"Already installed\"\n        state = CollectionStateV2(data=task_collect_status.sanitised_dict())\n        response.status_code == status.HTTP_200_OK\n        await db.close()\n        return state\n    settings = Inject(get_settings)\n\n    # Check that tasks are not already in the DB\n    for new_task in task_pkg.package_manifest.task_list:\n        new_task_name_slug = slugify_task_name(new_task.name)\n        new_task_source = f\"{task_pkg.package_source}:{new_task_name_slug}\"\n        stm = select(TaskV2).where(TaskV2.source == new_task_source)\n        res = await db.execute(stm)\n        if res.scalars().all():\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=(\n                    \"Cannot collect package. Task with source \"\n                    f'\"{new_task_source}\" already exists in the database.'\n                ),\n            )\n\n    # All checks are OK, proceed with task collection\n    full_venv_path = venv_path.relative_to(settings.FRACTAL_TASKS_DIR)\n    collection_status = TaskCollectStatusV2(\n        status=\"pending\", venv_path=full_venv_path, package=task_pkg.package\n    )\n\n    # Create State object (after casting venv_path to string)\n    collection_status_dict = collection_status.dict()\n    collection_status_dict[\"venv_path\"] = str(collection_status.venv_path)\n    state = CollectionStateV2(data=collection_status_dict)\n    db.add(state)\n    await db.commit()\n    await db.refresh(state)\n\n    background_tasks.add_task(\n        background_collect_pip,\n        state_id=state.id,\n        venv_path=venv_path,\n        task_pkg=task_pkg,\n    )\n    logger.debug(\n        \"Task-collection endpoint: start background collection \"\n        \"and return state\"\n    )\n    close_logger(logger)\n    info = (\n        \"Collecting tasks in the background. \"\n        f\"GET /task/collect/{state.id} to query collection status\"\n    )\n    state.data[\"info\"] = info\n    response.status_code = status.HTTP_201_CREATED\n    await db.close()\n\n    return state\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_legacy/","title":"task_legacy","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task_legacy/#fractal_server.app.routes.api.v2.task_legacy.get_list_task_legacy","title":"<code>get_list_task_legacy(args_schema=True, only_v2_compatible=False, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get list of available legacy tasks</p> Source code in <code>fractal_server/app/routes/api/v2/task_legacy.py</code> <pre><code>@router.get(\"/\", response_model=list[TaskLegacyReadV2])\nasync def get_list_task_legacy(\n    args_schema: bool = True,\n    only_v2_compatible: bool = False,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[TaskLegacyReadV2]:\n    \"\"\"\n    Get list of available legacy tasks\n    \"\"\"\n    stm = select(TaskV1)\n    if only_v2_compatible:\n        stm = stm.where(TaskV1.is_v2_compatible)\n    res = await db.execute(stm)\n    task_list = res.scalars().all()\n    await db.close()\n    if args_schema is False:\n        for task in task_list:\n            setattr(task, \"args_schema\", None)\n\n    return task_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_legacy/#fractal_server.app.routes.api.v2.task_legacy.get_task_legacy","title":"<code>get_task_legacy(task_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on a specific legacy task</p> Source code in <code>fractal_server/app/routes/api/v2/task_legacy.py</code> <pre><code>@router.get(\"/{task_id}/\", response_model=TaskLegacyReadV2)\nasync def get_task_legacy(\n    task_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskLegacyReadV2:\n    \"\"\"\n    Get info on a specific legacy task\n    \"\"\"\n    task = await db.get(TaskV1, task_id)\n    await db.close()\n    if not task:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"TaskV1[{task_id}] not found\",\n        )\n    return task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/","title":"workflow","text":""},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.create_workflow","title":"<code>create_workflow(project_id, workflow, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create a workflow, associate to a project</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/\",\n    response_model=WorkflowReadV2,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_workflow(\n    project_id: int,\n    workflow: WorkflowCreateV2,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowReadV2]:\n    \"\"\"\n    Create a workflow, associate to a project\n    \"\"\"\n    await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    await _check_workflow_exists(\n        name=workflow.name, project_id=project_id, db=db\n    )\n\n    db_workflow = WorkflowV2(project_id=project_id, **workflow.dict())\n    db.add(db_workflow)\n    await db.commit()\n    await db.refresh(db_workflow)\n    await db.close()\n    return db_workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.delete_workflow","title":"<code>delete_workflow(project_id, workflow_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    status_code=status.HTTP_204_NO_CONTENT,\n)\nasync def delete_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current workflow.\n    stm = _get_submitted_jobs_statement().where(\n        JobV2.workflow_id == workflow.id\n    )\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"Cannot delete workflow {workflow.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Cascade operations: set foreign-keys to null for jobs which are in\n    # relationship with the current workflow\n    stm = select(JobV2).where(JobV2.workflow_id == workflow_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    for job in jobs:\n        job.workflow_id = None\n\n    # Delete workflow\n    await db.delete(workflow)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.export_worfklow","title":"<code>export_worfklow(project_id, workflow_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Export an existing workflow, after stripping all IDs</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/export/\",\n    response_model=WorkflowExportV2,\n)\nasync def export_worfklow(\n    project_id: int,\n    workflow_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowExportV2]:\n    \"\"\"\n    Export an existing workflow, after stripping all IDs\n    \"\"\"\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n    # Emit a warning when exporting a workflow with custom tasks\n    logger = set_logger(None)\n    for wftask in workflow.task_list:\n        if wftask.is_legacy_task:\n            if wftask.task_legacy.owner is not None:\n                logger.warning(\n                    f\"Custom tasks (like the one with \"\n                    f\"id={wftask.task_legacy_id} and \"\n                    f\"source='{wftask.task_legacy.source}') are not meant to \"\n                    \"be portable; re-importing this workflow may not work as \"\n                    \"expected.\"\n                )\n        else:\n            if wftask.task.owner is not None:\n                logger.warning(\n                    f\"Custom tasks (like the one with id={wftask.task_id} and \"\n                    f'source=\"{wftask.task.source}\") are not meant to be '\n                    \"portable; re-importing this workflow may not work as \"\n                    \"expected.\"\n                )\n    close_logger(logger)\n\n    await db.close()\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.get_user_workflows","title":"<code>get_user_workflows(user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the workflows of the current user</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\"/workflow/\", response_model=list[WorkflowReadV2])\nasync def get_user_workflows(\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[WorkflowReadV2]:\n    \"\"\"\n    Returns all the workflows of the current user\n    \"\"\"\n    stm = select(WorkflowV2)\n    stm = stm.join(ProjectV2).where(\n        ProjectV2.user_list.any(User.id == user.id)\n    )\n    res = await db.execute(stm)\n    workflow_list = res.scalars().all()\n    return workflow_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.get_workflow_list","title":"<code>get_workflow_list(project_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get workflow list for given project</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/\",\n    response_model=list[WorkflowReadV2],\n)\nasync def get_workflow_list(\n    project_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[list[WorkflowReadV2]]:\n    \"\"\"\n    Get workflow list for given project\n    \"\"\"\n    # Access control\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    # Find workflows of the current project. Note: this select/where approach\n    # has much better scaling than refreshing all elements of\n    # `project.workflow_list` - ref\n    # https://github.com/fractal-analytics-platform/fractal-server/pull/1082#issuecomment-1856676097.\n    stm = select(WorkflowV2).where(WorkflowV2.project_id == project.id)\n    workflow_list = (await db.execute(stm)).scalars().all()\n    return workflow_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.import_workflow","title":"<code>import_workflow(project_id, workflow, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Import an existing workflow into a project</p> <p>Also create all required objects (i.e. Workflow and WorkflowTask's) along the way.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/import/\",\n    response_model=WorkflowReadV2,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def import_workflow(\n    project_id: int,\n    workflow: WorkflowImportV2,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowReadV2]:\n    \"\"\"\n    Import an existing workflow into a project\n\n    Also create all required objects (i.e. Workflow and WorkflowTask's) along\n    the way.\n    \"\"\"\n\n    # Preliminary checks\n    await _get_project_check_owner(\n        project_id=project_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    await _check_workflow_exists(\n        name=workflow.name, project_id=project_id, db=db\n    )\n\n    # Check that all required tasks are available\n    source_to_id = {}\n    source_to_id_legacy = {}\n\n    for wf_task in workflow.task_list:\n\n        if wf_task.is_legacy_task is True:\n            source = wf_task.task_legacy.source\n            if source not in source_to_id_legacy.keys():\n                stm = select(TaskV1).where(TaskV1.source == source)\n                tasks_by_source = (await db.execute(stm)).scalars().all()\n                if len(tasks_by_source) != 1:\n                    raise HTTPException(\n                        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                        detail=(\n                            f\"Found {len(tasks_by_source)} tasks legacy \"\n                            f\"with {source=}.\"\n                        ),\n                    )\n                source_to_id_legacy[source] = tasks_by_source[0].id\n        else:\n            source = wf_task.task.source\n            if source not in source_to_id.keys():\n                stm = select(TaskV2).where(TaskV2.source == source)\n                tasks_by_source = (await db.execute(stm)).scalars().all()\n                if len(tasks_by_source) != 1:\n                    raise HTTPException(\n                        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                        detail=(\n                            f\"Found {len(tasks_by_source)} tasks \"\n                            f\"with {source=}.\"\n                        ),\n                    )\n                source_to_id[source] = tasks_by_source[0].id\n\n    # Create new Workflow (with empty task_list)\n    db_workflow = WorkflowV2(\n        project_id=project_id,\n        **workflow.dict(exclude_none=True, exclude={\"task_list\"}),\n    )\n    db.add(db_workflow)\n    await db.commit()\n    await db.refresh(db_workflow)\n\n    # Insert tasks\n\n    for wf_task in workflow.task_list:\n        if wf_task.is_legacy_task is True:\n            source = wf_task.task_legacy.source\n            task_id = source_to_id_legacy[source]\n        else:\n            source = wf_task.task.source\n            task_id = source_to_id[source]\n\n        new_wf_task = WorkflowTaskCreateV2(\n            **wf_task.dict(exclude_none=True, exclude={\"task\", \"task_legacy\"})\n        )\n        # Insert task\n        await _workflow_insert_task(\n            **new_wf_task.dict(),\n            workflow_id=db_workflow.id,\n            task_id=task_id,\n            db=db,\n        )\n\n    await db.close()\n    return db_workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.read_workflow","title":"<code>read_workflow(project_id, workflow_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on an existing workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    response_model=WorkflowReadV2,\n)\nasync def read_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowReadV2]:\n    \"\"\"\n    Get info on an existing workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.update_workflow","title":"<code>update_workflow(project_id, workflow_id, patch, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    response_model=WorkflowReadV2,\n)\nasync def update_workflow(\n    project_id: int,\n    workflow_id: int,\n    patch: WorkflowUpdateV2,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowReadV2]:\n    \"\"\"\n    Edit a workflow\n    \"\"\"\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    if patch.name:\n        await _check_workflow_exists(\n            name=patch.name, project_id=project_id, db=db\n        )\n\n    for key, value in patch.dict(exclude_unset=True).items():\n        if key == \"reordered_workflowtask_ids\":\n            current_workflowtask_ids = [\n                wftask.id for wftask in workflow.task_list\n            ]\n            num_tasks = len(workflow.task_list)\n            if len(value) != num_tasks or set(value) != set(\n                current_workflowtask_ids\n            ):\n                raise HTTPException(\n                    status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                    detail=(\n                        \"`reordered_workflowtask_ids` must be a permutation of\"\n                        f\" {current_workflowtask_ids} (given {value})\"\n                    ),\n                )\n            for ind_wftask in range(num_tasks):\n                new_order = value.index(workflow.task_list[ind_wftask].id)\n                workflow.task_list[ind_wftask].order = new_order\n        else:\n            setattr(workflow, key, value)\n\n    await db.commit()\n    await db.refresh(workflow)\n    await db.close()\n\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflowtask/","title":"workflowtask","text":""},{"location":"reference/fractal_server/app/routes/api/v2/workflowtask/#fractal_server.app.routes.api.v2.workflowtask.create_workflowtask","title":"<code>create_workflowtask(project_id, workflow_id, task_id, new_task, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add a WorkflowTask to a Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflowtask.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/\",\n    response_model=WorkflowTaskReadV2,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    task_id: int,\n    new_task: WorkflowTaskCreateV2,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowTaskReadV2]:\n    \"\"\"\n    Add a WorkflowTask to a Workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n\n    if new_task.is_legacy_task is True:\n        task = await db.get(Task, task_id)\n        if not task:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND,\n                detail=f\"Task {task_id} not found.\",\n            )\n        if not task.is_v2_compatible:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=f\"Task {task_id} is not V2-compatible.\",\n            )\n    else:\n        task = await db.get(TaskV2, task_id)\n        if not task:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND,\n                detail=f\"TaskV2 {task_id} not found.\",\n            )\n\n    if new_task.is_legacy_task is True or task.type == \"parallel\":\n        if (\n            new_task.meta_non_parallel is not None\n            or new_task.args_non_parallel is not None\n        ):\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=(\n                    \"Cannot set `WorkflowTaskV2.meta_non_parallel` or \"\n                    \"`WorkflowTask.args_non_parallel` if the associated Task \"\n                    \"is `parallel` (or legacy).\"\n                ),\n            )\n    elif task.type == \"non_parallel\":\n        if (\n            new_task.meta_parallel is not None\n            or new_task.args_parallel is not None\n        ):\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=(\n                    \"Cannot set `WorkflowTaskV2.meta_parallel` or \"\n                    \"`WorkflowTask.args_parallel` if the associated Task \"\n                    \"is `non_parallel`.\"\n                ),\n            )\n\n    workflow_task = await _workflow_insert_task(\n        workflow_id=workflow.id,\n        is_legacy_task=new_task.is_legacy_task,\n        task_id=task_id,\n        order=new_task.order,\n        meta_non_parallel=new_task.meta_non_parallel,\n        meta_parallel=new_task.meta_parallel,\n        args_non_parallel=new_task.args_non_parallel,\n        args_parallel=new_task.args_parallel,\n        input_filters=new_task.input_filters,\n        db=db,\n    )\n\n    await db.close()\n\n    return workflow_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflowtask/#fractal_server.app.routes.api.v2.workflowtask.delete_workflowtask","title":"<code>delete_workflowtask(project_id, workflow_id, workflow_task_id, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a WorkflowTask of a Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflowtask.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}/\",\n    status_code=status.HTTP_204_NO_CONTENT,\n)\nasync def delete_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a WorkflowTask of a Workflow\n    \"\"\"\n\n    db_workflow_task, db_workflow = await _get_workflow_task_check_owner(\n        project_id=project_id,\n        workflow_task_id=workflow_task_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    await db.delete(db_workflow_task)\n    await db.commit()\n\n    await db.refresh(db_workflow)\n    db_workflow.task_list.reorder()\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflowtask/#fractal_server.app.routes.api.v2.workflowtask.update_workflowtask","title":"<code>update_workflowtask(project_id, workflow_id, workflow_task_id, workflow_task_update, user=Depends(current_active_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a WorkflowTask of a Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflowtask.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}/\",\n    response_model=WorkflowTaskReadV2,\n)\nasync def update_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    workflow_task_update: WorkflowTaskUpdateV2,\n    user: User = Depends(current_active_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Optional[WorkflowTaskReadV2]:\n    \"\"\"\n    Edit a WorkflowTask of a Workflow\n    \"\"\"\n\n    db_wf_task, db_workflow = await _get_workflow_task_check_owner(\n        project_id=project_id,\n        workflow_task_id=workflow_task_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    if db_wf_task.task_type == \"parallel\" and (\n        workflow_task_update.args_non_parallel is not None\n        or workflow_task_update.meta_non_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                \"Cannot patch `WorkflowTaskV2.args_non_parallel` or \"\n                \"`WorkflowTask.meta_non_parallel` if the associated Task is \"\n                \"parallel.\"\n            ),\n        )\n    elif db_wf_task.task_type == \"non_parallel\" and (\n        workflow_task_update.args_parallel is not None\n        or workflow_task_update.meta_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                \"Cannot patch `WorkflowTaskV2.args_parallel` or \"\n                \"`WorkflowTask.meta_parallel` if the associated Task is \"\n                \"non parallel.\"\n            ),\n        )\n\n    for key, value in workflow_task_update.dict(exclude_unset=True).items():\n        if key == \"args_parallel\":\n            # Get default arguments via a Task property method\n            if db_wf_task.is_legacy_task:\n                default_args = (\n                    db_wf_task.task_legacy.default_args_from_args_schema\n                )\n            else:\n                default_args = (\n                    db_wf_task.task.default_args_parallel_from_args_schema\n                )\n            # Override default_args with args value items\n            actual_args = deepcopy(default_args)\n            if value is not None:\n                for k, v in value.items():\n                    actual_args[k] = v\n            if not actual_args:\n                actual_args = None\n            setattr(db_wf_task, key, actual_args)\n        elif key == \"args_non_parallel\":\n            # Get default arguments via a Task property method\n            if db_wf_task.is_legacy_task:\n                # This is only needed so that we don't have to modify the rest\n                # of this block, but legacy task cannot take any non-parallel\n                # args (see checks above).\n                default_args = {}\n            else:\n                default_args = deepcopy(\n                    db_wf_task.task.default_args_non_parallel_from_args_schema\n                )\n            # Override default_args with args value items\n            actual_args = default_args.copy()\n            if value is not None:\n                for k, v in value.items():\n                    actual_args[k] = v\n            if not actual_args:\n                actual_args = None\n            setattr(db_wf_task, key, actual_args)\n        elif key in [\"meta_parallel\", \"meta_non_parallel\", \"input_filters\"]:\n            setattr(db_wf_task, key, value)\n        else:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=f\"patch_workflow_task endpoint cannot set {key=}\",\n            )\n\n    await db.commit()\n    await db.refresh(db_wf_task)\n    await db.close()\n\n    return db_wf_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/aux/","title":"aux","text":""},{"location":"reference/fractal_server/app/routes/aux/_job/","title":"_job","text":""},{"location":"reference/fractal_server/app/routes/aux/_job/#fractal_server.app.routes.aux._job._write_shutdown_file","title":"<code>_write_shutdown_file(*, job)</code>","text":"<p>Write job's shutdown file.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>ApplyWorkflow</code> required <p>Note: we are not marking the job as failed (by setting its <code>status</code> attribute) here, since this will be done by the runner backend as soon as it detects the shutdown-trigerring file and performs the actual shutdown.</p> Source code in <code>fractal_server/app/routes/aux/_job.py</code> <pre><code>def _write_shutdown_file(*, job: ApplyWorkflow):\n    \"\"\"\n    Write job's shutdown file.\n\n    Args:\n        job:\n\n    Note: we are **not** marking the job as failed (by setting its `status`\n    attribute) here, since this will be done by the runner backend as soon as\n    it detects the shutdown-trigerring file and performs the actual shutdown.\n    \"\"\"\n    shutdown_file = Path(job.working_dir) / SHUTDOWN_FILENAME\n    with shutdown_file.open(\"w\") as f:\n        f.write(f\"Trigger executor shutdown for {job.id=}.\")\n</code></pre>"},{"location":"reference/fractal_server/app/routes/aux/_job/#fractal_server.app.routes.aux._job._zip_folder_to_byte_stream","title":"<code>_zip_folder_to_byte_stream(*, folder, zip_filename)</code>","text":"<p>Get byte stream with the zipped log folder of a job.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>the folder to zip</p> required <code>zip_filename</code> <code>str</code> <p>name of the zipped archive</p> required Source code in <code>fractal_server/app/routes/aux/_job.py</code> <pre><code>def _zip_folder_to_byte_stream(*, folder: str, zip_filename: str) -&gt; BytesIO:\n    \"\"\"\n    Get byte stream with the zipped log folder of a job.\n\n    Args:\n        folder: the folder to zip\n        zip_filename: name of the zipped archive\n    \"\"\"\n    working_dir_path = Path(folder)\n\n    byte_stream = BytesIO()\n    with ZipFile(byte_stream, mode=\"w\", compression=ZIP_DEFLATED) as zipfile:\n        for fpath in working_dir_path.glob(\"*\"):\n            zipfile.write(filename=str(fpath), arcname=str(fpath.name))\n\n    return byte_stream\n</code></pre>"},{"location":"reference/fractal_server/app/routes/aux/_runner/","title":"_runner","text":""},{"location":"reference/fractal_server/app/routes/aux/_runner/#fractal_server.app.routes.aux._runner._check_backend_is_slurm","title":"<code>_check_backend_is_slurm()</code>","text":"<p>Raises:</p> Type Description <code>HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY)</code> <p>If FRACTAL_RUNNER_BACKEND is not 'slurm'</p> Source code in <code>fractal_server/app/routes/aux/_runner.py</code> <pre><code>def _check_backend_is_slurm():\n    \"\"\"\n    Raises:\n        HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY):\n            If FRACTAL_RUNNER_BACKEND is not 'slurm'\n    \"\"\"\n    settings = Inject(get_settings)\n    backend = settings.FRACTAL_RUNNER_BACKEND\n    if backend != \"slurm\":\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                \"Stopping a job execution is not implemented for \"\n                f\"FRACTAL_RUNNER_BACKEND={backend}.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/runner/","title":"runner","text":""},{"location":"reference/fractal_server/app/runner/async_wrap/","title":"async_wrap","text":""},{"location":"reference/fractal_server/app/runner/async_wrap/#fractal_server.app.runner.async_wrap.async_wrap","title":"<code>async_wrap(func)</code>","text":"<p>Wrap a synchronous callable in an async task</p> <p>Ref: issue #140 and this StackOverflow answer.</p> <p>Returns:</p> Name Type Description <code>async_wrapper</code> <code>Callable</code> <p>A factory that allows wrapping a blocking callable within a coroutine.</p> Source code in <code>fractal_server/app/runner/async_wrap.py</code> <pre><code>def async_wrap(func: Callable) -&gt; Callable:\n    \"\"\"\n    Wrap a synchronous callable in an async task\n\n    Ref: [issue #140](https://github.com/fractal-analytics-platform/fractal-server/issues/140)\n    and [this StackOverflow answer](https://stackoverflow.com/q/43241221/19085332).\n\n    Returns:\n        async_wrapper:\n            A factory that allows wrapping a blocking callable within a\n            coroutine.\n    \"\"\"  # noqa: E501\n\n    @wraps(func)\n    async def async_wrapper(*args, loop=None, executor=None, **kwargs):\n        if loop is None:\n            loop = asyncio.get_event_loop()\n        pfunc = partial(func, *args, **kwargs)\n        return await loop.run_in_executor(executor, pfunc)\n\n    return async_wrapper\n</code></pre>"},{"location":"reference/fractal_server/app/runner/components/","title":"components","text":""},{"location":"reference/fractal_server/app/runner/exceptions/","title":"exceptions","text":""},{"location":"reference/fractal_server/app/runner/exceptions/#fractal_server.app.runner.exceptions.JobExecutionError","title":"<code>JobExecutionError</code>","text":"<p>             Bases: <code>RuntimeError</code></p> <p>Forwards errors in the execution of a task that are due to external factors</p> <p>This error wraps and forwards errors occurred during the execution of tasks, but related to external factors like:</p> <ol> <li>A negative exit code (e.g. because the task received a TERM or KILL    signal);</li> <li>An error on the executor side (e.g. the SLURM executor could not    find the pickled file with task output).</li> </ol> <p>This error also adds information that is useful to track down and debug the failing task within a workflow.</p> <p>Attributes:</p> Name Type Description <code>info</code> <code>Optional[str]</code> <p>A free field for additional information</p> <code>cmd_file</code> <code>Optional[str]</code> <p>Path to the file of the command that was executed (e.g. a SLURM submission script).</p> <code>stdout_file</code> <code>Optional[str]</code> <p>Path to the file with the command stdout</p> <code>stderr_file</code> <code>Optional[str]</code> <p>Path to the file with the command stderr</p> Source code in <code>fractal_server/app/runner/exceptions.py</code> <pre><code>class JobExecutionError(RuntimeError):\n    \"\"\"\n    Forwards errors in the execution of a task that are due to external factors\n\n    This error wraps and forwards errors occurred during the execution of\n    tasks, but related to external factors like:\n\n    1. A negative exit code (e.g. because the task received a TERM or KILL\n       signal);\n    2. An error on the executor side (e.g. the SLURM executor could not\n       find the pickled file with task output).\n\n    This error also adds information that is useful to track down and debug the\n    failing task within a workflow.\n\n    Attributes:\n        info:\n            A free field for additional information\n        cmd_file:\n            Path to the file of the command that was executed (e.g. a SLURM\n            submission script).\n        stdout_file:\n            Path to the file with the command stdout\n        stderr_file:\n            Path to the file with the command stderr\n    \"\"\"\n\n    cmd_file: Optional[str] = None\n    stdout_file: Optional[str] = None\n    stderr_file: Optional[str] = None\n    info: Optional[str] = None\n\n    def __init__(\n        self,\n        *args,\n        cmd_file: Optional[str] = None,\n        stdout_file: Optional[str] = None,\n        stderr_file: Optional[str] = None,\n        info: Optional[str] = None,\n    ):\n        super().__init__(*args)\n        self.cmd_file = cmd_file\n        self.stdout_file = stdout_file\n        self.stderr_file = stderr_file\n        self.info = info\n\n    def _read_file(self, filepath: str) -&gt; str:\n        \"\"\"\n        Return the content of a text file, and handle the cases where it is\n        empty or missing\n        \"\"\"\n        if os.path.exists(filepath):\n            with open(filepath, \"r\") as f:\n                content = f.read()\n                if content:\n                    return f\"Content of {filepath}:\\n{content}\"\n                else:\n                    return f\"File {filepath} is empty\\n\"\n        else:\n            return f\"File {filepath} is missing\\n\"\n\n    def assemble_error(self) -&gt; str:\n        \"\"\"\n        Read the files that are specified in attributes, and combine them in an\n        error message.\n        \"\"\"\n        if self.cmd_file:\n            content = self._read_file(self.cmd_file)\n            cmd_content = f\"COMMAND:\\n{content}\\n\\n\"\n        else:\n            cmd_content = \"\"\n        if self.stdout_file:\n            content = self._read_file(self.stdout_file)\n            out_content = f\"STDOUT:\\n{content}\\n\\n\"\n        else:\n            out_content = \"\"\n        if self.stderr_file:\n            content = self._read_file(self.stderr_file)\n            err_content = f\"STDERR:\\n{content}\\n\\n\"\n        else:\n            err_content = \"\"\n\n        content = f\"{cmd_content}{out_content}{err_content}\"\n        if self.info:\n            content = f\"{content}ADDITIONAL INFO:\\n{self.info}\\n\\n\"\n\n        if not content:\n            content = str(self)\n        message = f\"JobExecutionError\\n\\n{content}\"\n        return message\n</code></pre>"},{"location":"reference/fractal_server/app/runner/exceptions/#fractal_server.app.runner.exceptions.JobExecutionError._read_file","title":"<code>_read_file(filepath)</code>","text":"<p>Return the content of a text file, and handle the cases where it is empty or missing</p> Source code in <code>fractal_server/app/runner/exceptions.py</code> <pre><code>def _read_file(self, filepath: str) -&gt; str:\n    \"\"\"\n    Return the content of a text file, and handle the cases where it is\n    empty or missing\n    \"\"\"\n    if os.path.exists(filepath):\n        with open(filepath, \"r\") as f:\n            content = f.read()\n            if content:\n                return f\"Content of {filepath}:\\n{content}\"\n            else:\n                return f\"File {filepath} is empty\\n\"\n    else:\n        return f\"File {filepath} is missing\\n\"\n</code></pre>"},{"location":"reference/fractal_server/app/runner/exceptions/#fractal_server.app.runner.exceptions.JobExecutionError.assemble_error","title":"<code>assemble_error()</code>","text":"<p>Read the files that are specified in attributes, and combine them in an error message.</p> Source code in <code>fractal_server/app/runner/exceptions.py</code> <pre><code>def assemble_error(self) -&gt; str:\n    \"\"\"\n    Read the files that are specified in attributes, and combine them in an\n    error message.\n    \"\"\"\n    if self.cmd_file:\n        content = self._read_file(self.cmd_file)\n        cmd_content = f\"COMMAND:\\n{content}\\n\\n\"\n    else:\n        cmd_content = \"\"\n    if self.stdout_file:\n        content = self._read_file(self.stdout_file)\n        out_content = f\"STDOUT:\\n{content}\\n\\n\"\n    else:\n        out_content = \"\"\n    if self.stderr_file:\n        content = self._read_file(self.stderr_file)\n        err_content = f\"STDERR:\\n{content}\\n\\n\"\n    else:\n        err_content = \"\"\n\n    content = f\"{cmd_content}{out_content}{err_content}\"\n    if self.info:\n        content = f\"{content}ADDITIONAL INFO:\\n{self.info}\\n\\n\"\n\n    if not content:\n        content = str(self)\n    message = f\"JobExecutionError\\n\\n{content}\"\n    return message\n</code></pre>"},{"location":"reference/fractal_server/app/runner/exceptions/#fractal_server.app.runner.exceptions.TaskExecutionError","title":"<code>TaskExecutionError</code>","text":"<p>             Bases: <code>RuntimeError</code></p> <p>Forwards errors occurred during the execution of a task</p> <p>This error wraps and forwards errors occurred during the execution of tasks, when the exit code is larger than 0 (i.e. the error took place within the task). This error also adds information that is useful to track down and debug the failing task within a workflow.</p> <p>Attributes:</p> Name Type Description <code>workflow_task_id</code> <code>Optional[int]</code> <p>ID of the workflow task that failed.</p> <code>workflow_task_order</code> <code>Optional[int]</code> <p>Order of the task within the workflow.</p> <code>task_name</code> <code>Optional[str]</code> <p>Human readable name of the failing task.</p> Source code in <code>fractal_server/app/runner/exceptions.py</code> <pre><code>class TaskExecutionError(RuntimeError):\n    \"\"\"\n    Forwards errors occurred during the execution of a task\n\n    This error wraps and forwards errors occurred during the execution of\n    tasks, when the exit code is larger than 0 (i.e. the error took place\n    within the task). This error also adds information that is useful to track\n    down and debug the failing task within a workflow.\n\n    Attributes:\n        workflow_task_id:\n            ID of the workflow task that failed.\n        workflow_task_order:\n            Order of the task within the workflow.\n        task_name:\n            Human readable name of the failing task.\n    \"\"\"\n\n    workflow_task_id: Optional[int] = None\n    workflow_task_order: Optional[int] = None\n    task_name: Optional[str] = None\n\n    def __init__(\n        self,\n        *args,\n        workflow_task_id: Optional[int] = None,\n        workflow_task_order: Optional[int] = None,\n        task_name: Optional[str] = None,\n    ):\n        super().__init__(*args)\n        self.workflow_task_id = workflow_task_id\n        self.workflow_task_order = workflow_task_order\n        self.task_name = task_name\n</code></pre>"},{"location":"reference/fractal_server/app/runner/filenames/","title":"filenames","text":""},{"location":"reference/fractal_server/app/runner/set_start_and_last_task_index/","title":"set_start_and_last_task_index","text":""},{"location":"reference/fractal_server/app/runner/set_start_and_last_task_index/#fractal_server.app.runner.set_start_and_last_task_index.set_start_and_last_task_index","title":"<code>set_start_and_last_task_index(num_tasks, first_task_index=None, last_task_index=None)</code>","text":"<p>Handle <code>first_task_index</code> and <code>last_task_index</code>, by setting defaults and validating values.</p> num_tasks <p>Total number of tasks in a workflow task list</p> <p>first_task_index:     Positional index of the first task to execute last_task_index:     Positional index of the last task to execute</p> Source code in <code>fractal_server/app/runner/set_start_and_last_task_index.py</code> <pre><code>def set_start_and_last_task_index(\n    num_tasks: int,\n    first_task_index: Optional[int] = None,\n    last_task_index: Optional[int] = None,\n) -&gt; tuple[int, int]:\n    \"\"\"\n    Handle `first_task_index` and `last_task_index`, by setting defaults and\n    validating values.\n\n    num_tasks:\n        Total number of tasks in a workflow task list\n    first_task_index:\n        Positional index of the first task to execute\n    last_task_index:\n        Positional index of the last task to execute\n    \"\"\"\n    # Set default values\n    if first_task_index is None:\n        first_task_index = 0\n    if last_task_index is None:\n        last_task_index = num_tasks - 1\n\n    # Perform checks\n    if first_task_index &lt; 0:\n        raise ValueError(f\"{first_task_index=} cannot be negative\")\n    if last_task_index &lt; 0:\n        raise ValueError(f\"{last_task_index=} cannot be negative\")\n    if last_task_index &gt; num_tasks - 1:\n        raise ValueError(\n            f\"{last_task_index=} cannot be larger than {(num_tasks-1)=}\"\n        )\n    if first_task_index &gt; last_task_index:\n        raise ValueError(\n            f\"{first_task_index=} cannot be larger than {last_task_index=}\"\n        )\n    return (first_task_index, last_task_index)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/task_files/","title":"task_files","text":""},{"location":"reference/fractal_server/app/runner/task_files/#fractal_server.app.runner.task_files.TaskFiles","title":"<code>TaskFiles</code>","text":"<p>Group all file paths pertaining to a task</p> <p>Attributes:</p> Name Type Description <code>workflow_dir</code> <code>Path</code> <p>Server-owned directory to store all task-execution-related relevant files (inputs, outputs, errors, and all meta files related to the job execution). Note: users cannot write directly to this folder.</p> <code>workflow_dir_user</code> <code>Path</code> <p>User-side directory with the same scope as <code>workflow_dir</code>, and where a user can write.</p> <code>task_order</code> <code>Optional[int]</code> <p>Positional order of the task within a workflow.</p> <code>component</code> <code>Optional[str]</code> <p>Specific component to run the task for (relevant for tasks that will be executed in parallel over many components).</p> <code>file_prefix</code> <code>str</code> <p>Prefix for all task-related files.</p> <code>args</code> <code>Path</code> <p>Path for input json file.</p> <code>metadiff</code> <code>Path</code> <p>Path for output json file with metadata update.</p> <code>out</code> <code>Path</code> <p>Path for task-execution stdout.</p> <code>err</code> <code>Path</code> <p>Path for task-execution stderr.</p> Source code in <code>fractal_server/app/runner/task_files.py</code> <pre><code>class TaskFiles:\n    \"\"\"\n    Group all file paths pertaining to a task\n\n    Attributes:\n        workflow_dir:\n            Server-owned directory to store all task-execution-related relevant\n            files (inputs, outputs, errors, and all meta files related to the\n            job execution). Note: users cannot write directly to this folder.\n        workflow_dir_user:\n            User-side directory with the same scope as `workflow_dir`, and\n            where a user can write.\n        task_order:\n            Positional order of the task within a workflow.\n        component:\n            Specific component to run the task for (relevant for tasks that\n            will be executed in parallel over many components).\n        file_prefix:\n            Prefix for all task-related files.\n        args:\n            Path for input json file.\n        metadiff:\n            Path for output json file with metadata update.\n        out:\n            Path for task-execution stdout.\n        err:\n            Path for task-execution stderr.\n    \"\"\"\n\n    workflow_dir: Path\n    workflow_dir_user: Path\n    task_order: Optional[int] = None\n    component: Optional[str] = None  # FIXME: this is actually for V1 only\n\n    file_prefix: str\n    args: Path\n    out: Path\n    err: Path\n    log: Path\n    metadiff: Path\n\n    def __init__(\n        self,\n        workflow_dir: Path,\n        workflow_dir_user: Path,\n        task_order: Optional[int] = None,\n        component: Optional[str] = None,\n    ):\n        self.workflow_dir = workflow_dir\n        self.workflow_dir_user = workflow_dir_user\n        self.task_order = task_order\n        self.component = component\n\n        if self.component is not None:\n            component_safe = sanitize_component(str(self.component))\n            component_safe = f\"_par_{component_safe}\"\n        else:\n            component_safe = \"\"\n\n        if self.task_order is not None:\n            order = str(self.task_order)\n        else:\n            order = \"task\"\n        self.file_prefix = f\"{order}{component_safe}\"\n        self.args = self.workflow_dir_user / f\"{self.file_prefix}.args.json\"\n        self.out = self.workflow_dir_user / f\"{self.file_prefix}.out\"\n        self.err = self.workflow_dir_user / f\"{self.file_prefix}.err\"\n        self.log = self.workflow_dir_user / f\"{self.file_prefix}.log\"\n        self.metadiff = (\n            self.workflow_dir_user / f\"{self.file_prefix}.metadiff.json\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/runner/task_files/#fractal_server.app.runner.task_files.get_task_file_paths","title":"<code>get_task_file_paths(workflow_dir, workflow_dir_user, task_order=None, component=None)</code>","text":"<p>Return the corrisponding TaskFiles object</p> <p>This function is mainly used as a cache to avoid instantiating needless objects.</p> Source code in <code>fractal_server/app/runner/task_files.py</code> <pre><code>def get_task_file_paths(\n    workflow_dir: Path,\n    workflow_dir_user: Path,\n    task_order: Optional[int] = None,\n    component: Optional[str] = None,\n) -&gt; TaskFiles:\n    \"\"\"\n    Return the corrisponding TaskFiles object\n\n    This function is mainly used as a cache to avoid instantiating needless\n    objects.\n    \"\"\"\n    return TaskFiles(\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n        task_order=task_order,\n        component=component,\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/runner/task_files/#fractal_server.app.runner.task_files.sanitize_component","title":"<code>sanitize_component(value)</code>","text":"<p>Remove {\" \", \"/\", \".\"} form a string, e.g. going from 'plate.zarr/B/03/0' to 'plate_zarr_B_03_0'.</p> Source code in <code>fractal_server/app/runner/task_files.py</code> <pre><code>def sanitize_component(value: str) -&gt; str:\n    \"\"\"\n    Remove {\" \", \"/\", \".\"} form a string, e.g. going from\n    'plate.zarr/B/03/0' to 'plate_zarr_B_03_0'.\n    \"\"\"\n    return value.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\".\", \"_\")\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/","title":"executors","text":""},{"location":"reference/fractal_server/app/runner/executors/slurm/","title":"slurm","text":""},{"location":"reference/fractal_server/app/runner/executors/slurm/_batching/","title":"_batching","text":"<p>Submodule to determine the number of total/parallel tasks per SLURM job.</p>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_batching/#fractal_server.app.runner.executors.slurm._batching._estimate_parallel_tasks_per_job","title":"<code>_estimate_parallel_tasks_per_job(*, cpus_per_task, mem_per_task, max_cpus_per_job, max_mem_per_job)</code>","text":"<p>Compute how many parallel tasks can fit in a given SLURM job</p> <p>Note: If more resources than available are requested, return 1. This assumes that further checks will be performed on the output of the current function, as is the case in the <code>heuristics</code> function below.</p> <p>Parameters:</p> Name Type Description Default <code>cpus_per_task</code> <code>int</code> <p>Number of CPUs needed for one task.</p> required <code>mem_per_task</code> <code>int</code> <p>Memory (in MB) needed for one task.</p> required <code>max_cpus_per_job</code> <code>int</code> <p>Maximum number of CPUs available for one job.</p> required <code>max_mem_per_job</code> <code>int</code> <p>Maximum memory (in MB) available for one job.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of parallel tasks per job</p> Source code in <code>fractal_server/app/runner/executors/slurm/_batching.py</code> <pre><code>def _estimate_parallel_tasks_per_job(\n    *,\n    cpus_per_task: int,\n    mem_per_task: int,\n    max_cpus_per_job: int,\n    max_mem_per_job: int,\n) -&gt; int:\n    \"\"\"\n    Compute how many parallel tasks can fit in a given SLURM job\n\n\n    Note: If more resources than available are requested, return 1. This\n    assumes that further checks will be performed on the output of the current\n    function, as is the case in the `heuristics` function below.\n\n    Arguments:\n        cpus_per_task: Number of CPUs needed for one task.\n        mem_per_task: Memory (in MB) needed for one task.\n        max_cpus_per_job: Maximum number of CPUs available for one job.\n        max_mem_per_job: Maximum memory (in MB) available for one job.\n\n    Returns:\n        Number of parallel tasks per job\n    \"\"\"\n    if cpus_per_task &gt; max_cpus_per_job or mem_per_task &gt; max_mem_per_job:\n        return 1\n    val_based_on_cpus = max_cpus_per_job // cpus_per_task\n    val_based_on_mem = max_mem_per_job // mem_per_task\n    return min(val_based_on_cpus, val_based_on_mem)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_batching/#fractal_server.app.runner.executors.slurm._batching.heuristics","title":"<code>heuristics(*, tot_tasks, tasks_per_job=None, parallel_tasks_per_job=None, cpus_per_task, mem_per_task, target_cpus_per_job, max_cpus_per_job, target_mem_per_job, max_mem_per_job, target_num_jobs, max_num_jobs)</code>","text":"<p>Heuristically determine parameters for multi-task batching</p> <p>\"In-job queues\" refer to the case where <code>parallel_tasks_per_job&lt;tasks_per_job</code>, that is, where not all tasks of a given SLURM job will be executed at the same time.</p> <p>This function goes through the following branches:</p> <ol> <li>Validate/fix parameters, if they are provided as input.</li> <li>Heuristically determine parameters based on the per-task resource    requirements and on the target amount of per-job resources, without    resorting to in-job queues.</li> <li>Heuristically determine parameters based on the per-task resource    requirements and on the maximum amount of per-job resources, without    resorting to in-job queues.</li> <li>Heuristically determine parameters (based on the per-task resource    requirements and on the maximum amount of per-job resources) and then    introduce in-job queues to satisfy the hard constraint on the maximum    number of jobs.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tot_tasks</code> <code>int</code> <p>Total number of elements to be processed (e.g. number of images in a OME-NGFF array).</p> required <code>tasks_per_job</code> <code>Optional[int]</code> <p>If <code>tasks_per_job</code> and <code>parallel_tasks_per_job</code> are not <code>None</code>, validate/edit this choice.</p> <code>None</code> <code>parallel_tasks_per_job</code> <code>Optional[int]</code> <p>If <code>tasks_per_job</code> and <code>parallel_tasks_per_job</code> are not <code>None</code>, validate/edit this choice.</p> <code>None</code> <code>cpus_per_task</code> <code>int</code> <p>Number of CPUs needed for each parallel task.</p> required <code>mem_per_task</code> <code>int</code> <p>Memory (in MB) needed for each parallel task.</p> required <code>target_cpus_per_job</code> <code>int</code> <p>Optimal number of CPUs for each SLURM job.</p> required <code>max_cpus_per_job</code> <code>int</code> <p>Maximum number of CPUs for each SLURM job.</p> required <code>target_mem_per_job</code> <code>int</code> <p>Optimal amount of memory (in MB) for each SLURM job.</p> required <code>max_mem_per_job</code> <code>int</code> <p>Maximum amount of memory (in MB) for each SLURM job.</p> required <code>target_num_jobs</code> <code>int</code> <p>Optimal total number of SLURM jobs for a given WorkflowTask.</p> required <code>max_num_jobs</code> <code>int</code> <p>Maximum total number of SLURM jobs for a given WorkflowTask.</p> required <p>Return:     Valid values of <code>tasks_per_job</code> and <code>parallel_tasks_per_job</code>.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_batching.py</code> <pre><code>def heuristics(\n    *,\n    # Number of parallel components (always known)\n    tot_tasks: int,\n    # Optional WorkflowTask attributes:\n    tasks_per_job: Optional[int] = None,\n    parallel_tasks_per_job: Optional[int] = None,\n    # Task requirements (multiple possible sources):\n    cpus_per_task: int,\n    mem_per_task: int,\n    # Fractal configuration variables (soft/hard limits):\n    target_cpus_per_job: int,\n    max_cpus_per_job: int,\n    target_mem_per_job: int,  # in MB\n    max_mem_per_job: int,  # in MB\n    target_num_jobs: int,\n    max_num_jobs: int,\n) -&gt; tuple[int, int]:\n    \"\"\"\n    Heuristically determine parameters for multi-task batching\n\n    \"In-job queues\" refer to the case where\n    `parallel_tasks_per_job&lt;tasks_per_job`, that is, where not all\n    tasks of a given SLURM job will be executed at the same time.\n\n    This function goes through the following branches:\n\n    1. Validate/fix parameters, if they are provided as input.\n    2. Heuristically determine parameters based on the per-task resource\n       requirements and on the target amount of per-job resources, without\n       resorting to in-job queues.\n    3. Heuristically determine parameters based on the per-task resource\n       requirements and on the maximum amount of per-job resources, without\n       resorting to in-job queues.\n    4. Heuristically determine parameters (based on the per-task resource\n       requirements and on the maximum amount of per-job resources) and then\n       introduce in-job queues to satisfy the hard constraint on the maximum\n       number of jobs.\n\n    Arguments:\n        tot_tasks:\n            Total number of elements to be processed (e.g. number of images in\n            a OME-NGFF array).\n        tasks_per_job:\n            If `tasks_per_job` and `parallel_tasks_per_job` are not\n            `None`, validate/edit this choice.\n        parallel_tasks_per_job:\n            If `tasks_per_job` and `parallel_tasks_per_job` are not\n            `None`, validate/edit this choice.\n        cpus_per_task:\n            Number of CPUs needed for each parallel task.\n        mem_per_task:\n            Memory (in MB) needed for each parallel task.\n        target_cpus_per_job:\n            Optimal number of CPUs for each SLURM job.\n        max_cpus_per_job:\n            Maximum number of CPUs for each SLURM job.\n        target_mem_per_job:\n            Optimal amount of memory (in MB) for each SLURM job.\n        max_mem_per_job:\n            Maximum amount of memory (in MB) for each SLURM job.\n        target_num_jobs:\n            Optimal total number of SLURM jobs for a given WorkflowTask.\n        max_num_jobs:\n            Maximum total number of SLURM jobs for a given WorkflowTask.\n    Return:\n        Valid values of `tasks_per_job` and `parallel_tasks_per_job`.\n    \"\"\"\n    # Preliminary checks\n    if bool(tasks_per_job) != bool(parallel_tasks_per_job):\n        msg = (\n            \"tasks_per_job and parallel_tasks_per_job must \"\n            \"be both set or both unset\"\n        )\n        logger.error(msg)\n        raise SlurmHeuristicsError(msg)\n    if cpus_per_task &gt; max_cpus_per_job:\n        msg = (\n            f\"[heuristics] Requested {cpus_per_task=} \"\n            f\"but {max_cpus_per_job=}.\"\n        )\n        logger.error(msg)\n        raise SlurmHeuristicsError(msg)\n    if mem_per_task &gt; max_mem_per_job:\n        msg = (\n            f\"[heuristics] Requested {mem_per_task=} \"\n            f\"but {max_mem_per_job=}.\"\n        )\n        logger.error(msg)\n        raise SlurmHeuristicsError(msg)\n\n    # Branch 1: validate/update given parameters\n    if tasks_per_job and parallel_tasks_per_job:\n        # Reduce parallel_tasks_per_job if it exceeds tasks_per_job\n        if parallel_tasks_per_job &gt; tasks_per_job:\n            logger.warning(\n                \"[heuristics] Set parallel_tasks_per_job=\"\n                f\"tasks_per_job={tasks_per_job}\"\n            )\n            parallel_tasks_per_job = tasks_per_job\n\n        # Check requested cpus_per_job\n        cpus_per_job = parallel_tasks_per_job * cpus_per_task\n        if cpus_per_job &gt; target_cpus_per_job:\n            logger.warning(\n                f\"[heuristics] Requested {cpus_per_job=} \"\n                f\"but {target_cpus_per_job=}.\"\n            )\n        if cpus_per_job &gt; max_cpus_per_job:\n            msg = (\n                f\"[heuristics] Requested {cpus_per_job=} \"\n                f\"but {max_cpus_per_job=}.\"\n            )\n            logger.error(msg)\n            raise SlurmHeuristicsError(msg)\n\n        # Check requested mem_per_job\n        mem_per_job = parallel_tasks_per_job * mem_per_task\n        if mem_per_job &gt; target_mem_per_job:\n            logger.warning(\n                f\"[heuristics] Requested {mem_per_job=} \"\n                f\"but {target_mem_per_job=}.\"\n            )\n        if mem_per_job &gt; max_mem_per_job:\n            msg = (\n                f\"[heuristics] Requested {mem_per_job=} \"\n                f\"but {max_mem_per_job=}.\"\n            )\n            logger.error(msg)\n            raise SlurmHeuristicsError(msg)\n\n        # Check number of jobs\n        num_jobs = math.ceil(tot_tasks / tasks_per_job)\n        if num_jobs &gt; target_num_jobs:\n            logger.debug(\n                f\"[heuristics] Requested {num_jobs=} \"\n                f\"but {target_num_jobs=}.\"\n            )\n        if num_jobs &gt; max_num_jobs:\n            msg = f\"[heuristics] Requested {num_jobs=} but {max_num_jobs=}.\"\n            logger.error(msg)\n            raise SlurmHeuristicsError(msg)\n        logger.debug(\"[heuristics] Return from branch 1\")\n        return (tasks_per_job, parallel_tasks_per_job)\n\n    # 2: Target-resources-based heuristics, without in-job queues\n    parallel_tasks_per_job = _estimate_parallel_tasks_per_job(\n        cpus_per_task=cpus_per_task,\n        mem_per_task=mem_per_task,\n        max_cpus_per_job=target_cpus_per_job,\n        max_mem_per_job=target_mem_per_job,\n    )\n    tasks_per_job = parallel_tasks_per_job\n    num_jobs = math.ceil(tot_tasks / tasks_per_job)\n    if num_jobs &lt;= target_num_jobs:\n        logger.debug(\"[heuristics] Return from branch 2\")\n        return (tasks_per_job, parallel_tasks_per_job)\n\n    # Branch 3: Max-resources-based heuristics, without in-job queues\n    parallel_tasks_per_job = _estimate_parallel_tasks_per_job(\n        cpus_per_task=cpus_per_task,\n        mem_per_task=mem_per_task,\n        max_cpus_per_job=max_cpus_per_job,\n        max_mem_per_job=max_mem_per_job,\n    )\n    tasks_per_job = parallel_tasks_per_job\n    num_jobs = math.ceil(tot_tasks / tasks_per_job)\n    if num_jobs &lt;= max_num_jobs:\n        logger.debug(\"[heuristics] Return from branch 3\")\n        return (tasks_per_job, parallel_tasks_per_job)\n\n    # Branch 4: Max-resources-based heuristics, with in-job queues\n    parallel_tasks_per_job = _estimate_parallel_tasks_per_job(\n        cpus_per_task=cpus_per_task,\n        mem_per_task=mem_per_task,\n        max_cpus_per_job=max_cpus_per_job,\n        max_mem_per_job=max_mem_per_job,\n    )\n    tasks_per_job = math.ceil(tot_tasks / max_num_jobs)\n    logger.debug(\"[heuristics] Return from branch 4\")\n    return (tasks_per_job, parallel_tasks_per_job)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_check_jobs_status/","title":"_check_jobs_status","text":""},{"location":"reference/fractal_server/app/runner/executors/slurm/_check_jobs_status/#fractal_server.app.runner.executors.slurm._check_jobs_status._jobs_finished","title":"<code>_jobs_finished(job_ids)</code>","text":"<p>Check which ones of the given Slurm jobs already finished</p> <p>The function is based on the <code>_jobs_finished</code> function from clusterfutures (version 0.5). Original Copyright: 2022 Adrian Sampson (released under the MIT licence)</p> Source code in <code>fractal_server/app/runner/executors/slurm/_check_jobs_status.py</code> <pre><code>def _jobs_finished(job_ids) -&gt; set[str]:\n    \"\"\"\n    Check which ones of the given Slurm jobs already finished\n\n    The function is based on the `_jobs_finished` function from\n    clusterfutures (version 0.5).\n    Original Copyright: 2022 Adrian Sampson\n    (released under the MIT licence)\n    \"\"\"\n\n    # If there is no Slurm job to check, return right away\n    if not job_ids:\n        return set()\n    id_to_state = dict()\n\n    res = run_squeue(job_ids)\n    if res.returncode == 0:\n        id_to_state = {\n            out.split()[0]: out.split()[1] for out in res.stdout.splitlines()\n        }\n    else:\n        id_to_state = dict()\n        for j in job_ids:\n            res = run_squeue([j])\n            if res.returncode != 0:\n                logger.info(f\"Job {j} not found. Marked it as completed\")\n                id_to_state.update({str(j): \"COMPLETED\"})\n            else:\n                id_to_state.update(\n                    {res.stdout.split()[0]: res.stdout.split()[1]}\n                )\n\n    # Finished jobs only stay in squeue for a few mins (configurable). If\n    # a job ID isn't there, we'll assume it's finished.\n    return {\n        j\n        for j in job_ids\n        if id_to_state.get(j, \"COMPLETED\") in STATES_FINISHED\n    }\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_executor_wait_thread/","title":"_executor_wait_thread","text":""},{"location":"reference/fractal_server/app/runner/executors/slurm/_executor_wait_thread/#fractal_server.app.runner.executors.slurm._executor_wait_thread.FractalFileWaitThread","title":"<code>FractalFileWaitThread</code>","text":"<p>             Bases: <code>FileWaitThread</code></p> <p>Overrides the original clusterfutures.FileWaitThread, so that:</p> <ol> <li>Each jobid in the waiting list is associated to a tuple of filenames,    rather than a single one.</li> <li>In the <code>check</code> method, we avoid output-file existence checks (which    would require <code>sudo -u user ls</code> calls), and we rather check for the    existence of the shutdown file. All the logic to check whether a job is    complete is deferred to the <code>cfut.slurm.jobs_finished</code> function.</li> <li>There are additional attributes (<code>slurm_user</code>, <code>shutdown_file</code> and    <code>shutdown_callback</code>).</li> </ol> <p>This class is copied from clusterfutures 0.5. Original Copyright: 2022 Adrian Sampson, released under the MIT licence</p> <p>Note: in principle we could avoid the definition of <code>FractalFileWaitThread</code>, and pack all this code in <code>FractalSlurmWaitThread</code>.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_executor_wait_thread.py</code> <pre><code>class FractalFileWaitThread(FileWaitThread):\n    \"\"\"\n    Overrides the original clusterfutures.FileWaitThread, so that:\n\n    1. Each jobid in the waiting list is associated to a tuple of filenames,\n       rather than a single one.\n    2. In the `check` method, we avoid output-file existence checks (which\n       would require `sudo -u user ls` calls), and we rather check for the\n       existence of the shutdown file. All the logic to check whether a job is\n       complete is deferred to the `cfut.slurm.jobs_finished` function.\n    3. There are additional attributes (`slurm_user`, `shutdown_file` and\n       `shutdown_callback`).\n\n    This class is copied from clusterfutures 0.5. Original Copyright: 2022\n    Adrian Sampson, released under the MIT licence\n\n    Note: in principle we could avoid the definition of\n    `FractalFileWaitThread`, and pack all this code in\n    `FractalSlurmWaitThread`.\n    \"\"\"\n\n    slurm_user: str\n    shutdown_file: Optional[str] = None\n    shutdown_callback: Callable\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def wait(\n        self,\n        *,\n        filenames: tuple[str, ...],\n        jobid: str,\n    ):\n        \"\"\"\n        Add a a new job to the set of jobs being waited for.\n\n        A job consists of a tuple of filenames and a callback value (i.e. a\n        SLURM job ID).\n\n        Note that (with respect to clusterfutures) we replaced `filename` with\n        `filenames`.\n        \"\"\"\n        with self.lock:\n            self.waiting[filenames] = jobid\n\n    def check(self, i):\n        \"\"\"\n        Do one shutdown-file-existence check.\n\n        Note: the `i` parameter allows subclasses like `SlurmWaitThread` to do\n        something on every Nth check.\n\n        Changed from clusterfutures:\n        * Do not check for output-pickle-file existence (we rather rely on\n          `cfut.slurm.jobs_finished`);\n        * Check for the existence of shutdown-file.\n        \"\"\"\n        if self.shutdown_file and os.path.exists(self.shutdown_file):\n            logger.info(\n                f\"Detected executor-shutdown file {str(self.shutdown_file)}\"\n            )\n            self.shutdown = True\n\n    def run(self):\n        \"\"\"\n        Overrides the original clusterfutures.FileWaitThread.run, adding a call\n        to self.shutdown_callback.\n\n        Changed from clusterfutures:\n        * We do not rely on output-file-existence checks to verify whether a\n          job is complete.\n\n        Note that `shutdown_callback` only takes care of cleaning up the\n        FractalSlurmExecutor variables, and then the `return` here is enough to\n        fully clean up the `FractalFileWaitThread` object.\n        \"\"\"\n        for i in count():\n            if self.shutdown:\n                self.shutdown_callback()\n                return\n            with self.lock:\n                self.check(i)\n            time.sleep(self.interval)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_executor_wait_thread/#fractal_server.app.runner.executors.slurm._executor_wait_thread.FractalFileWaitThread.check","title":"<code>check(i)</code>","text":"<p>Do one shutdown-file-existence check.</p> <p>Note: the <code>i</code> parameter allows subclasses like <code>SlurmWaitThread</code> to do something on every Nth check.</p> <p>Changed from clusterfutures: * Do not check for output-pickle-file existence (we rather rely on   <code>cfut.slurm.jobs_finished</code>); * Check for the existence of shutdown-file.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_executor_wait_thread.py</code> <pre><code>def check(self, i):\n    \"\"\"\n    Do one shutdown-file-existence check.\n\n    Note: the `i` parameter allows subclasses like `SlurmWaitThread` to do\n    something on every Nth check.\n\n    Changed from clusterfutures:\n    * Do not check for output-pickle-file existence (we rather rely on\n      `cfut.slurm.jobs_finished`);\n    * Check for the existence of shutdown-file.\n    \"\"\"\n    if self.shutdown_file and os.path.exists(self.shutdown_file):\n        logger.info(\n            f\"Detected executor-shutdown file {str(self.shutdown_file)}\"\n        )\n        self.shutdown = True\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_executor_wait_thread/#fractal_server.app.runner.executors.slurm._executor_wait_thread.FractalFileWaitThread.run","title":"<code>run()</code>","text":"<p>Overrides the original clusterfutures.FileWaitThread.run, adding a call to self.shutdown_callback.</p> <p>Changed from clusterfutures: * We do not rely on output-file-existence checks to verify whether a   job is complete.</p> <p>Note that <code>shutdown_callback</code> only takes care of cleaning up the FractalSlurmExecutor variables, and then the <code>return</code> here is enough to fully clean up the <code>FractalFileWaitThread</code> object.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_executor_wait_thread.py</code> <pre><code>def run(self):\n    \"\"\"\n    Overrides the original clusterfutures.FileWaitThread.run, adding a call\n    to self.shutdown_callback.\n\n    Changed from clusterfutures:\n    * We do not rely on output-file-existence checks to verify whether a\n      job is complete.\n\n    Note that `shutdown_callback` only takes care of cleaning up the\n    FractalSlurmExecutor variables, and then the `return` here is enough to\n    fully clean up the `FractalFileWaitThread` object.\n    \"\"\"\n    for i in count():\n        if self.shutdown:\n            self.shutdown_callback()\n            return\n        with self.lock:\n            self.check(i)\n        time.sleep(self.interval)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_executor_wait_thread/#fractal_server.app.runner.executors.slurm._executor_wait_thread.FractalFileWaitThread.wait","title":"<code>wait(*, filenames, jobid)</code>","text":"<p>Add a a new job to the set of jobs being waited for.</p> <p>A job consists of a tuple of filenames and a callback value (i.e. a SLURM job ID).</p> <p>Note that (with respect to clusterfutures) we replaced <code>filename</code> with <code>filenames</code>.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_executor_wait_thread.py</code> <pre><code>def wait(\n    self,\n    *,\n    filenames: tuple[str, ...],\n    jobid: str,\n):\n    \"\"\"\n    Add a a new job to the set of jobs being waited for.\n\n    A job consists of a tuple of filenames and a callback value (i.e. a\n    SLURM job ID).\n\n    Note that (with respect to clusterfutures) we replaced `filename` with\n    `filenames`.\n    \"\"\"\n    with self.lock:\n        self.waiting[filenames] = jobid\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_executor_wait_thread/#fractal_server.app.runner.executors.slurm._executor_wait_thread.FractalSlurmWaitThread","title":"<code>FractalSlurmWaitThread</code>","text":"<p>             Bases: <code>FractalFileWaitThread</code></p> <p>Replaces the original clusterfutures.SlurmWaitThread, to inherit from FractalFileWaitThread instead of FileWaitThread.</p> <p>The function is copied from clusterfutures 0.5. Original Copyright: 2022 Adrian Sampson, released under the MIT licence</p> <p>Note: if <code>self.interval != 1</code> then this should be modified, but for <code>clusterfutures</code> v0.5 <code>self.interval</code> is indeed equal to <code>1</code>.</p> <p>Changed from clusterfutures: * Rename <code>id_to_filename</code> to <code>id_to_filenames</code></p> Source code in <code>fractal_server/app/runner/executors/slurm/_executor_wait_thread.py</code> <pre><code>class FractalSlurmWaitThread(FractalFileWaitThread):\n    \"\"\"\n    Replaces the original clusterfutures.SlurmWaitThread, to inherit from\n    FractalFileWaitThread instead of FileWaitThread.\n\n    The function is copied from clusterfutures 0.5. Original Copyright: 2022\n    Adrian Sampson, released under the MIT licence\n\n    **Note**: if `self.interval != 1` then this should be modified, but for\n    `clusterfutures` v0.5 `self.interval` is indeed equal to `1`.\n\n    Changed from clusterfutures:\n    * Rename `id_to_filename` to `id_to_filenames`\n    \"\"\"\n\n    slurm_poll_interval = 30\n\n    def check(self, i):\n        super().check(i)\n        if i % (self.slurm_poll_interval // self.interval) == 0:\n            try:\n                finished_jobs = _jobs_finished(self.waiting.values())\n            except Exception:\n                # Don't abandon completion checking if jobs_finished errors\n                traceback.print_exc()\n                return\n\n            if not finished_jobs:\n                return\n\n            id_to_filenames = {v: k for (k, v) in self.waiting.items()}\n            for finished_id in finished_jobs:\n                self.callback(finished_id)\n                self.waiting.pop(id_to_filenames[finished_id])\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/","title":"_slurm_config","text":"<p>Submodule to handle the SLURM configuration for a WorkflowTask</p>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config.SlurmConfig","title":"<code>SlurmConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Abstraction for SLURM parameters</p> <p>NOTE: <code>SlurmConfig</code> objects are created internally in <code>fractal-server</code>, and they are not meant to be initialized by the user; the same holds for <code>SlurmConfig</code> attributes (e.g. <code>mem_per_task_MB</code>), which are not meant to be part of the <code>FRACTAL_SLURM_CONFIG_FILE</code> JSON file (details on the expected file content are defined in <code>SlurmConfigFile</code>).</p> <p>Part of the attributes map directly to some of the SLURM attribues (see https://slurm.schedmd.com/sbatch.html), e.g. <code>partition</code>. Other attributes are metaparameters which are needed in fractal-server to combine multiple tasks in the same SLURM job (e.g. <code>parallel_tasks_per_job</code> or <code>max_num_jobs</code>).</p> <p>Attributes:</p> Name Type Description <code>partition</code> <code>str</code> <p>Corresponds to SLURM option.</p> <code>cpus_per_task</code> <code>int</code> <p>Corresponds to SLURM option.</p> <code>mem_per_task_MB</code> <code>int</code> <p>Corresponds to <code>mem</code> SLURM option.</p> <code>job_name</code> <code>Optional[str]</code> <p>Corresponds to <code>name</code> SLURM option.</p> <code>constraint</code> <code>Optional[str]</code> <p>Corresponds to SLURM option.</p> <code>gres</code> <code>Optional[str]</code> <p>Corresponds to SLURM option.</p> <code>account</code> <code>Optional[str]</code> <p>Corresponds to SLURM option.</p> <code>time</code> <code>Optional[str]</code> <p>Corresponds to SLURM option (WARNING: not fully supported).</p> <code>prefix</code> <code>str</code> <p>Prefix of configuration lines in SLURM submission scripts.</p> <code>shebang_line</code> <code>str</code> <p>Shebang line for SLURM submission scripts.</p> <code>extra_lines</code> <code>Optional[list[str]]</code> <p>Additional lines to include in SLURM submission scripts.</p> <code>tasks_per_job</code> <code>Optional[int]</code> <p>Number of tasks for each SLURM job.</p> <code>parallel_tasks_per_job</code> <code>Optional[int]</code> <p>Number of tasks to run in parallel for                     each SLURM job.</p> <code>target_cpus_per_job</code> <code>int</code> <p>Optimal number of CPUs to be requested in each                  SLURM job.</p> <code>max_cpus_per_job</code> <code>int</code> <p>Maximum number of CPUs that can be requested in each               SLURM job.</p> <code>target_mem_per_job</code> <code>int</code> <p>Optimal amount of memory (in MB) to be requested in                 each SLURM job.</p> <code>max_mem_per_job</code> <code>int</code> <p>Maximum amount of memory (in MB) that can be requested              in each SLURM job.</p> <code>target_num_jobs</code> <code>int</code> <p>Optimal number of SLURM jobs for a given WorkflowTask.</p> <code>max_num_jobs</code> <code>int</code> <p>Maximum number of SLURM jobs for a given WorkflowTask.</p> <code>user_local_exports</code> <code>Optional[dict[str, str]]</code> <p>Key-value pairs to be included as <code>export</code>-ed variables in SLURM submission script, after prepending values with the user's cache directory.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>class SlurmConfig(BaseModel, extra=Extra.forbid):\n    \"\"\"\n    Abstraction for SLURM parameters\n\n    **NOTE**: `SlurmConfig` objects are created internally in `fractal-server`,\n    and they are not meant to be initialized by the user; the same holds for\n    `SlurmConfig` attributes (e.g. `mem_per_task_MB`), which are not meant to\n    be part of the `FRACTAL_SLURM_CONFIG_FILE` JSON file (details on the\n    expected file content are defined in\n    [`SlurmConfigFile`](./#fractal_server.app.runner._slurm._slurm_config.SlurmConfigFile)).\n\n    Part of the attributes map directly to some of the SLURM attribues (see\n    https://slurm.schedmd.com/sbatch.html), e.g. `partition`. Other attributes\n    are metaparameters which are needed in fractal-server to combine multiple\n    tasks in the same SLURM job (e.g. `parallel_tasks_per_job` or\n    `max_num_jobs`).\n\n    Attributes:\n        partition: Corresponds to SLURM option.\n        cpus_per_task: Corresponds to SLURM option.\n        mem_per_task_MB: Corresponds to `mem` SLURM option.\n        job_name: Corresponds to `name` SLURM option.\n        constraint: Corresponds to SLURM option.\n        gres: Corresponds to SLURM option.\n        account: Corresponds to SLURM option.\n        time: Corresponds to SLURM option (WARNING: not fully supported).\n        prefix: Prefix of configuration lines in SLURM submission scripts.\n        shebang_line: Shebang line for SLURM submission scripts.\n        extra_lines: Additional lines to include in SLURM submission scripts.\n        tasks_per_job: Number of tasks for each SLURM job.\n        parallel_tasks_per_job: Number of tasks to run in parallel for\n                                each SLURM job.\n        target_cpus_per_job: Optimal number of CPUs to be requested in each\n                             SLURM job.\n        max_cpus_per_job: Maximum number of CPUs that can be requested in each\n                          SLURM job.\n        target_mem_per_job: Optimal amount of memory (in MB) to be requested in\n                            each SLURM job.\n        max_mem_per_job: Maximum amount of memory (in MB) that can be requested\n                         in each SLURM job.\n        target_num_jobs: Optimal number of SLURM jobs for a given WorkflowTask.\n        max_num_jobs: Maximum number of SLURM jobs for a given WorkflowTask.\n        user_local_exports:\n            Key-value pairs to be included as `export`-ed variables in SLURM\n            submission script, after prepending values with the user's cache\n            directory.\n    \"\"\"\n\n    # Required SLURM parameters (note that the integer attributes are those\n    # that will need to scale up with the number of parallel tasks per job)\n    partition: str\n    cpus_per_task: int\n    mem_per_task_MB: int\n    prefix: str = \"#SBATCH\"\n    shebang_line: str = \"#!/bin/sh\"\n\n    # Optional SLURM parameters\n    job_name: Optional[str] = None\n    constraint: Optional[str] = None\n    gres: Optional[str] = None\n    time: Optional[str] = None\n    account: Optional[str] = None\n\n    # Free-field attribute for extra lines to be added to the SLURM job\n    # preamble\n    extra_lines: Optional[list[str]] = Field(default_factory=list)\n\n    # Variables that will be `export`ed in the SLURM submission script\n    user_local_exports: Optional[dict[str, str]] = None\n\n    # Metaparameters needed to combine multiple tasks in each SLURM job\n    tasks_per_job: Optional[int] = None\n    parallel_tasks_per_job: Optional[int] = None\n    target_cpus_per_job: int\n    max_cpus_per_job: int\n    target_mem_per_job: int\n    max_mem_per_job: int\n    target_num_jobs: int\n    max_num_jobs: int\n\n    def _sorted_extra_lines(self) -&gt; list[str]:\n        \"\"\"\n        Return a copy of `self.extra_lines`, where lines starting with\n        `self.prefix` are listed first.\n        \"\"\"\n\n        def _no_prefix(_line):\n            if _line.startswith(self.prefix):\n                return 0\n            else:\n                return 1\n\n        return sorted(self.extra_lines, key=_no_prefix)\n\n    def sort_script_lines(self, script_lines: list[str]) -&gt; list[str]:\n        \"\"\"\n        Return a copy of `script_lines`, where lines are sorted as in:\n\n        1. `self.shebang_line` (if present);\n        2. Lines starting with `self.prefix`;\n        3. Other lines.\n\n        Arguments:\n            script_lines:\n        \"\"\"\n\n        def _sorting_function(_line):\n            if _line == self.shebang_line:\n                return 0\n            elif _line.startswith(self.prefix):\n                return 1\n            else:\n                return 2\n\n        return sorted(script_lines, key=_sorting_function)\n\n    def to_sbatch_preamble(\n        self,\n        user_cache_dir: Optional[str] = None,\n    ) -&gt; list[str]:\n        \"\"\"\n        Compile `SlurmConfig` object into the preamble of a SLURM submission\n        script.\n\n        Arguments:\n            user_cache_dir:\n                Base directory for exports defined in\n                `self.user_local_exports`.\n        \"\"\"\n        if self.parallel_tasks_per_job is None:\n            raise ValueError(\n                \"SlurmConfig.sbatch_preamble requires that \"\n                f\"{self.parallel_tasks_per_job=} is not None.\"\n            )\n        if self.extra_lines:\n            if len(self.extra_lines) != len(set(self.extra_lines)):\n                raise ValueError(f\"{self.extra_lines=} contains repetitions\")\n\n        mem_per_job_MB = self.parallel_tasks_per_job * self.mem_per_task_MB\n        lines = [\n            self.shebang_line,\n            f\"{self.prefix} --partition={self.partition}\",\n            f\"{self.prefix} --ntasks={self.parallel_tasks_per_job}\",\n            f\"{self.prefix} --cpus-per-task={self.cpus_per_task}\",\n            f\"{self.prefix} --mem={mem_per_job_MB}M\",\n        ]\n        for key in [\"job_name\", \"constraint\", \"gres\", \"time\", \"account\"]:\n            value = getattr(self, key)\n            if value is not None:\n                # Handle the `time` parameter\n                if key == \"time\" and self.parallel_tasks_per_job &gt; 1:\n                    logger.warning(\n                        \"Ignore `#SBATCH --time=...` line (given: \"\n                        f\"{self.time=}) for parallel_tasks_per_job&gt;1\"\n                        f\" (given: {self.parallel_tasks_per_job}), \"\n                        \"since scaling of time with number of tasks is \"\n                        \"not implemented.\"\n                    )\n                    continue\n                option = key.replace(\"_\", \"-\")\n                lines.append(f\"{self.prefix} --{option}={value}\")\n\n        if self.extra_lines:\n            for line in self._sorted_extra_lines():\n                lines.append(line)\n\n        if self.user_local_exports:\n            if user_cache_dir is None:\n                raise ValueError(\n                    f\"user_cache_dir=None but {self.user_local_exports=}\"\n                )\n            for key, value in self.user_local_exports.items():\n                tmp_value = str(Path(user_cache_dir) / value)\n                lines.append(f\"export {key}={tmp_value}\")\n\n        \"\"\"\n        FIXME export SRUN_CPUS_PER_TASK\n        # From https://slurm.schedmd.com/sbatch.html: Beginning with 22.05,\n        # srun will not inherit the --cpus-per-task value requested by salloc\n        # or sbatch.  It must be requested again with the call to srun or set\n        # with the SRUN_CPUS_PER_TASK environment variable if desired for the\n        # task(s).\n        if config.cpus_per_task:\n            #additional_setup_lines.append(\n                f\"export SRUN_CPUS_PER_TASK={config.cpus_per_task}\"\n            )\n        \"\"\"\n\n        return lines\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config.SlurmConfig._sorted_extra_lines","title":"<code>_sorted_extra_lines()</code>","text":"<p>Return a copy of <code>self.extra_lines</code>, where lines starting with <code>self.prefix</code> are listed first.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>def _sorted_extra_lines(self) -&gt; list[str]:\n    \"\"\"\n    Return a copy of `self.extra_lines`, where lines starting with\n    `self.prefix` are listed first.\n    \"\"\"\n\n    def _no_prefix(_line):\n        if _line.startswith(self.prefix):\n            return 0\n        else:\n            return 1\n\n    return sorted(self.extra_lines, key=_no_prefix)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config.SlurmConfig.sort_script_lines","title":"<code>sort_script_lines(script_lines)</code>","text":"<p>Return a copy of <code>script_lines</code>, where lines are sorted as in:</p> <ol> <li><code>self.shebang_line</code> (if present);</li> <li>Lines starting with <code>self.prefix</code>;</li> <li>Other lines.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>script_lines</code> <code>list[str]</code> required Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>def sort_script_lines(self, script_lines: list[str]) -&gt; list[str]:\n    \"\"\"\n    Return a copy of `script_lines`, where lines are sorted as in:\n\n    1. `self.shebang_line` (if present);\n    2. Lines starting with `self.prefix`;\n    3. Other lines.\n\n    Arguments:\n        script_lines:\n    \"\"\"\n\n    def _sorting_function(_line):\n        if _line == self.shebang_line:\n            return 0\n        elif _line.startswith(self.prefix):\n            return 1\n        else:\n            return 2\n\n    return sorted(script_lines, key=_sorting_function)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config.SlurmConfig.to_sbatch_preamble","title":"<code>to_sbatch_preamble(user_cache_dir=None)</code>","text":"<p>Compile <code>SlurmConfig</code> object into the preamble of a SLURM submission script.</p> <p>Parameters:</p> Name Type Description Default <code>user_cache_dir</code> <code>Optional[str]</code> <p>Base directory for exports defined in <code>self.user_local_exports</code>.</p> <code>None</code> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>def to_sbatch_preamble(\n    self,\n    user_cache_dir: Optional[str] = None,\n) -&gt; list[str]:\n    \"\"\"\n    Compile `SlurmConfig` object into the preamble of a SLURM submission\n    script.\n\n    Arguments:\n        user_cache_dir:\n            Base directory for exports defined in\n            `self.user_local_exports`.\n    \"\"\"\n    if self.parallel_tasks_per_job is None:\n        raise ValueError(\n            \"SlurmConfig.sbatch_preamble requires that \"\n            f\"{self.parallel_tasks_per_job=} is not None.\"\n        )\n    if self.extra_lines:\n        if len(self.extra_lines) != len(set(self.extra_lines)):\n            raise ValueError(f\"{self.extra_lines=} contains repetitions\")\n\n    mem_per_job_MB = self.parallel_tasks_per_job * self.mem_per_task_MB\n    lines = [\n        self.shebang_line,\n        f\"{self.prefix} --partition={self.partition}\",\n        f\"{self.prefix} --ntasks={self.parallel_tasks_per_job}\",\n        f\"{self.prefix} --cpus-per-task={self.cpus_per_task}\",\n        f\"{self.prefix} --mem={mem_per_job_MB}M\",\n    ]\n    for key in [\"job_name\", \"constraint\", \"gres\", \"time\", \"account\"]:\n        value = getattr(self, key)\n        if value is not None:\n            # Handle the `time` parameter\n            if key == \"time\" and self.parallel_tasks_per_job &gt; 1:\n                logger.warning(\n                    \"Ignore `#SBATCH --time=...` line (given: \"\n                    f\"{self.time=}) for parallel_tasks_per_job&gt;1\"\n                    f\" (given: {self.parallel_tasks_per_job}), \"\n                    \"since scaling of time with number of tasks is \"\n                    \"not implemented.\"\n                )\n                continue\n            option = key.replace(\"_\", \"-\")\n            lines.append(f\"{self.prefix} --{option}={value}\")\n\n    if self.extra_lines:\n        for line in self._sorted_extra_lines():\n            lines.append(line)\n\n    if self.user_local_exports:\n        if user_cache_dir is None:\n            raise ValueError(\n                f\"user_cache_dir=None but {self.user_local_exports=}\"\n            )\n        for key, value in self.user_local_exports.items():\n            tmp_value = str(Path(user_cache_dir) / value)\n            lines.append(f\"export {key}={tmp_value}\")\n\n    \"\"\"\n    FIXME export SRUN_CPUS_PER_TASK\n    # From https://slurm.schedmd.com/sbatch.html: Beginning with 22.05,\n    # srun will not inherit the --cpus-per-task value requested by salloc\n    # or sbatch.  It must be requested again with the call to srun or set\n    # with the SRUN_CPUS_PER_TASK environment variable if desired for the\n    # task(s).\n    if config.cpus_per_task:\n        #additional_setup_lines.append(\n            f\"export SRUN_CPUS_PER_TASK={config.cpus_per_task}\"\n        )\n    \"\"\"\n\n    return lines\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config.SlurmConfigError","title":"<code>SlurmConfigError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Slurm configuration error</p> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>class SlurmConfigError(ValueError):\n    \"\"\"\n    Slurm configuration error\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config.SlurmConfigFile","title":"<code>SlurmConfigFile</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Specifications for the content of <code>FRACTAL_SLURM_CONFIG_FILE</code></p> <p>This must be a JSON file, and a valid example is <pre><code>{\n  \"default_slurm_config\": {\n      \"partition\": \"main\",\n      \"cpus_per_task\": 1\n  },\n  \"gpu_slurm_config\": {\n      \"partition\": \"gpu\",\n      \"extra_lines\": [\"#SBATCH --gres=gpu:v100:1\"]\n  },\n  \"batching_config\": {\n      \"target_cpus_per_job\": 1,\n      \"max_cpus_per_job\": 1,\n      \"target_mem_per_job\": 200,\n      \"max_mem_per_job\": 500,\n      \"target_num_jobs\": 2,\n      \"max_num_jobs\": 4\n  },\n  \"user_local_exports\": {\n      \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n      \"NAPARI_CONFIG\": \"napari_config.json\"\n  }\n}\n</code></pre></p> <p>See <code>_SlurmConfigSet</code> and <code>_BatchingConfigSet</code> for more details.</p> <p>Attributes:</p> Name Type Description <code>default_slurm_config</code> <code>_SlurmConfigSet</code> <p>Common default options for all tasks.</p> <code>gpu_slurm_config</code> <code>Optional[_SlurmConfigSet]</code> <p>Default configuration for all GPU tasks.</p> <code>batching_config</code> <code>_BatchingConfigSet</code> <p>Configuration of the batching strategy.</p> <code>user_local_exports</code> <code>Optional[dict[str, str]]</code> <p>Key-value pairs to be included as <code>export</code>-ed variables in SLURM submission script, after prepending values with the user's cache directory.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>class SlurmConfigFile(BaseModel, extra=Extra.forbid):\n    \"\"\"\n    Specifications for the content of `FRACTAL_SLURM_CONFIG_FILE`\n\n    This must be a JSON file, and a valid example is\n    ```JSON\n    {\n      \"default_slurm_config\": {\n          \"partition\": \"main\",\n          \"cpus_per_task\": 1\n      },\n      \"gpu_slurm_config\": {\n          \"partition\": \"gpu\",\n          \"extra_lines\": [\"#SBATCH --gres=gpu:v100:1\"]\n      },\n      \"batching_config\": {\n          \"target_cpus_per_job\": 1,\n          \"max_cpus_per_job\": 1,\n          \"target_mem_per_job\": 200,\n          \"max_mem_per_job\": 500,\n          \"target_num_jobs\": 2,\n          \"max_num_jobs\": 4\n      },\n      \"user_local_exports\": {\n          \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n          \"NAPARI_CONFIG\": \"napari_config.json\"\n      }\n    }\n    ```\n\n    See `_SlurmConfigSet` and `_BatchingConfigSet` for more details.\n\n    Attributes:\n        default_slurm_config:\n            Common default options for all tasks.\n        gpu_slurm_config:\n            Default configuration for all GPU tasks.\n        batching_config:\n            Configuration of the batching strategy.\n        user_local_exports:\n            Key-value pairs to be included as `export`-ed variables in SLURM\n            submission script, after prepending values with the user's cache\n            directory.\n    \"\"\"\n\n    default_slurm_config: _SlurmConfigSet\n    gpu_slurm_config: Optional[_SlurmConfigSet]\n    batching_config: _BatchingConfigSet\n    user_local_exports: Optional[dict[str, str]]\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config._BatchingConfigSet","title":"<code>_BatchingConfigSet</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Options that can be set in <code>FRACTAL_SLURM_CONFIG_FILE</code> to configure the batching strategy (that is, how to combine several tasks in a single SLURM job). Only used as part of <code>SlurmConfigFile</code>.</p> <p>Attributes:</p> Name Type Description <code>target_cpus_per_job</code> <code>int</code> <code>max_cpus_per_job</code> <code>int</code> <code>target_mem_per_job</code> <code>Union[int, str]</code> <p>(see <code>_parse_mem_value</code> for details on allowed values)</p> <code>max_mem_per_job</code> <code>Union[int, str]</code> <p>(see <code>_parse_mem_value</code> for details on allowed values)</p> <code>target_num_jobs</code> <code>int</code> <code>max_num_jobs</code> <code>int</code> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>class _BatchingConfigSet(BaseModel, extra=Extra.forbid):\n    \"\"\"\n    Options that can be set in `FRACTAL_SLURM_CONFIG_FILE` to configure the\n    batching strategy (that is, how to combine several tasks in a single SLURM\n    job). Only used as part of `SlurmConfigFile`.\n\n    Attributes:\n        target_cpus_per_job:\n        max_cpus_per_job:\n        target_mem_per_job:\n            (see `_parse_mem_value` for details on allowed values)\n        max_mem_per_job:\n            (see `_parse_mem_value` for details on allowed values)\n        target_num_jobs:\n        max_num_jobs:\n    \"\"\"\n\n    target_cpus_per_job: int\n    max_cpus_per_job: int\n    target_mem_per_job: Union[int, str]\n    max_mem_per_job: Union[int, str]\n    target_num_jobs: int\n    max_num_jobs: int\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config._SlurmConfigSet","title":"<code>_SlurmConfigSet</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Options that can be set in <code>FRACTAL_SLURM_CONFIG_FILE</code> for the default/gpu SLURM config. Only used as part of <code>SlurmConfigFile</code>.</p> <p>Attributes:</p> Name Type Description <code>partition</code> <code>Optional[str]</code> <code>cpus_per_task</code> <code>Optional[int]</code> <code>mem</code> <code>Optional[Union[int, str]]</code> <p>See <code>_parse_mem_value</code> for details on allowed values.</p> <code>constraint</code> <code>Optional[str]</code> <code>gres</code> <code>Optional[str]</code> <code>time</code> <code>Optional[str]</code> <code>account</code> <code>Optional[str]</code> <code>extra_lines</code> <code>Optional[list[str]]</code> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>class _SlurmConfigSet(BaseModel, extra=Extra.forbid):\n    \"\"\"\n    Options that can be set in `FRACTAL_SLURM_CONFIG_FILE` for the default/gpu\n    SLURM config. Only used as part of `SlurmConfigFile`.\n\n    Attributes:\n        partition:\n        cpus_per_task:\n        mem:\n            See `_parse_mem_value` for details on allowed values.\n        constraint:\n        gres:\n        time:\n        account:\n        extra_lines:\n    \"\"\"\n\n    partition: Optional[str]\n    cpus_per_task: Optional[int]\n    mem: Optional[Union[int, str]]\n    constraint: Optional[str]\n    gres: Optional[str]\n    time: Optional[str]\n    account: Optional[str]\n    extra_lines: Optional[list[str]]\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config._parse_mem_value","title":"<code>_parse_mem_value(raw_mem)</code>","text":"<p>Convert a memory-specification string into an integer (in MB units), or simply return the input if it is already an integer.</p> <p>Supported units are <code>\"M\", \"G\", \"T\"</code>, with <code>\"M\"</code> being the default; some parsing examples are: <code>\"10M\" -&gt; 10000</code>, <code>\"3G\" -&gt; 3000000</code>.</p> <p>Parameters:</p> Name Type Description Default <code>raw_mem</code> <code>Union[str, int]</code> <p>A string (e.g. <code>\"100M\"</code>) or an integer (in MB).</p> required <p>Returns:</p> Type Description <code>int</code> <p>Integer value of memory in MB units.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>def _parse_mem_value(raw_mem: Union[str, int]) -&gt; int:\n    \"\"\"\n    Convert a memory-specification string into an integer (in MB units), or\n    simply return the input if it is already an integer.\n\n    Supported units are `\"M\", \"G\", \"T\"`, with `\"M\"` being the default; some\n    parsing examples are: `\"10M\" -&gt; 10000`, `\"3G\" -&gt; 3000000`.\n\n    Arguments:\n        raw_mem:\n            A string (e.g. `\"100M\"`) or an integer (in MB).\n\n    Returns:\n        Integer value of memory in MB units.\n    \"\"\"\n\n    info = f\"[_parse_mem_value] {raw_mem=}\"\n    error_msg = (\n        f\"{info}, invalid specification of memory requirements \"\n        \"(valid examples: 93, 71M, 93G, 71T).\"\n    )\n\n    # Handle integer argument\n    if isinstance(raw_mem, int):\n        return raw_mem\n\n    # Handle string argument\n    if not raw_mem[0].isdigit():  # fail e.g. for raw_mem=\"M100\"\n        logger.error(error_msg)\n        raise SlurmConfigError(error_msg)\n    if raw_mem.isdigit():\n        mem_MB = int(raw_mem)\n    elif raw_mem.endswith(\"M\"):\n        stripped_raw_mem = raw_mem.strip(\"M\")\n        if not stripped_raw_mem.isdigit():\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        mem_MB = int(stripped_raw_mem)\n    elif raw_mem.endswith(\"G\"):\n        stripped_raw_mem = raw_mem.strip(\"G\")\n        if not stripped_raw_mem.isdigit():\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        mem_MB = int(stripped_raw_mem) * 10**3\n    elif raw_mem.endswith(\"T\"):\n        stripped_raw_mem = raw_mem.strip(\"T\")\n        if not stripped_raw_mem.isdigit():\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        mem_MB = int(stripped_raw_mem) * 10**6\n    else:\n        logger.error(error_msg)\n        raise SlurmConfigError(error_msg)\n\n    logger.debug(f\"{info}, return {mem_MB}\")\n    return mem_MB\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config.get_default_slurm_config","title":"<code>get_default_slurm_config()</code>","text":"<p>Return a default <code>SlurmConfig</code> configuration object</p> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>def get_default_slurm_config():\n    \"\"\"\n    Return a default `SlurmConfig` configuration object\n    \"\"\"\n    return SlurmConfig(\n        partition=\"main\",\n        cpus_per_task=1,\n        mem_per_task_MB=100,\n        target_cpus_per_job=1,\n        max_cpus_per_job=2,\n        target_mem_per_job=100,\n        max_mem_per_job=500,\n        target_num_jobs=2,\n        max_num_jobs=4,\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_slurm_config/#fractal_server.app.runner.executors.slurm._slurm_config.load_slurm_config_file","title":"<code>load_slurm_config_file(config_path=None)</code>","text":"<p>Load a SLURM configuration file and validate its content with <code>SlurmConfigFile</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Optional[Path]</code> <code>None</code> Source code in <code>fractal_server/app/runner/executors/slurm/_slurm_config.py</code> <pre><code>def load_slurm_config_file(\n    config_path: Optional[Path] = None,\n) -&gt; SlurmConfigFile:\n    \"\"\"\n    Load a SLURM configuration file and validate its content with\n    `SlurmConfigFile`.\n\n    Arguments:\n        config_path:\n    \"\"\"\n\n    if not config_path:\n        settings = Inject(get_settings)\n        config_path = settings.FRACTAL_SLURM_CONFIG_FILE\n\n    # Load file\n    logger.debug(f\"[get_slurm_config] Now loading {config_path=}\")\n    try:\n        with config_path.open(\"r\") as f:\n            slurm_env = json.load(f)\n    except Exception as e:\n        raise SlurmConfigError(\n            f\"Error while loading {config_path=}. \"\n            f\"Original error:\\n{str(e)}\"\n        )\n\n    # Validate file content\n    logger.debug(f\"[load_slurm_config_file] Now validating {config_path=}\")\n    logger.debug(f\"[load_slurm_config_file] {slurm_env=}\")\n    try:\n        obj = SlurmConfigFile(**slurm_env)\n    except ValidationError as e:\n        raise SlurmConfigError(\n            f\"Error while loading {config_path=}. \"\n            f\"Original error:\\n{str(e)}\"\n        )\n\n    # Convert memory to MB units, in all relevant attributes\n    if obj.default_slurm_config.mem:\n        obj.default_slurm_config.mem = _parse_mem_value(\n            obj.default_slurm_config.mem\n        )\n    if obj.gpu_slurm_config and obj.gpu_slurm_config.mem:\n        obj.gpu_slurm_config.mem = _parse_mem_value(obj.gpu_slurm_config.mem)\n    obj.batching_config.target_mem_per_job = _parse_mem_value(\n        obj.batching_config.target_mem_per_job\n    )\n    obj.batching_config.max_mem_per_job = _parse_mem_value(\n        obj.batching_config.max_mem_per_job\n    )\n\n    return obj\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_subprocess_run_as_user/","title":"_subprocess_run_as_user","text":"<p>Run simple commands as another user</p> <p>This module provides a set of tools similar to <code>subprocess.run</code>, <code>glob.glob</code> or <code>os.path.exists</code>, but extended so that they can be executed on behalf of another user. Note that this requires appropriate sudo permissions.</p>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_subprocess_run_as_user/#fractal_server.app.runner.executors.slurm._subprocess_run_as_user._glob_as_user","title":"<code>_glob_as_user(*, folder, user, startswith=None)</code>","text":"<p>Run <code>ls</code> in a folder (as a user) and filter results</p> <p>Execute <code>ls</code> on a folder (impersonating a user, if <code>user</code> is not <code>None</code>) and select results that start with <code>startswith</code> (if not <code>None</code>).</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Absolute path to the folder</p> required <code>user</code> <code>str</code> <p>If not <code>None</code>, the user to be impersonated via <code>sudo -u</code></p> required <code>startswith</code> <code>Optional[str]</code> <p>If not <code>None</code>, this is used to filter output of <code>ls</code>.</p> <code>None</code> Source code in <code>fractal_server/app/runner/executors/slurm/_subprocess_run_as_user.py</code> <pre><code>def _glob_as_user(\n    *, folder: str, user: str, startswith: Optional[str] = None\n) -&gt; list[str]:\n    \"\"\"\n    Run `ls` in a folder (as a user) and filter results\n\n    Execute `ls` on a folder (impersonating a user, if `user` is not `None`)\n    and select results that start with `startswith` (if not `None`).\n\n    Arguments:\n        folder: Absolute path to the folder\n        user: If not `None`, the user to be impersonated via `sudo -u`\n        startswith: If not `None`, this is used to filter output of `ls`.\n    \"\"\"\n\n    res = _run_command_as_user(cmd=f\"ls {folder}\", user=user, check=True)\n    output = res.stdout.split()\n    if startswith:\n        output = [f for f in output if f.startswith(startswith)]\n    return output\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_subprocess_run_as_user/#fractal_server.app.runner.executors.slurm._subprocess_run_as_user._glob_as_user_strict","title":"<code>_glob_as_user_strict(*, folder, user, startswith)</code>","text":"<p>Run <code>ls</code> in a folder (as a user) and filter results</p> <p>Execute <code>ls</code> on a folder (impersonating a user, if <code>user</code> is not <code>None</code>) and select results that comply with a set of rules. They all start with <code>startswith</code> (if not <code>None</code>), and they match one of the known filename patterns. See details in https://github.com/fractal-analytics-platform/fractal-server/issues/1240</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Absolute path to the folder</p> required <code>user</code> <code>str</code> <p>If not <code>None</code>, the user to be impersonated via <code>sudo -u</code></p> required <code>startswith</code> <code>str</code> <p>If not <code>None</code>, this is used to filter output of <code>ls</code>.</p> required Source code in <code>fractal_server/app/runner/executors/slurm/_subprocess_run_as_user.py</code> <pre><code>def _glob_as_user_strict(\n    *,\n    folder: str,\n    user: str,\n    startswith: str,\n) -&gt; list[str]:\n    \"\"\"\n    Run `ls` in a folder (as a user) and filter results\n\n    Execute `ls` on a folder (impersonating a user, if `user` is not `None`)\n    and select results that comply with a set of rules. They all start with\n    `startswith` (if not `None`), and they match one of the known filename\n    patterns. See details in\n    https://github.com/fractal-analytics-platform/fractal-server/issues/1240\n\n\n    Arguments:\n        folder: Absolute path to the folder\n        user: If not `None`, the user to be impersonated via `sudo -u`\n        startswith: If not `None`, this is used to filter output of `ls`.\n    \"\"\"\n\n    res = _run_command_as_user(cmd=f\"ls {folder}\", user=user, check=True)\n    output = res.stdout.split()\n\n    new_output = []\n    known_filenames = [\n        f\"{startswith}{suffix}\"\n        for suffix in [\".args.json\", \".metadiff.json\", \".err\", \".out\"]\n    ]\n    for filename in output:\n        if filename in known_filenames:\n            new_output.append(filename)\n        elif filename.startswith(f\"{startswith}_out_\") and filename.endswith(\n            \".pickle\"\n        ):\n            new_output.append(filename)\n\n    return new_output\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_subprocess_run_as_user/#fractal_server.app.runner.executors.slurm._subprocess_run_as_user._mkdir_as_user","title":"<code>_mkdir_as_user(*, folder, user)</code>","text":"<p>Create a folder as a different user</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Absolute path to the folder</p> required <code>user</code> <code>str</code> <p>User to be impersonated</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if <code>user</code> is not correctly defined, or if subprocess           returncode is not 0.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_subprocess_run_as_user.py</code> <pre><code>def _mkdir_as_user(*, folder: str, user: str) -&gt; None:\n    \"\"\"\n    Create a folder as a different user\n\n    Arguments:\n        folder: Absolute path to the folder\n        user: User to be impersonated\n\n    Raises:\n        RuntimeError: if `user` is not correctly defined, or if subprocess\n                      returncode is not 0.\n    \"\"\"\n    if not user:\n        raise RuntimeError(f\"{user=} not allowed in _mkdir_as_user\")\n\n    cmd = f\"mkdir -p {folder}\"\n    _run_command_as_user(cmd=cmd, user=user, check=True)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_subprocess_run_as_user/#fractal_server.app.runner.executors.slurm._subprocess_run_as_user._path_exists_as_user","title":"<code>_path_exists_as_user(*, path, user=None)</code>","text":"<p>Impersonate a user and check if <code>path</code> exists via <code>ls</code></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Absolute file/folder path</p> required <code>user</code> <code>Optional[str]</code> <p>If not <code>None</code>, user to be impersonated</p> <code>None</code> Source code in <code>fractal_server/app/runner/executors/slurm/_subprocess_run_as_user.py</code> <pre><code>def _path_exists_as_user(*, path: str, user: Optional[str] = None) -&gt; bool:\n    \"\"\"\n    Impersonate a user and check if `path` exists via `ls`\n\n    Arguments:\n        path: Absolute file/folder path\n        user: If not `None`, user to be impersonated\n    \"\"\"\n    res = _run_command_as_user(cmd=f\"ls {path}\", user=user)\n    if res.returncode == 0:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/_subprocess_run_as_user/#fractal_server.app.runner.executors.slurm._subprocess_run_as_user._run_command_as_user","title":"<code>_run_command_as_user(*, cmd, user=None, encoding='utf-8', check=False)</code>","text":"<p>Use <code>sudo -u</code> to impersonate another user and run a command</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>str</code> <p>Command to be run</p> required <code>user</code> <code>Optional[str]</code> <p>User to be impersonated</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>Argument for <code>subprocess.run</code>. Note that this must be <code>None</code>       to have stdout/stderr as bytes.</p> <code>'utf-8'</code> <code>check</code> <code>bool</code> <p>If <code>True</code>, check that <code>returncode=0</code> and fail otherwise.</p> <code>False</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if <code>check=True</code> and returncode is non-zero.</p> <p>Returns:</p> Name Type Description <code>res</code> <code>CompletedProcess</code> <p>The return value from <code>subprocess.run</code>.</p> Source code in <code>fractal_server/app/runner/executors/slurm/_subprocess_run_as_user.py</code> <pre><code>def _run_command_as_user(\n    *,\n    cmd: str,\n    user: Optional[str] = None,\n    encoding: Optional[str] = \"utf-8\",\n    check: bool = False,\n) -&gt; subprocess.CompletedProcess:\n    \"\"\"\n    Use `sudo -u` to impersonate another user and run a command\n\n    Arguments:\n        cmd: Command to be run\n        user: User to be impersonated\n        encoding: Argument for `subprocess.run`. Note that this must be `None`\n                  to have stdout/stderr as bytes.\n        check: If `True`, check that `returncode=0` and fail otherwise.\n\n    Raises:\n        RuntimeError: if `check=True` and returncode is non-zero.\n\n    Returns:\n        res: The return value from `subprocess.run`.\n    \"\"\"\n    logger.debug(f'[_run_command_as_user] {user=}, cmd=\"{cmd}\"')\n    if user:\n        new_cmd = f\"sudo --non-interactive -u {user} {cmd}\"\n    else:\n        new_cmd = cmd\n    res = subprocess.run(  # nosec\n        shlex.split(new_cmd),\n        capture_output=True,\n        encoding=encoding,\n    )\n    logger.debug(f\"[_run_command_as_user] {res.returncode=}\")\n    logger.debug(f\"[_run_command_as_user] {res.stdout=}\")\n    logger.debug(f\"[_run_command_as_user] {res.stderr=}\")\n\n    if check and not res.returncode == 0:\n        raise RuntimeError(\n            f\"{cmd=}\\n\\n\"\n            f\"{res.returncode=}\\n\\n\"\n            f\"{res.stdout=}\\n\\n\"\n            f\"{res.stderr=}\\n\"\n        )\n\n    return res\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/","title":"executor","text":""},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor","title":"<code>FractalSlurmExecutor</code>","text":"<p>             Bases: <code>SlurmExecutor</code></p> <p>FractalSlurmExecutor (inherits from cfut.SlurmExecutor)</p> <p>Attributes:</p> Name Type Description <code>slurm_user</code> <code>str</code> <p>Shell username that runs the <code>sbatch</code> command.</p> <code>common_script_lines</code> <code>list[str]</code> <p>Arbitrary script lines that will always be included in the sbatch script</p> <code>working_dir</code> <code>Path</code> <p>Directory for both the cfut/SLURM and fractal-server files and logs</p> <code>working_dir_user</code> <code>Path</code> <p>Directory for both the cfut/SLURM and fractal-server files and logs</p> <code>map_jobid_to_slurm_files</code> <code>dict[str, tuple[str, str, str]]</code> <p>Dictionary with paths of slurm-related files for active jobs</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>class FractalSlurmExecutor(SlurmExecutor):\n    \"\"\"\n    FractalSlurmExecutor (inherits from cfut.SlurmExecutor)\n\n    Attributes:\n        slurm_user:\n            Shell username that runs the `sbatch` command.\n        common_script_lines:\n            Arbitrary script lines that will always be included in the\n            sbatch script\n        working_dir:\n            Directory for both the cfut/SLURM and fractal-server files and logs\n        working_dir_user:\n            Directory for both the cfut/SLURM and fractal-server files and logs\n        map_jobid_to_slurm_files:\n            Dictionary with paths of slurm-related files for active jobs\n    \"\"\"\n\n    wait_thread_cls = FractalSlurmWaitThread\n    slurm_user: str\n    shutdown_file: str\n    common_script_lines: list[str]\n    user_cache_dir: str\n    working_dir: Path\n    working_dir_user: Path\n    map_jobid_to_slurm_files: dict[str, tuple[str, str, str]]\n    keep_pickle_files: bool\n    slurm_account: Optional[str]\n\n    def __init__(\n        self,\n        slurm_user: str,\n        working_dir: Path,\n        working_dir_user: Path,\n        shutdown_file: Optional[str] = None,\n        user_cache_dir: Optional[str] = None,\n        common_script_lines: Optional[list[str]] = None,\n        slurm_poll_interval: Optional[int] = None,\n        keep_pickle_files: bool = False,\n        slurm_account: Optional[str] = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Init method for FractalSlurmExecutor\n        \"\"\"\n\n        if not slurm_user:\n            raise RuntimeError(\n                \"Missing attribute FractalSlurmExecutor.slurm_user\"\n            )\n\n        super().__init__(*args, **kwargs)\n\n        self.keep_pickle_files = keep_pickle_files\n        self.slurm_user = slurm_user\n        self.slurm_account = slurm_account\n\n        self.common_script_lines = common_script_lines or []\n\n        # Check that SLURM account is not set here\n        try:\n            invalid_line = next(\n                line\n                for line in self.common_script_lines\n                if line.startswith(\"#SBATCH --account=\")\n            )\n            raise RuntimeError(\n                \"Invalid line in `FractalSlurmExecutor.common_script_lines`: \"\n                f\"'{invalid_line}'.\\n\"\n                \"SLURM account must be set via the request body of the \"\n                \"apply-workflow endpoint, or by modifying the user properties.\"\n            )\n        except StopIteration:\n            pass\n\n        self.working_dir = working_dir\n        if not _path_exists_as_user(\n            path=str(working_dir_user), user=self.slurm_user\n        ):\n            logger.info(f\"Missing folder {working_dir_user=}\")\n        self.user_cache_dir = user_cache_dir\n\n        self.working_dir_user = working_dir_user\n        self.map_jobid_to_slurm_files = {}\n\n        # Set the attribute slurm_poll_interval for self.wait_thread (see\n        # cfut.SlurmWaitThread)\n        if not slurm_poll_interval:\n            settings = Inject(get_settings)\n            slurm_poll_interval = settings.FRACTAL_SLURM_POLL_INTERVAL\n        self.wait_thread.slurm_poll_interval = slurm_poll_interval\n        self.wait_thread.slurm_user = self.slurm_user\n\n        self.wait_thread.shutdown_file = (\n            shutdown_file or (self.working_dir / SHUTDOWN_FILENAME).as_posix()\n        )\n        self.wait_thread.shutdown_callback = self.shutdown\n\n    def _cleanup(self, jobid: str) -&gt; None:\n        \"\"\"\n        Given a job ID as returned by _start, perform any necessary\n        cleanup after the job has finished.\n        \"\"\"\n        with self.jobs_lock:\n            self.map_jobid_to_slurm_files.pop(jobid)\n\n    def get_input_pickle_file_path(\n        self, arg: str, prefix: Optional[str] = None\n    ) -&gt; Path:\n        prefix = prefix or \"cfut\"\n        return self.working_dir / f\"{prefix}_in_{arg}.pickle\"\n\n    def get_output_pickle_file_path(\n        self, arg: str, prefix: Optional[str] = None\n    ) -&gt; Path:\n        prefix = prefix or \"cfut\"\n        return self.working_dir_user / f\"{prefix}_out_{arg}.pickle\"\n\n    def get_slurm_script_file_path(self, prefix: Optional[str] = None) -&gt; Path:\n        prefix = prefix or \"_temp\"\n        return self.working_dir / f\"{prefix}_slurm_submit.sbatch\"\n\n    def get_slurm_stdout_file_path(\n        self, arg: str = \"%j\", prefix: Optional[str] = None\n    ) -&gt; Path:\n        prefix = prefix or \"slurmpy.stdout\"\n        return self.working_dir_user / f\"{prefix}_slurm_{arg}.out\"\n\n    def get_slurm_stderr_file_path(\n        self, arg: str = \"%j\", prefix: Optional[str] = None\n    ) -&gt; Path:\n        prefix = prefix or \"slurmpy.stderr\"\n        return self.working_dir_user / f\"{prefix}_slurm_{arg}.err\"\n\n    def submit(\n        self,\n        fun: Callable[..., Any],\n        *fun_args: Sequence[Any],\n        slurm_config: Optional[SlurmConfig] = None,\n        task_files: Optional[TaskFiles] = None,\n        **fun_kwargs: dict,\n    ) -&gt; Future:\n        \"\"\"\n        Submit a function for execution on `FractalSlurmExecutor`\n\n        Arguments:\n            fun: The function to be executed\n            fun_args: Function positional arguments\n            fun_kwargs: Function keyword arguments\n            slurm_config:\n                A `SlurmConfig` object; if `None`, use\n                `get_default_slurm_config()`.\n            task_files:\n                A `TaskFiles` object; if `None`, use\n                `self.get_default_task_files()`.\n\n        Returns:\n            Future representing the execution of the current SLURM job.\n        \"\"\"\n\n        # Set defaults, if needed\n        if slurm_config is None:\n            slurm_config = get_default_slurm_config()\n        if task_files is None:\n            task_files = self.get_default_task_files()\n\n        # Set slurm_file_prefix\n        slurm_file_prefix = task_files.file_prefix\n\n        # Include common_script_lines in extra_lines\n        logger.debug(\n            f\"Adding {self.common_script_lines=} to \"\n            f\"{slurm_config.extra_lines=}, from submit method.\"\n        )\n        current_extra_lines = slurm_config.extra_lines or []\n        slurm_config.extra_lines = (\n            current_extra_lines + self.common_script_lines\n        )\n\n        # Adapt slurm_config to the fact that this is a single-task SlurmJob\n        # instance\n        slurm_config.tasks_per_job = 1\n        slurm_config.parallel_tasks_per_job = 1\n\n        fut = self._submit_job(\n            fun,\n            slurm_config=slurm_config,\n            slurm_file_prefix=slurm_file_prefix,\n            task_files=task_files,\n            single_task_submission=True,\n            args=fun_args,\n            kwargs=fun_kwargs,\n        )\n        return fut\n\n    def map(\n        self,\n        fn: Callable[..., Any],\n        iterable: list[Sequence[Any]],\n        *,\n        slurm_config: Optional[SlurmConfig] = None,\n        task_files: Optional[TaskFiles] = None,\n    ):\n        \"\"\"\n        Return an iterator with the results of several execution of a function\n\n        This function is based on `concurrent.futures.Executor.map` from Python\n        Standard Library 3.11.\n        Original Copyright 2009 Brian Quinlan. All Rights Reserved. Licensed to\n        PSF under a Contributor Agreement.\n\n        Main modifications from the PSF function:\n\n        1. Only `fn` and `iterable` can be assigned as positional arguments;\n        2. `*iterables` argument replaced with a single `iterable`;\n        3. `timeout` and `chunksize` arguments are not supported.\n\n        Arguments:\n            fn:\n                The function to be executed\n            iterable:\n                An iterable such that each element is the list of arguments to\n                be passed to `fn`, as in `fn(*args)`.\n            slurm_config:\n                A `SlurmConfig` object; if `None`, use\n                `get_default_slurm_config()`.\n            task_files:\n                A `TaskFiles` object; if `None`, use\n                `self.get_default_task_files()`.\n\n        \"\"\"\n\n        def _result_or_cancel(fut):\n            \"\"\"\n            This function is based on the Python Standard Library 3.11.\n            Original Copyright 2009 Brian Quinlan. All Rights Reserved.\n            Licensed to PSF under a Contributor Agreement.\n            \"\"\"\n            try:\n                try:\n                    return fut.result()\n                finally:\n                    fut.cancel()\n            finally:\n                # Break a reference cycle with the exception in\n                # self._exception\n                del fut\n\n        # Set defaults, if needed\n        if not slurm_config:\n            slurm_config = get_default_slurm_config()\n        if task_files is None:\n            task_files = self.get_default_task_files()\n\n        # Include common_script_lines in extra_lines\n        logger.debug(\n            f\"Adding {self.common_script_lines=} to \"\n            f\"{slurm_config.extra_lines=}, from map method.\"\n        )\n        current_extra_lines = slurm_config.extra_lines or []\n        slurm_config.extra_lines = (\n            current_extra_lines + self.common_script_lines\n        )\n\n        # Set file prefixes\n        general_slurm_file_prefix = str(task_files.task_order)\n\n        # Transform iterable into a list and count its elements\n        list_args = list(iterable)\n        tot_tasks = len(list_args)\n\n        # Set/validate parameters for task batching\n        tasks_per_job, parallel_tasks_per_job = heuristics(\n            # Number of parallel components (always known)\n            tot_tasks=len(list_args),\n            # Optional WorkflowTask attributes:\n            tasks_per_job=slurm_config.tasks_per_job,\n            parallel_tasks_per_job=slurm_config.parallel_tasks_per_job,  # noqa\n            # Task requirements (multiple possible sources):\n            cpus_per_task=slurm_config.cpus_per_task,\n            mem_per_task=slurm_config.mem_per_task_MB,\n            # Fractal configuration variables (soft/hard limits):\n            target_cpus_per_job=slurm_config.target_cpus_per_job,\n            target_mem_per_job=slurm_config.target_mem_per_job,\n            target_num_jobs=slurm_config.target_num_jobs,\n            max_cpus_per_job=slurm_config.max_cpus_per_job,\n            max_mem_per_job=slurm_config.max_mem_per_job,\n            max_num_jobs=slurm_config.max_num_jobs,\n        )\n        slurm_config.parallel_tasks_per_job = parallel_tasks_per_job\n        slurm_config.tasks_per_job = tasks_per_job\n\n        # Divide arguments in batches of `n_tasks_per_script` tasks each\n        args_batches = []\n        batch_size = tasks_per_job\n        for ind_chunk in range(0, tot_tasks, batch_size):\n            args_batches.append(\n                list_args[ind_chunk : ind_chunk + batch_size]  # noqa\n            )\n        if len(args_batches) != math.ceil(tot_tasks / tasks_per_job):\n            raise RuntimeError(\"Something wrong here while batching tasks\")\n\n        # Construct list of futures (one per SLURM job, i.e. one per batch)\n        fs = []\n        current_component_index = 0\n        for ind_batch, batch in enumerate(args_batches):\n            batch_size = len(batch)\n            this_slurm_file_prefix = (\n                f\"{general_slurm_file_prefix}_batch_{ind_batch:06d}\"\n            )\n            fs.append(\n                self._submit_job(\n                    fn,\n                    slurm_config=slurm_config,\n                    slurm_file_prefix=this_slurm_file_prefix,\n                    task_files=task_files,\n                    single_task_submission=False,\n                    components=batch,\n                )\n            )\n            current_component_index += batch_size\n\n        # Yield must be hidden in closure so that the futures are submitted\n        # before the first iterator value is required.\n        # NOTE: In this custom map() method, _result_or_cancel(fs.pop()) is an\n        # iterable of results (if successful), and we should yield its elements\n        # rather than the whole iterable.\n        def result_iterator():\n            \"\"\"\n            This function is based on the Python Standard Library 3.11.\n            Original Copyright 2009 Brian Quinlan. All Rights Reserved.\n            Licensed to PSF under a Contributor Agreement.\n            \"\"\"\n            try:\n                # reverse to keep finishing order\n                fs.reverse()\n                while fs:\n                    # Careful not to keep a reference to the popped future\n                    results = _result_or_cancel(fs.pop())\n                    for res in results:\n                        yield res\n            finally:\n                for future in fs:\n                    future.cancel()\n\n        return result_iterator()\n\n    def _submit_job(\n        self,\n        fun: Callable[..., Any],\n        slurm_file_prefix: str,\n        task_files: TaskFiles,\n        slurm_config: SlurmConfig,\n        single_task_submission: bool = False,\n        args: Optional[Sequence[Any]] = None,\n        kwargs: Optional[dict] = None,\n        components: Optional[list[Any]] = None,\n    ) -&gt; Future:\n        \"\"\"\n        Submit a multi-task job to the pool, where each task is handled via the\n        pickle/remote logic\n\n        NOTE: this method has different behaviors when it is called from the\n        `self.submit` or `self.map` methods (which is also encoded in\n        `single_task_submission`):\n\n        * When called from `self.submit`, it supports general `args` and\n          `kwargs` arguments;\n        * When called from `self.map`, there cannot be any `args` or `kwargs`\n          argument, but there must be a `components` argument.\n\n        Arguments:\n            fun:\n            slurm_file_prefix:\n            task_files:\n            slurm_config:\n            single_task_submission:\n            args:\n            kwargs:\n            components:\n\n        Returns:\n            Future representing the execution of the current SLURM job.\n        \"\"\"\n        fut: Future = Future()\n\n        # Inject SLURM account (if set) into slurm_config\n        if self.slurm_account:\n            slurm_config.account = self.slurm_account\n\n        # Define slurm-job-related files\n        if single_task_submission:\n            if components is not None:\n                raise ValueError(\n                    f\"{single_task_submission=} but components is not None\"\n                )\n            job = SlurmJob(\n                slurm_file_prefix=slurm_file_prefix,\n                num_tasks_tot=1,\n                slurm_config=slurm_config,\n            )\n            if job.num_tasks_tot &gt; 1:\n                raise ValueError(\n                    \"{single_task_submission=} but {job.num_tasks_tot=}\"\n                )\n            job.single_task_submission = True\n            job.wftask_file_prefixes = (task_files.file_prefix,)\n        else:\n            if not components or len(components) &lt; 1:\n                raise ValueError(\n                    \"In FractalSlurmExecutor._submit_job, given \"\n                    f\"{components=}.\"\n                )\n            num_tasks_tot = len(components)\n            job = SlurmJob(\n                slurm_file_prefix=slurm_file_prefix,\n                num_tasks_tot=num_tasks_tot,\n                slurm_config=slurm_config,\n            )\n\n            _prefixes = []\n            for component in components:\n                if isinstance(component, dict):\n                    # This is needed for V2\n                    actual_component = component.get(_COMPONENT_KEY_, None)\n                else:\n                    actual_component = component\n                _prefixes.append(\n                    get_task_file_paths(\n                        workflow_dir=task_files.workflow_dir,\n                        workflow_dir_user=task_files.workflow_dir_user,\n                        task_order=task_files.task_order,\n                        component=actual_component,\n                    ).file_prefix\n                )\n            job.wftask_file_prefixes = tuple(_prefixes)\n\n        # Define I/O pickle file names/paths\n        job.input_pickle_files = tuple(\n            self.get_input_pickle_file_path(\n                job.workerids[ind],\n                prefix=job.wftask_file_prefixes[ind],\n            )\n            for ind in range(job.num_tasks_tot)\n        )\n        job.output_pickle_files = tuple(\n            self.get_output_pickle_file_path(\n                job.workerids[ind],\n                prefix=job.wftask_file_prefixes[ind],\n            )\n            for ind in range(job.num_tasks_tot)\n        )\n\n        # Define SLURM-job file names/paths\n        job.slurm_script = self.get_slurm_script_file_path(\n            prefix=job.slurm_file_prefix\n        )\n        job.slurm_stdout = self.get_slurm_stdout_file_path(\n            prefix=job.slurm_file_prefix\n        )\n        job.slurm_stderr = self.get_slurm_stderr_file_path(\n            prefix=job.slurm_file_prefix\n        )\n\n        # Dump serialized versions+function+args+kwargs to pickle file\n        versions = dict(\n            python=sys.version_info[:3],\n            cloudpickle=cloudpickle.__version__,\n            fractal_server=__VERSION__,\n        )\n        if job.single_task_submission:\n            _args = args or []\n            _kwargs = kwargs or {}\n            funcser = cloudpickle.dumps((versions, fun, _args, _kwargs))\n            with open(job.input_pickle_files[0], \"wb\") as f:\n                f.write(funcser)\n        else:\n            for ind_component, component in enumerate(components):\n                _args = [component]\n                _kwargs = {}\n                funcser = cloudpickle.dumps((versions, fun, _args, _kwargs))\n                with open(job.input_pickle_files[ind_component], \"wb\") as f:\n                    f.write(funcser)\n\n        # Submit job to SLURM, and get jobid\n        jobid, job = self._start(job)\n\n        # Add the SLURM script/out/err paths to map_jobid_to_slurm_files (this\n        # must be after self._start(job), so that \"%j\" has already been\n        # replaced with the job ID)\n        with self.jobs_lock:\n            self.map_jobid_to_slurm_files[jobid] = (\n                job.slurm_script.as_posix(),\n                job.slurm_stdout.as_posix(),\n                job.slurm_stderr.as_posix(),\n            )\n\n        # Thread will wait for it to finish.\n        self.wait_thread.wait(\n            filenames=job.get_clean_output_pickle_files(),\n            jobid=jobid,\n        )\n\n        with self.jobs_lock:\n            self.jobs[jobid] = (fut, job)\n        return fut\n\n    def _prepare_JobExecutionError(\n        self, jobid: str, info: str\n    ) -&gt; JobExecutionError:\n        \"\"\"\n        Prepare the `JobExecutionError` for a given job\n\n        This method creates a `JobExecutionError` object and sets its attribute\n        to the appropriate SLURM-related file names. Note that the method shoul\n        always be called after values in `self.map_jobid_to_slurm_files` have\n        been updated, so that they point to `self.working_dir` files which are\n        readable from `fractal-server`.\n\n        Arguments:\n            jobid:\n                ID of the SLURM job.\n            info:\n        \"\"\"\n        # Extract SLURM file paths\n        with self.jobs_lock:\n            (\n                slurm_script_file,\n                slurm_stdout_file,\n                slurm_stderr_file,\n            ) = self.map_jobid_to_slurm_files[jobid]\n        # Construct JobExecutionError exception\n        job_exc = JobExecutionError(\n            cmd_file=slurm_script_file,\n            stdout_file=slurm_stdout_file,\n            stderr_file=slurm_stderr_file,\n            info=info,\n        )\n        return job_exc\n\n    def _completion(self, jobid: str) -&gt; None:\n        \"\"\"\n        Callback function to be executed whenever a job finishes.\n\n        This function is executed by self.wait_thread (triggered by either\n        finding an existing output pickle file `out_path` or finding that the\n        SLURM job is over). Since this takes place on a different thread,\n        failures may not be captured by the main thread; we use a broad\n        try/except block, so that those exceptions are reported to the main\n        thread via `fut.set_exception(...)`.\n\n        Arguments:\n            jobid: ID of the SLURM job\n        \"\"\"\n        # Handle all uncaught exceptions in this broad try/except block\n        try:\n\n            # Retrieve job\n            with self.jobs_lock:\n                try:\n                    fut, job = self.jobs.pop(jobid)\n                except KeyError:\n                    return\n                if not self.jobs:\n                    self.jobs_empty_cond.notify_all()\n\n            # Copy all relevant files from self.working_dir_user to\n            # self.working_dir\n\n            self._copy_files_from_user_to_server(job)\n\n            # Update the paths to use the files in self.working_dir (rather\n            # than the user's ones in self.working_dir_user)\n            with self.jobs_lock:\n                self.map_jobid_to_slurm_files[jobid]\n                (\n                    slurm_script_file,\n                    slurm_stdout_file,\n                    slurm_stderr_file,\n                ) = self.map_jobid_to_slurm_files[jobid]\n            new_slurm_stdout_file = str(\n                self.working_dir / Path(slurm_stdout_file).name\n            )\n            new_slurm_stderr_file = str(\n                self.working_dir / Path(slurm_stderr_file).name\n            )\n            with self.jobs_lock:\n                self.map_jobid_to_slurm_files[jobid] = (\n                    slurm_script_file,\n                    new_slurm_stdout_file,\n                    new_slurm_stderr_file,\n                )\n\n            in_paths = job.input_pickle_files\n            out_paths = tuple(\n                self.working_dir / f.name for f in job.output_pickle_files\n            )\n\n            outputs = []\n            for ind_out_path, out_path in enumerate(out_paths):\n                in_path = in_paths[ind_out_path]\n\n                # The output pickle file may be missing because of some slow\n                # filesystem operation; wait some time before considering it as\n                # missing\n                if not out_path.exists():\n                    settings = Inject(get_settings)\n                    time.sleep(settings.FRACTAL_SLURM_ERROR_HANDLING_INTERVAL)\n                if not out_path.exists():\n                    # Output pickle file is missing\n                    info = (\n                        \"Output pickle file of the FractalSlurmExecutor job \"\n                        \"not found.\\n\"\n                        f\"Expected file path: {str(out_path)}.\\n\"\n                        \"Here are some possible reasons:\\n\"\n                        \"1. The SLURM job was scancel-ed, either by the user \"\n                        \"or due to an error (e.g. an out-of-memory or timeout \"\n                        \"error). Note that if the scancel took place before \"\n                        \"the job started running, the SLURM out/err files \"\n                        \"will be empty.\\n\"\n                        \"2. Some error occurred upon writing the file to disk \"\n                        \"(e.g. because there is not enough space on disk, or \"\n                        \"due to an overloaded NFS filesystem). \"\n                        \"Note that the server configuration has \"\n                        \"FRACTAL_SLURM_ERROR_HANDLING_INTERVAL=\"\n                        f\"{settings.FRACTAL_SLURM_ERROR_HANDLING_INTERVAL} \"\n                        \"seconds.\\n\"\n                    )\n                    job_exc = self._prepare_JobExecutionError(jobid, info=info)\n                    try:\n                        fut.set_exception(job_exc)\n                        return\n                    except InvalidStateError:\n                        logger.warning(\n                            f\"Future {fut} (SLURM job ID: {jobid}) was already\"\n                            \" cancelled, exit from\"\n                            \" FractalSlurmExecutor._completion.\"\n                        )\n                        if not self.keep_pickle_files:\n                            in_path.unlink()\n                        self._cleanup(jobid)\n                        return\n\n                # Read the task output (note: we now know that out_path exists)\n                with out_path.open(\"rb\") as f:\n                    outdata = f.read()\n                # Note: output can be either the task result (typically a\n                # dictionary) or an ExceptionProxy object; in the latter\n                # case, the ExceptionProxy definition is also part of the\n                # pickle file (thanks to cloudpickle.dumps).\n                success, output = cloudpickle.loads(outdata)\n                try:\n                    if success:\n                        outputs.append(output)\n                    else:\n                        proxy = output\n                        if proxy.exc_type_name == \"JobExecutionError\":\n                            job_exc = self._prepare_JobExecutionError(\n                                jobid, info=proxy.kwargs.get(\"info\", None)\n                            )\n                            fut.set_exception(job_exc)\n                            return\n                        else:\n                            # This branch catches both TaskExecutionError's\n                            # (coming from the typical fractal-server\n                            # execution of tasks, and with additional\n                            # fractal-specific kwargs) or arbitrary\n                            # exceptions (coming from a direct use of\n                            # FractalSlurmExecutor, possibly outside\n                            # fractal-server)\n                            kwargs = {}\n                            for key in [\n                                \"workflow_task_id\",\n                                \"workflow_task_order\",\n                                \"task_name\",\n                            ]:\n                                if key in proxy.kwargs.keys():\n                                    kwargs[key] = proxy.kwargs[key]\n                            exc = TaskExecutionError(proxy.tb, **kwargs)\n                            fut.set_exception(exc)\n                            return\n                    if not self.keep_pickle_files:\n                        out_path.unlink()\n                except InvalidStateError:\n                    logger.warning(\n                        f\"Future {fut} (SLURM job ID: {jobid}) was already\"\n                        \" cancelled, exit from\"\n                        \" FractalSlurmExecutor._completion.\"\n                    )\n                    if not self.keep_pickle_files:\n                        out_path.unlink()\n                        in_path.unlink()\n                    self._cleanup(jobid)\n                    return\n\n                # Clean up input pickle file\n                if not self.keep_pickle_files:\n                    in_path.unlink()\n            self._cleanup(jobid)\n            if job.single_task_submission:\n                fut.set_result(outputs[0])\n            else:\n                fut.set_result(outputs)\n            return\n\n        except Exception as e:\n            try:\n                fut.set_exception(e)\n                return\n            except InvalidStateError:\n                logger.warning(\n                    f\"Future {fut} (SLURM job ID: {jobid}) was already\"\n                    \" cancelled, exit from\"\n                    \" FractalSlurmExecutor._completion.\"\n                )\n\n    def _copy_files_from_user_to_server(\n        self,\n        job: SlurmJob,\n    ):\n        \"\"\"\n        Impersonate the user and copy task-related files\n\n        For all files in `self.working_dir_user` that start with\n        `job.file_prefix`, read them (with `sudo -u` impersonation) and write\n        them to `self.working_dir`.\n\n        Files to copy:\n        * Job-related files (SLURM stderr/stdout files); with prefix\n          `job.slurm_file_prefix`;\n        * Task-related files (stderr/stdout, args.json, metadiff.json, output\n          pickle), with prefixes `job.wftask_file_prefixes`.\n\n        Arguments:\n            job:\n                `SlurmJob` object (needed for its prefixes-related attributes).\n\n        Raises:\n            JobExecutionError: If a `cat` command fails.\n        \"\"\"\n        logger.debug(\"Enter _copy_files_from_user_to_server\")\n        if self.working_dir_user == self.working_dir:\n            return\n\n        prefixes = set(\n            [job.slurm_file_prefix] + list(job.wftask_file_prefixes)\n        )\n\n        logger.debug(f\"[_copy_files_from_user_to_server] {prefixes=}\")\n        logger.debug(\n            f\"[_copy_files_from_user_to_server] {str(self.working_dir_user)=}\"\n        )\n\n        for prefix in prefixes:\n\n            if prefix == job.slurm_file_prefix:\n                files_to_copy = _glob_as_user(\n                    folder=str(self.working_dir_user),\n                    user=self.slurm_user,\n                    startswith=prefix,\n                )\n            else:\n                files_to_copy = _glob_as_user_strict(\n                    folder=str(self.working_dir_user),\n                    user=self.slurm_user,\n                    startswith=prefix,\n                )\n\n            logger.debug(\n                \"[_copy_files_from_user_to_server] \"\n                f\"{prefix=}, {len(files_to_copy)=}\"\n            )\n\n            for source_file_name in files_to_copy:\n                if \" \" in source_file_name:\n                    raise ValueError(\n                        f'source_file_name=\"{source_file_name}\" '\n                        \"contains whitespaces\"\n                    )\n                source_file_path = str(\n                    self.working_dir_user / source_file_name\n                )\n\n                # Read source_file_path (requires sudo)\n                # NOTE: By setting encoding=None, we read/write bytes instead\n                # of strings; this is needed to also handle pickle files.\n                cmd = f\"cat {source_file_path}\"\n                res = _run_command_as_user(\n                    cmd=cmd, user=self.slurm_user, encoding=None\n                )\n                if res.returncode != 0:\n                    info = (\n                        f'Running cmd=\"{cmd}\" as {self.slurm_user=} failed\\n\\n'\n                        f\"{res.returncode=}\\n\\n\"\n                        f\"{res.stdout=}\\n\\n{res.stderr=}\\n\"\n                    )\n                    logger.error(info)\n                    raise JobExecutionError(info)\n                # Write to dest_file_path (including empty files)\n                dest_file_path = str(self.working_dir / source_file_name)\n                with open(dest_file_path, \"wb\") as f:\n                    f.write(res.stdout)\n        logger.debug(\"[_copy_files_from_user_to_server] End\")\n\n    def _start(\n        self,\n        job: SlurmJob,\n    ) -&gt; tuple[str, SlurmJob]:\n        \"\"\"\n        Submit function for execution on a SLURM cluster\n        \"\"\"\n\n        # Prepare commands to be included in SLURM submission script\n        settings = Inject(get_settings)\n        python_worker_interpreter = (\n            settings.FRACTAL_SLURM_WORKER_PYTHON or sys.executable\n        )\n\n        cmdlines = []\n        for ind_task in range(job.num_tasks_tot):\n            input_pickle_file = job.input_pickle_files[ind_task]\n            output_pickle_file = job.output_pickle_files[ind_task]\n            cmdlines.append(\n                (\n                    f\"{python_worker_interpreter}\"\n                    \" -m fractal_server.app.runner.executors.slurm.remote \"\n                    f\"--input-file {input_pickle_file} \"\n                    f\"--output-file {output_pickle_file}\"\n                )\n            )\n\n        # ...\n        sbatch_script = self._prepare_sbatch_script(\n            slurm_config=job.slurm_config,\n            list_commands=cmdlines,\n            slurm_out_path=str(job.slurm_stdout),\n            slurm_err_path=str(job.slurm_stderr),\n        )\n\n        # Submit job via sbatch, and retrieve jobid\n\n        # Write script content to a job.slurm_script\n        with job.slurm_script.open(\"w\") as f:\n            f.write(sbatch_script)\n\n        # Prepare submission command\n        pre_command = f\"sudo --non-interactive -u {self.slurm_user}\"\n        submit_command = f\"sbatch --parsable {job.slurm_script}\"\n        full_command = f\"{pre_command} {submit_command}\"\n\n        # Submit SLURM job and retrieve job ID\n        output = _subprocess_run_or_raise(full_command)\n        try:\n            jobid = int(output.stdout)\n        except ValueError as e:\n            error_msg = (\n                f\"Submit command `{full_command}` returned \"\n                f\"`{output.stdout=}` which cannot be cast to an integer \"\n                f\"SLURM-job ID. Original error:\\n{str(e)}\"\n            )\n            logger.error(error_msg)\n            raise JobExecutionError(info=error_msg)\n        jobid_str = str(jobid)\n\n        # Plug SLURM job id in stdout/stderr file paths\n        job.slurm_stdout = Path(\n            job.slurm_stdout.as_posix().replace(\"%j\", jobid_str)\n        )\n        job.slurm_stderr = Path(\n            job.slurm_stderr.as_posix().replace(\"%j\", jobid_str)\n        )\n\n        return jobid_str, job\n\n    def _prepare_sbatch_script(\n        self,\n        *,\n        list_commands: list[str],\n        slurm_out_path: str,\n        slurm_err_path: str,\n        slurm_config: SlurmConfig,\n    ):\n\n        num_tasks_max_running = slurm_config.parallel_tasks_per_job\n        mem_per_task_MB = slurm_config.mem_per_task_MB\n\n        # Set ntasks\n        ntasks = min(len(list_commands), num_tasks_max_running)\n        if len(list_commands) &lt; num_tasks_max_running:\n            ntasks = len(list_commands)\n            slurm_config.parallel_tasks_per_job = ntasks\n            logger.debug(\n                f\"{len(list_commands)=} is smaller than \"\n                f\"{num_tasks_max_running=}. Setting {ntasks=}.\"\n            )\n\n        # Prepare SLURM preamble based on SlurmConfig object\n        script_lines = slurm_config.to_sbatch_preamble(\n            user_cache_dir=self.user_cache_dir\n        )\n\n        # Extend SLURM preamble with variable which are not in SlurmConfig, and\n        # fix their order\n        script_lines.extend(\n            [\n                f\"#SBATCH --err={slurm_err_path}\",\n                f\"#SBATCH --out={slurm_out_path}\",\n                f\"#SBATCH -D {self.working_dir_user}\",\n            ]\n        )\n        script_lines = slurm_config.sort_script_lines(script_lines)\n        logger.debug(script_lines)\n\n        # Always print output of `pwd`\n        script_lines.append('echo \"Working directory (pwd): `pwd`\"\\n')\n\n        # Complete script preamble\n        script_lines.append(\"\\n\")\n\n        # Include command lines\n        tmp_list_commands = copy(list_commands)\n        while tmp_list_commands:\n            if tmp_list_commands:\n                cmd = tmp_list_commands.pop(0)  # take first element\n                script_lines.append(\n                    \"srun --ntasks=1 --cpus-per-task=$SLURM_CPUS_PER_TASK \"\n                    f\"--mem={mem_per_task_MB}MB \"\n                    f\"{cmd} &amp;\"\n                )\n        script_lines.append(\"wait\\n\")\n\n        script = \"\\n\".join(script_lines)\n        return script\n\n    def get_default_task_files(self) -&gt; TaskFiles:\n        \"\"\"\n        This will be called when self.submit or self.map are called from\n        outside fractal-server, and then lack some optional arguments.\n        \"\"\"\n        import random\n\n        task_files = TaskFiles(\n            workflow_dir=self.working_dir,\n            workflow_dir_user=self.working_dir_user,\n            task_order=random.randint(10000, 99999),  # nosec\n        )\n        return task_files\n\n    def shutdown(self, wait=True, *, cancel_futures=False):\n        \"\"\"\n        Clean up all executor variables. Note that this function is executed on\n        the self.wait_thread thread, see _completion.\n        \"\"\"\n\n        logger.debug(\"Executor shutdown: start\")\n\n        # Handle all job futures\n        slurm_jobs_to_scancel = []\n        with self.jobs_lock:\n            while self.jobs:\n                jobid, fut_and_job = self.jobs.popitem()\n                slurm_jobs_to_scancel.append(jobid)\n                fut, job = fut_and_job[:]\n                self.map_jobid_to_slurm_files.pop(jobid)\n                if not fut.cancelled():\n                    fut.set_exception(\n                        JobExecutionError(\n                            \"Job cancelled due to executor shutdown.\"\n                        )\n                    )\n                    fut.cancel()\n\n        # Cancel SLURM jobs\n        if slurm_jobs_to_scancel:\n            scancel_string = \" \".join(slurm_jobs_to_scancel)\n            logger.warning(f\"Now scancel-ing SLURM jobs {scancel_string}\")\n            pre_command = f\"sudo --non-interactive -u {self.slurm_user}\"\n            submit_command = f\"scancel {scancel_string}\"\n            full_command = f\"{pre_command} {submit_command}\"\n            logger.debug(f\"Now execute `{full_command}`\")\n            try:\n                subprocess.run(  # nosec\n                    shlex.split(full_command),\n                    capture_output=True,\n                    check=True,\n                    encoding=\"utf-8\",\n                )\n            except subprocess.CalledProcessError as e:\n                error_msg = (\n                    f\"Cancel command `{full_command}` failed. \"\n                    f\"Original error:\\n{str(e)}\"\n                )\n                logger.error(error_msg)\n                raise JobExecutionError(info=error_msg)\n\n        logger.debug(\"Executor shutdown: end\")\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor.__init__","title":"<code>__init__(slurm_user, working_dir, working_dir_user, shutdown_file=None, user_cache_dir=None, common_script_lines=None, slurm_poll_interval=None, keep_pickle_files=False, slurm_account=None, *args, **kwargs)</code>","text":"<p>Init method for FractalSlurmExecutor</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def __init__(\n    self,\n    slurm_user: str,\n    working_dir: Path,\n    working_dir_user: Path,\n    shutdown_file: Optional[str] = None,\n    user_cache_dir: Optional[str] = None,\n    common_script_lines: Optional[list[str]] = None,\n    slurm_poll_interval: Optional[int] = None,\n    keep_pickle_files: bool = False,\n    slurm_account: Optional[str] = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Init method for FractalSlurmExecutor\n    \"\"\"\n\n    if not slurm_user:\n        raise RuntimeError(\n            \"Missing attribute FractalSlurmExecutor.slurm_user\"\n        )\n\n    super().__init__(*args, **kwargs)\n\n    self.keep_pickle_files = keep_pickle_files\n    self.slurm_user = slurm_user\n    self.slurm_account = slurm_account\n\n    self.common_script_lines = common_script_lines or []\n\n    # Check that SLURM account is not set here\n    try:\n        invalid_line = next(\n            line\n            for line in self.common_script_lines\n            if line.startswith(\"#SBATCH --account=\")\n        )\n        raise RuntimeError(\n            \"Invalid line in `FractalSlurmExecutor.common_script_lines`: \"\n            f\"'{invalid_line}'.\\n\"\n            \"SLURM account must be set via the request body of the \"\n            \"apply-workflow endpoint, or by modifying the user properties.\"\n        )\n    except StopIteration:\n        pass\n\n    self.working_dir = working_dir\n    if not _path_exists_as_user(\n        path=str(working_dir_user), user=self.slurm_user\n    ):\n        logger.info(f\"Missing folder {working_dir_user=}\")\n    self.user_cache_dir = user_cache_dir\n\n    self.working_dir_user = working_dir_user\n    self.map_jobid_to_slurm_files = {}\n\n    # Set the attribute slurm_poll_interval for self.wait_thread (see\n    # cfut.SlurmWaitThread)\n    if not slurm_poll_interval:\n        settings = Inject(get_settings)\n        slurm_poll_interval = settings.FRACTAL_SLURM_POLL_INTERVAL\n    self.wait_thread.slurm_poll_interval = slurm_poll_interval\n    self.wait_thread.slurm_user = self.slurm_user\n\n    self.wait_thread.shutdown_file = (\n        shutdown_file or (self.working_dir / SHUTDOWN_FILENAME).as_posix()\n    )\n    self.wait_thread.shutdown_callback = self.shutdown\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor._cleanup","title":"<code>_cleanup(jobid)</code>","text":"<p>Given a job ID as returned by _start, perform any necessary cleanup after the job has finished.</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def _cleanup(self, jobid: str) -&gt; None:\n    \"\"\"\n    Given a job ID as returned by _start, perform any necessary\n    cleanup after the job has finished.\n    \"\"\"\n    with self.jobs_lock:\n        self.map_jobid_to_slurm_files.pop(jobid)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor._completion","title":"<code>_completion(jobid)</code>","text":"<p>Callback function to be executed whenever a job finishes.</p> <p>This function is executed by self.wait_thread (triggered by either finding an existing output pickle file <code>out_path</code> or finding that the SLURM job is over). Since this takes place on a different thread, failures may not be captured by the main thread; we use a broad try/except block, so that those exceptions are reported to the main thread via <code>fut.set_exception(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>jobid</code> <code>str</code> <p>ID of the SLURM job</p> required Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def _completion(self, jobid: str) -&gt; None:\n    \"\"\"\n    Callback function to be executed whenever a job finishes.\n\n    This function is executed by self.wait_thread (triggered by either\n    finding an existing output pickle file `out_path` or finding that the\n    SLURM job is over). Since this takes place on a different thread,\n    failures may not be captured by the main thread; we use a broad\n    try/except block, so that those exceptions are reported to the main\n    thread via `fut.set_exception(...)`.\n\n    Arguments:\n        jobid: ID of the SLURM job\n    \"\"\"\n    # Handle all uncaught exceptions in this broad try/except block\n    try:\n\n        # Retrieve job\n        with self.jobs_lock:\n            try:\n                fut, job = self.jobs.pop(jobid)\n            except KeyError:\n                return\n            if not self.jobs:\n                self.jobs_empty_cond.notify_all()\n\n        # Copy all relevant files from self.working_dir_user to\n        # self.working_dir\n\n        self._copy_files_from_user_to_server(job)\n\n        # Update the paths to use the files in self.working_dir (rather\n        # than the user's ones in self.working_dir_user)\n        with self.jobs_lock:\n            self.map_jobid_to_slurm_files[jobid]\n            (\n                slurm_script_file,\n                slurm_stdout_file,\n                slurm_stderr_file,\n            ) = self.map_jobid_to_slurm_files[jobid]\n        new_slurm_stdout_file = str(\n            self.working_dir / Path(slurm_stdout_file).name\n        )\n        new_slurm_stderr_file = str(\n            self.working_dir / Path(slurm_stderr_file).name\n        )\n        with self.jobs_lock:\n            self.map_jobid_to_slurm_files[jobid] = (\n                slurm_script_file,\n                new_slurm_stdout_file,\n                new_slurm_stderr_file,\n            )\n\n        in_paths = job.input_pickle_files\n        out_paths = tuple(\n            self.working_dir / f.name for f in job.output_pickle_files\n        )\n\n        outputs = []\n        for ind_out_path, out_path in enumerate(out_paths):\n            in_path = in_paths[ind_out_path]\n\n            # The output pickle file may be missing because of some slow\n            # filesystem operation; wait some time before considering it as\n            # missing\n            if not out_path.exists():\n                settings = Inject(get_settings)\n                time.sleep(settings.FRACTAL_SLURM_ERROR_HANDLING_INTERVAL)\n            if not out_path.exists():\n                # Output pickle file is missing\n                info = (\n                    \"Output pickle file of the FractalSlurmExecutor job \"\n                    \"not found.\\n\"\n                    f\"Expected file path: {str(out_path)}.\\n\"\n                    \"Here are some possible reasons:\\n\"\n                    \"1. The SLURM job was scancel-ed, either by the user \"\n                    \"or due to an error (e.g. an out-of-memory or timeout \"\n                    \"error). Note that if the scancel took place before \"\n                    \"the job started running, the SLURM out/err files \"\n                    \"will be empty.\\n\"\n                    \"2. Some error occurred upon writing the file to disk \"\n                    \"(e.g. because there is not enough space on disk, or \"\n                    \"due to an overloaded NFS filesystem). \"\n                    \"Note that the server configuration has \"\n                    \"FRACTAL_SLURM_ERROR_HANDLING_INTERVAL=\"\n                    f\"{settings.FRACTAL_SLURM_ERROR_HANDLING_INTERVAL} \"\n                    \"seconds.\\n\"\n                )\n                job_exc = self._prepare_JobExecutionError(jobid, info=info)\n                try:\n                    fut.set_exception(job_exc)\n                    return\n                except InvalidStateError:\n                    logger.warning(\n                        f\"Future {fut} (SLURM job ID: {jobid}) was already\"\n                        \" cancelled, exit from\"\n                        \" FractalSlurmExecutor._completion.\"\n                    )\n                    if not self.keep_pickle_files:\n                        in_path.unlink()\n                    self._cleanup(jobid)\n                    return\n\n            # Read the task output (note: we now know that out_path exists)\n            with out_path.open(\"rb\") as f:\n                outdata = f.read()\n            # Note: output can be either the task result (typically a\n            # dictionary) or an ExceptionProxy object; in the latter\n            # case, the ExceptionProxy definition is also part of the\n            # pickle file (thanks to cloudpickle.dumps).\n            success, output = cloudpickle.loads(outdata)\n            try:\n                if success:\n                    outputs.append(output)\n                else:\n                    proxy = output\n                    if proxy.exc_type_name == \"JobExecutionError\":\n                        job_exc = self._prepare_JobExecutionError(\n                            jobid, info=proxy.kwargs.get(\"info\", None)\n                        )\n                        fut.set_exception(job_exc)\n                        return\n                    else:\n                        # This branch catches both TaskExecutionError's\n                        # (coming from the typical fractal-server\n                        # execution of tasks, and with additional\n                        # fractal-specific kwargs) or arbitrary\n                        # exceptions (coming from a direct use of\n                        # FractalSlurmExecutor, possibly outside\n                        # fractal-server)\n                        kwargs = {}\n                        for key in [\n                            \"workflow_task_id\",\n                            \"workflow_task_order\",\n                            \"task_name\",\n                        ]:\n                            if key in proxy.kwargs.keys():\n                                kwargs[key] = proxy.kwargs[key]\n                        exc = TaskExecutionError(proxy.tb, **kwargs)\n                        fut.set_exception(exc)\n                        return\n                if not self.keep_pickle_files:\n                    out_path.unlink()\n            except InvalidStateError:\n                logger.warning(\n                    f\"Future {fut} (SLURM job ID: {jobid}) was already\"\n                    \" cancelled, exit from\"\n                    \" FractalSlurmExecutor._completion.\"\n                )\n                if not self.keep_pickle_files:\n                    out_path.unlink()\n                    in_path.unlink()\n                self._cleanup(jobid)\n                return\n\n            # Clean up input pickle file\n            if not self.keep_pickle_files:\n                in_path.unlink()\n        self._cleanup(jobid)\n        if job.single_task_submission:\n            fut.set_result(outputs[0])\n        else:\n            fut.set_result(outputs)\n        return\n\n    except Exception as e:\n        try:\n            fut.set_exception(e)\n            return\n        except InvalidStateError:\n            logger.warning(\n                f\"Future {fut} (SLURM job ID: {jobid}) was already\"\n                \" cancelled, exit from\"\n                \" FractalSlurmExecutor._completion.\"\n            )\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor._copy_files_from_user_to_server","title":"<code>_copy_files_from_user_to_server(job)</code>","text":"<p>Impersonate the user and copy task-related files</p> <p>For all files in <code>self.working_dir_user</code> that start with <code>job.file_prefix</code>, read them (with <code>sudo -u</code> impersonation) and write them to <code>self.working_dir</code>.</p> <p>Files to copy: * Job-related files (SLURM stderr/stdout files); with prefix   <code>job.slurm_file_prefix</code>; * Task-related files (stderr/stdout, args.json, metadiff.json, output   pickle), with prefixes <code>job.wftask_file_prefixes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>SlurmJob</code> <p><code>SlurmJob</code> object (needed for its prefixes-related attributes).</p> required <p>Raises:</p> Type Description <code>JobExecutionError</code> <p>If a <code>cat</code> command fails.</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def _copy_files_from_user_to_server(\n    self,\n    job: SlurmJob,\n):\n    \"\"\"\n    Impersonate the user and copy task-related files\n\n    For all files in `self.working_dir_user` that start with\n    `job.file_prefix`, read them (with `sudo -u` impersonation) and write\n    them to `self.working_dir`.\n\n    Files to copy:\n    * Job-related files (SLURM stderr/stdout files); with prefix\n      `job.slurm_file_prefix`;\n    * Task-related files (stderr/stdout, args.json, metadiff.json, output\n      pickle), with prefixes `job.wftask_file_prefixes`.\n\n    Arguments:\n        job:\n            `SlurmJob` object (needed for its prefixes-related attributes).\n\n    Raises:\n        JobExecutionError: If a `cat` command fails.\n    \"\"\"\n    logger.debug(\"Enter _copy_files_from_user_to_server\")\n    if self.working_dir_user == self.working_dir:\n        return\n\n    prefixes = set(\n        [job.slurm_file_prefix] + list(job.wftask_file_prefixes)\n    )\n\n    logger.debug(f\"[_copy_files_from_user_to_server] {prefixes=}\")\n    logger.debug(\n        f\"[_copy_files_from_user_to_server] {str(self.working_dir_user)=}\"\n    )\n\n    for prefix in prefixes:\n\n        if prefix == job.slurm_file_prefix:\n            files_to_copy = _glob_as_user(\n                folder=str(self.working_dir_user),\n                user=self.slurm_user,\n                startswith=prefix,\n            )\n        else:\n            files_to_copy = _glob_as_user_strict(\n                folder=str(self.working_dir_user),\n                user=self.slurm_user,\n                startswith=prefix,\n            )\n\n        logger.debug(\n            \"[_copy_files_from_user_to_server] \"\n            f\"{prefix=}, {len(files_to_copy)=}\"\n        )\n\n        for source_file_name in files_to_copy:\n            if \" \" in source_file_name:\n                raise ValueError(\n                    f'source_file_name=\"{source_file_name}\" '\n                    \"contains whitespaces\"\n                )\n            source_file_path = str(\n                self.working_dir_user / source_file_name\n            )\n\n            # Read source_file_path (requires sudo)\n            # NOTE: By setting encoding=None, we read/write bytes instead\n            # of strings; this is needed to also handle pickle files.\n            cmd = f\"cat {source_file_path}\"\n            res = _run_command_as_user(\n                cmd=cmd, user=self.slurm_user, encoding=None\n            )\n            if res.returncode != 0:\n                info = (\n                    f'Running cmd=\"{cmd}\" as {self.slurm_user=} failed\\n\\n'\n                    f\"{res.returncode=}\\n\\n\"\n                    f\"{res.stdout=}\\n\\n{res.stderr=}\\n\"\n                )\n                logger.error(info)\n                raise JobExecutionError(info)\n            # Write to dest_file_path (including empty files)\n            dest_file_path = str(self.working_dir / source_file_name)\n            with open(dest_file_path, \"wb\") as f:\n                f.write(res.stdout)\n    logger.debug(\"[_copy_files_from_user_to_server] End\")\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor._prepare_JobExecutionError","title":"<code>_prepare_JobExecutionError(jobid, info)</code>","text":"<p>Prepare the <code>JobExecutionError</code> for a given job</p> <p>This method creates a <code>JobExecutionError</code> object and sets its attribute to the appropriate SLURM-related file names. Note that the method shoul always be called after values in <code>self.map_jobid_to_slurm_files</code> have been updated, so that they point to <code>self.working_dir</code> files which are readable from <code>fractal-server</code>.</p> <p>Parameters:</p> Name Type Description Default <code>jobid</code> <code>str</code> <p>ID of the SLURM job.</p> required <code>info</code> <code>str</code> required Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def _prepare_JobExecutionError(\n    self, jobid: str, info: str\n) -&gt; JobExecutionError:\n    \"\"\"\n    Prepare the `JobExecutionError` for a given job\n\n    This method creates a `JobExecutionError` object and sets its attribute\n    to the appropriate SLURM-related file names. Note that the method shoul\n    always be called after values in `self.map_jobid_to_slurm_files` have\n    been updated, so that they point to `self.working_dir` files which are\n    readable from `fractal-server`.\n\n    Arguments:\n        jobid:\n            ID of the SLURM job.\n        info:\n    \"\"\"\n    # Extract SLURM file paths\n    with self.jobs_lock:\n        (\n            slurm_script_file,\n            slurm_stdout_file,\n            slurm_stderr_file,\n        ) = self.map_jobid_to_slurm_files[jobid]\n    # Construct JobExecutionError exception\n    job_exc = JobExecutionError(\n        cmd_file=slurm_script_file,\n        stdout_file=slurm_stdout_file,\n        stderr_file=slurm_stderr_file,\n        info=info,\n    )\n    return job_exc\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor._start","title":"<code>_start(job)</code>","text":"<p>Submit function for execution on a SLURM cluster</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def _start(\n    self,\n    job: SlurmJob,\n) -&gt; tuple[str, SlurmJob]:\n    \"\"\"\n    Submit function for execution on a SLURM cluster\n    \"\"\"\n\n    # Prepare commands to be included in SLURM submission script\n    settings = Inject(get_settings)\n    python_worker_interpreter = (\n        settings.FRACTAL_SLURM_WORKER_PYTHON or sys.executable\n    )\n\n    cmdlines = []\n    for ind_task in range(job.num_tasks_tot):\n        input_pickle_file = job.input_pickle_files[ind_task]\n        output_pickle_file = job.output_pickle_files[ind_task]\n        cmdlines.append(\n            (\n                f\"{python_worker_interpreter}\"\n                \" -m fractal_server.app.runner.executors.slurm.remote \"\n                f\"--input-file {input_pickle_file} \"\n                f\"--output-file {output_pickle_file}\"\n            )\n        )\n\n    # ...\n    sbatch_script = self._prepare_sbatch_script(\n        slurm_config=job.slurm_config,\n        list_commands=cmdlines,\n        slurm_out_path=str(job.slurm_stdout),\n        slurm_err_path=str(job.slurm_stderr),\n    )\n\n    # Submit job via sbatch, and retrieve jobid\n\n    # Write script content to a job.slurm_script\n    with job.slurm_script.open(\"w\") as f:\n        f.write(sbatch_script)\n\n    # Prepare submission command\n    pre_command = f\"sudo --non-interactive -u {self.slurm_user}\"\n    submit_command = f\"sbatch --parsable {job.slurm_script}\"\n    full_command = f\"{pre_command} {submit_command}\"\n\n    # Submit SLURM job and retrieve job ID\n    output = _subprocess_run_or_raise(full_command)\n    try:\n        jobid = int(output.stdout)\n    except ValueError as e:\n        error_msg = (\n            f\"Submit command `{full_command}` returned \"\n            f\"`{output.stdout=}` which cannot be cast to an integer \"\n            f\"SLURM-job ID. Original error:\\n{str(e)}\"\n        )\n        logger.error(error_msg)\n        raise JobExecutionError(info=error_msg)\n    jobid_str = str(jobid)\n\n    # Plug SLURM job id in stdout/stderr file paths\n    job.slurm_stdout = Path(\n        job.slurm_stdout.as_posix().replace(\"%j\", jobid_str)\n    )\n    job.slurm_stderr = Path(\n        job.slurm_stderr.as_posix().replace(\"%j\", jobid_str)\n    )\n\n    return jobid_str, job\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor._submit_job","title":"<code>_submit_job(fun, slurm_file_prefix, task_files, slurm_config, single_task_submission=False, args=None, kwargs=None, components=None)</code>","text":"<p>Submit a multi-task job to the pool, where each task is handled via the pickle/remote logic</p> <p>NOTE: this method has different behaviors when it is called from the <code>self.submit</code> or <code>self.map</code> methods (which is also encoded in <code>single_task_submission</code>):</p> <ul> <li>When called from <code>self.submit</code>, it supports general <code>args</code> and   <code>kwargs</code> arguments;</li> <li>When called from <code>self.map</code>, there cannot be any <code>args</code> or <code>kwargs</code>   argument, but there must be a <code>components</code> argument.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable[..., Any]</code> required <code>slurm_file_prefix</code> <code>str</code> required <code>task_files</code> <code>TaskFiles</code> required <code>slurm_config</code> <code>SlurmConfig</code> required <code>single_task_submission</code> <code>bool</code> <code>False</code> <code>args</code> <code>Optional[Sequence[Any]]</code> <code>None</code> <code>kwargs</code> <code>Optional[dict]</code> <code>None</code> <code>components</code> <code>Optional[list[Any]]</code> <code>None</code> <p>Returns:</p> Type Description <code>Future</code> <p>Future representing the execution of the current SLURM job.</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def _submit_job(\n    self,\n    fun: Callable[..., Any],\n    slurm_file_prefix: str,\n    task_files: TaskFiles,\n    slurm_config: SlurmConfig,\n    single_task_submission: bool = False,\n    args: Optional[Sequence[Any]] = None,\n    kwargs: Optional[dict] = None,\n    components: Optional[list[Any]] = None,\n) -&gt; Future:\n    \"\"\"\n    Submit a multi-task job to the pool, where each task is handled via the\n    pickle/remote logic\n\n    NOTE: this method has different behaviors when it is called from the\n    `self.submit` or `self.map` methods (which is also encoded in\n    `single_task_submission`):\n\n    * When called from `self.submit`, it supports general `args` and\n      `kwargs` arguments;\n    * When called from `self.map`, there cannot be any `args` or `kwargs`\n      argument, but there must be a `components` argument.\n\n    Arguments:\n        fun:\n        slurm_file_prefix:\n        task_files:\n        slurm_config:\n        single_task_submission:\n        args:\n        kwargs:\n        components:\n\n    Returns:\n        Future representing the execution of the current SLURM job.\n    \"\"\"\n    fut: Future = Future()\n\n    # Inject SLURM account (if set) into slurm_config\n    if self.slurm_account:\n        slurm_config.account = self.slurm_account\n\n    # Define slurm-job-related files\n    if single_task_submission:\n        if components is not None:\n            raise ValueError(\n                f\"{single_task_submission=} but components is not None\"\n            )\n        job = SlurmJob(\n            slurm_file_prefix=slurm_file_prefix,\n            num_tasks_tot=1,\n            slurm_config=slurm_config,\n        )\n        if job.num_tasks_tot &gt; 1:\n            raise ValueError(\n                \"{single_task_submission=} but {job.num_tasks_tot=}\"\n            )\n        job.single_task_submission = True\n        job.wftask_file_prefixes = (task_files.file_prefix,)\n    else:\n        if not components or len(components) &lt; 1:\n            raise ValueError(\n                \"In FractalSlurmExecutor._submit_job, given \"\n                f\"{components=}.\"\n            )\n        num_tasks_tot = len(components)\n        job = SlurmJob(\n            slurm_file_prefix=slurm_file_prefix,\n            num_tasks_tot=num_tasks_tot,\n            slurm_config=slurm_config,\n        )\n\n        _prefixes = []\n        for component in components:\n            if isinstance(component, dict):\n                # This is needed for V2\n                actual_component = component.get(_COMPONENT_KEY_, None)\n            else:\n                actual_component = component\n            _prefixes.append(\n                get_task_file_paths(\n                    workflow_dir=task_files.workflow_dir,\n                    workflow_dir_user=task_files.workflow_dir_user,\n                    task_order=task_files.task_order,\n                    component=actual_component,\n                ).file_prefix\n            )\n        job.wftask_file_prefixes = tuple(_prefixes)\n\n    # Define I/O pickle file names/paths\n    job.input_pickle_files = tuple(\n        self.get_input_pickle_file_path(\n            job.workerids[ind],\n            prefix=job.wftask_file_prefixes[ind],\n        )\n        for ind in range(job.num_tasks_tot)\n    )\n    job.output_pickle_files = tuple(\n        self.get_output_pickle_file_path(\n            job.workerids[ind],\n            prefix=job.wftask_file_prefixes[ind],\n        )\n        for ind in range(job.num_tasks_tot)\n    )\n\n    # Define SLURM-job file names/paths\n    job.slurm_script = self.get_slurm_script_file_path(\n        prefix=job.slurm_file_prefix\n    )\n    job.slurm_stdout = self.get_slurm_stdout_file_path(\n        prefix=job.slurm_file_prefix\n    )\n    job.slurm_stderr = self.get_slurm_stderr_file_path(\n        prefix=job.slurm_file_prefix\n    )\n\n    # Dump serialized versions+function+args+kwargs to pickle file\n    versions = dict(\n        python=sys.version_info[:3],\n        cloudpickle=cloudpickle.__version__,\n        fractal_server=__VERSION__,\n    )\n    if job.single_task_submission:\n        _args = args or []\n        _kwargs = kwargs or {}\n        funcser = cloudpickle.dumps((versions, fun, _args, _kwargs))\n        with open(job.input_pickle_files[0], \"wb\") as f:\n            f.write(funcser)\n    else:\n        for ind_component, component in enumerate(components):\n            _args = [component]\n            _kwargs = {}\n            funcser = cloudpickle.dumps((versions, fun, _args, _kwargs))\n            with open(job.input_pickle_files[ind_component], \"wb\") as f:\n                f.write(funcser)\n\n    # Submit job to SLURM, and get jobid\n    jobid, job = self._start(job)\n\n    # Add the SLURM script/out/err paths to map_jobid_to_slurm_files (this\n    # must be after self._start(job), so that \"%j\" has already been\n    # replaced with the job ID)\n    with self.jobs_lock:\n        self.map_jobid_to_slurm_files[jobid] = (\n            job.slurm_script.as_posix(),\n            job.slurm_stdout.as_posix(),\n            job.slurm_stderr.as_posix(),\n        )\n\n    # Thread will wait for it to finish.\n    self.wait_thread.wait(\n        filenames=job.get_clean_output_pickle_files(),\n        jobid=jobid,\n    )\n\n    with self.jobs_lock:\n        self.jobs[jobid] = (fut, job)\n    return fut\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor.get_default_task_files","title":"<code>get_default_task_files()</code>","text":"<p>This will be called when self.submit or self.map are called from outside fractal-server, and then lack some optional arguments.</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def get_default_task_files(self) -&gt; TaskFiles:\n    \"\"\"\n    This will be called when self.submit or self.map are called from\n    outside fractal-server, and then lack some optional arguments.\n    \"\"\"\n    import random\n\n    task_files = TaskFiles(\n        workflow_dir=self.working_dir,\n        workflow_dir_user=self.working_dir_user,\n        task_order=random.randint(10000, 99999),  # nosec\n    )\n    return task_files\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor.map","title":"<code>map(fn, iterable, *, slurm_config=None, task_files=None)</code>","text":"<p>Return an iterator with the results of several execution of a function</p> <p>This function is based on <code>concurrent.futures.Executor.map</code> from Python Standard Library 3.11. Original Copyright 2009 Brian Quinlan. All Rights Reserved. Licensed to PSF under a Contributor Agreement.</p> <p>Main modifications from the PSF function:</p> <ol> <li>Only <code>fn</code> and <code>iterable</code> can be assigned as positional arguments;</li> <li><code>*iterables</code> argument replaced with a single <code>iterable</code>;</li> <li><code>timeout</code> and <code>chunksize</code> arguments are not supported.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>The function to be executed</p> required <code>iterable</code> <code>list[Sequence[Any]]</code> <p>An iterable such that each element is the list of arguments to be passed to <code>fn</code>, as in <code>fn(*args)</code>.</p> required <code>slurm_config</code> <code>Optional[SlurmConfig]</code> <p>A <code>SlurmConfig</code> object; if <code>None</code>, use <code>get_default_slurm_config()</code>.</p> <code>None</code> <code>task_files</code> <code>Optional[TaskFiles]</code> <p>A <code>TaskFiles</code> object; if <code>None</code>, use <code>self.get_default_task_files()</code>.</p> <code>None</code> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def map(\n    self,\n    fn: Callable[..., Any],\n    iterable: list[Sequence[Any]],\n    *,\n    slurm_config: Optional[SlurmConfig] = None,\n    task_files: Optional[TaskFiles] = None,\n):\n    \"\"\"\n    Return an iterator with the results of several execution of a function\n\n    This function is based on `concurrent.futures.Executor.map` from Python\n    Standard Library 3.11.\n    Original Copyright 2009 Brian Quinlan. All Rights Reserved. Licensed to\n    PSF under a Contributor Agreement.\n\n    Main modifications from the PSF function:\n\n    1. Only `fn` and `iterable` can be assigned as positional arguments;\n    2. `*iterables` argument replaced with a single `iterable`;\n    3. `timeout` and `chunksize` arguments are not supported.\n\n    Arguments:\n        fn:\n            The function to be executed\n        iterable:\n            An iterable such that each element is the list of arguments to\n            be passed to `fn`, as in `fn(*args)`.\n        slurm_config:\n            A `SlurmConfig` object; if `None`, use\n            `get_default_slurm_config()`.\n        task_files:\n            A `TaskFiles` object; if `None`, use\n            `self.get_default_task_files()`.\n\n    \"\"\"\n\n    def _result_or_cancel(fut):\n        \"\"\"\n        This function is based on the Python Standard Library 3.11.\n        Original Copyright 2009 Brian Quinlan. All Rights Reserved.\n        Licensed to PSF under a Contributor Agreement.\n        \"\"\"\n        try:\n            try:\n                return fut.result()\n            finally:\n                fut.cancel()\n        finally:\n            # Break a reference cycle with the exception in\n            # self._exception\n            del fut\n\n    # Set defaults, if needed\n    if not slurm_config:\n        slurm_config = get_default_slurm_config()\n    if task_files is None:\n        task_files = self.get_default_task_files()\n\n    # Include common_script_lines in extra_lines\n    logger.debug(\n        f\"Adding {self.common_script_lines=} to \"\n        f\"{slurm_config.extra_lines=}, from map method.\"\n    )\n    current_extra_lines = slurm_config.extra_lines or []\n    slurm_config.extra_lines = (\n        current_extra_lines + self.common_script_lines\n    )\n\n    # Set file prefixes\n    general_slurm_file_prefix = str(task_files.task_order)\n\n    # Transform iterable into a list and count its elements\n    list_args = list(iterable)\n    tot_tasks = len(list_args)\n\n    # Set/validate parameters for task batching\n    tasks_per_job, parallel_tasks_per_job = heuristics(\n        # Number of parallel components (always known)\n        tot_tasks=len(list_args),\n        # Optional WorkflowTask attributes:\n        tasks_per_job=slurm_config.tasks_per_job,\n        parallel_tasks_per_job=slurm_config.parallel_tasks_per_job,  # noqa\n        # Task requirements (multiple possible sources):\n        cpus_per_task=slurm_config.cpus_per_task,\n        mem_per_task=slurm_config.mem_per_task_MB,\n        # Fractal configuration variables (soft/hard limits):\n        target_cpus_per_job=slurm_config.target_cpus_per_job,\n        target_mem_per_job=slurm_config.target_mem_per_job,\n        target_num_jobs=slurm_config.target_num_jobs,\n        max_cpus_per_job=slurm_config.max_cpus_per_job,\n        max_mem_per_job=slurm_config.max_mem_per_job,\n        max_num_jobs=slurm_config.max_num_jobs,\n    )\n    slurm_config.parallel_tasks_per_job = parallel_tasks_per_job\n    slurm_config.tasks_per_job = tasks_per_job\n\n    # Divide arguments in batches of `n_tasks_per_script` tasks each\n    args_batches = []\n    batch_size = tasks_per_job\n    for ind_chunk in range(0, tot_tasks, batch_size):\n        args_batches.append(\n            list_args[ind_chunk : ind_chunk + batch_size]  # noqa\n        )\n    if len(args_batches) != math.ceil(tot_tasks / tasks_per_job):\n        raise RuntimeError(\"Something wrong here while batching tasks\")\n\n    # Construct list of futures (one per SLURM job, i.e. one per batch)\n    fs = []\n    current_component_index = 0\n    for ind_batch, batch in enumerate(args_batches):\n        batch_size = len(batch)\n        this_slurm_file_prefix = (\n            f\"{general_slurm_file_prefix}_batch_{ind_batch:06d}\"\n        )\n        fs.append(\n            self._submit_job(\n                fn,\n                slurm_config=slurm_config,\n                slurm_file_prefix=this_slurm_file_prefix,\n                task_files=task_files,\n                single_task_submission=False,\n                components=batch,\n            )\n        )\n        current_component_index += batch_size\n\n    # Yield must be hidden in closure so that the futures are submitted\n    # before the first iterator value is required.\n    # NOTE: In this custom map() method, _result_or_cancel(fs.pop()) is an\n    # iterable of results (if successful), and we should yield its elements\n    # rather than the whole iterable.\n    def result_iterator():\n        \"\"\"\n        This function is based on the Python Standard Library 3.11.\n        Original Copyright 2009 Brian Quinlan. All Rights Reserved.\n        Licensed to PSF under a Contributor Agreement.\n        \"\"\"\n        try:\n            # reverse to keep finishing order\n            fs.reverse()\n            while fs:\n                # Careful not to keep a reference to the popped future\n                results = _result_or_cancel(fs.pop())\n                for res in results:\n                    yield res\n        finally:\n            for future in fs:\n                future.cancel()\n\n    return result_iterator()\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor.shutdown","title":"<code>shutdown(wait=True, *, cancel_futures=False)</code>","text":"<p>Clean up all executor variables. Note that this function is executed on the self.wait_thread thread, see _completion.</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def shutdown(self, wait=True, *, cancel_futures=False):\n    \"\"\"\n    Clean up all executor variables. Note that this function is executed on\n    the self.wait_thread thread, see _completion.\n    \"\"\"\n\n    logger.debug(\"Executor shutdown: start\")\n\n    # Handle all job futures\n    slurm_jobs_to_scancel = []\n    with self.jobs_lock:\n        while self.jobs:\n            jobid, fut_and_job = self.jobs.popitem()\n            slurm_jobs_to_scancel.append(jobid)\n            fut, job = fut_and_job[:]\n            self.map_jobid_to_slurm_files.pop(jobid)\n            if not fut.cancelled():\n                fut.set_exception(\n                    JobExecutionError(\n                        \"Job cancelled due to executor shutdown.\"\n                    )\n                )\n                fut.cancel()\n\n    # Cancel SLURM jobs\n    if slurm_jobs_to_scancel:\n        scancel_string = \" \".join(slurm_jobs_to_scancel)\n        logger.warning(f\"Now scancel-ing SLURM jobs {scancel_string}\")\n        pre_command = f\"sudo --non-interactive -u {self.slurm_user}\"\n        submit_command = f\"scancel {scancel_string}\"\n        full_command = f\"{pre_command} {submit_command}\"\n        logger.debug(f\"Now execute `{full_command}`\")\n        try:\n            subprocess.run(  # nosec\n                shlex.split(full_command),\n                capture_output=True,\n                check=True,\n                encoding=\"utf-8\",\n            )\n        except subprocess.CalledProcessError as e:\n            error_msg = (\n                f\"Cancel command `{full_command}` failed. \"\n                f\"Original error:\\n{str(e)}\"\n            )\n            logger.error(error_msg)\n            raise JobExecutionError(info=error_msg)\n\n    logger.debug(\"Executor shutdown: end\")\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.FractalSlurmExecutor.submit","title":"<code>submit(fun, *fun_args, slurm_config=None, task_files=None, **fun_kwargs)</code>","text":"<p>Submit a function for execution on <code>FractalSlurmExecutor</code></p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable[..., Any]</code> <p>The function to be executed</p> required <code>fun_args</code> <code>Sequence[Any]</code> <p>Function positional arguments</p> <code>()</code> <code>fun_kwargs</code> <code>dict</code> <p>Function keyword arguments</p> <code>{}</code> <code>slurm_config</code> <code>Optional[SlurmConfig]</code> <p>A <code>SlurmConfig</code> object; if <code>None</code>, use <code>get_default_slurm_config()</code>.</p> <code>None</code> <code>task_files</code> <code>Optional[TaskFiles]</code> <p>A <code>TaskFiles</code> object; if <code>None</code>, use <code>self.get_default_task_files()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Future</code> <p>Future representing the execution of the current SLURM job.</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def submit(\n    self,\n    fun: Callable[..., Any],\n    *fun_args: Sequence[Any],\n    slurm_config: Optional[SlurmConfig] = None,\n    task_files: Optional[TaskFiles] = None,\n    **fun_kwargs: dict,\n) -&gt; Future:\n    \"\"\"\n    Submit a function for execution on `FractalSlurmExecutor`\n\n    Arguments:\n        fun: The function to be executed\n        fun_args: Function positional arguments\n        fun_kwargs: Function keyword arguments\n        slurm_config:\n            A `SlurmConfig` object; if `None`, use\n            `get_default_slurm_config()`.\n        task_files:\n            A `TaskFiles` object; if `None`, use\n            `self.get_default_task_files()`.\n\n    Returns:\n        Future representing the execution of the current SLURM job.\n    \"\"\"\n\n    # Set defaults, if needed\n    if slurm_config is None:\n        slurm_config = get_default_slurm_config()\n    if task_files is None:\n        task_files = self.get_default_task_files()\n\n    # Set slurm_file_prefix\n    slurm_file_prefix = task_files.file_prefix\n\n    # Include common_script_lines in extra_lines\n    logger.debug(\n        f\"Adding {self.common_script_lines=} to \"\n        f\"{slurm_config.extra_lines=}, from submit method.\"\n    )\n    current_extra_lines = slurm_config.extra_lines or []\n    slurm_config.extra_lines = (\n        current_extra_lines + self.common_script_lines\n    )\n\n    # Adapt slurm_config to the fact that this is a single-task SlurmJob\n    # instance\n    slurm_config.tasks_per_job = 1\n    slurm_config.parallel_tasks_per_job = 1\n\n    fut = self._submit_job(\n        fun,\n        slurm_config=slurm_config,\n        slurm_file_prefix=slurm_file_prefix,\n        task_files=task_files,\n        single_task_submission=True,\n        args=fun_args,\n        kwargs=fun_kwargs,\n    )\n    return fut\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.SlurmJob","title":"<code>SlurmJob</code>","text":"<p>Collect information related to a FractalSlurmExecutor job</p> <p>This includes three groups of attributes:</p> <ol> <li>Attributes related to the (possibly multi-task) SLURM job, e.g.    submission-file path.</li> <li>Attributes related to single tasks, e.g. the paths of their input/output    pickle files.</li> <li>SLURM configuration options, encoded in a SlurmConfig object.</li> </ol> <p>Note: A SlurmJob object is generally defined as a multi-task job. Jobs coming from the <code>map</code> method must have <code>single_task_submission=False</code> (even if <code>num_tasks_tot=1</code>), while jobs coming from <code>submit</code> must have it set to <code>True</code>.</p> <p>Attributes:</p> Name Type Description <code>num_tasks_tot</code> <code>int</code> <p>Total number of tasks to be executed as part of this SLURM job.</p> <code>single_task_submission</code> <code>bool</code> <p>This must be <code>True</code> for jobs submitted as part of the <code>submit</code> method, and <code>False</code> for jobs coming from the <code>map</code> method.</p> <code>slurm_file_prefix</code> <code>str</code> <p>Prefix for SLURM-job related files (submission script and SLURM stdout/stderr); this is also needed in the <code>_copy_files_from_user_to_server</code> method.</p> <code>wftask_file_prefixes</code> <code>tuple[str, ...]</code> <p>Prefix for files that are created as part of the functions submitted for execution on the <code>FractalSlurmExecutor</code>; this is needed in the <code>_copy_files_from_user_to_server</code> method, and also to construct the names of per-task input/output pickle files.</p> <code>slurm_script</code> <code>Path</code> <p>Path of SLURM submission script.</p> <code>slurm_stdout</code> <code>Path</code> <p>Path of SLURM stdout file; if this includes <code>\"%j\"</code>, then this string will be replaced by the SLURM job ID upon <code>sbatch</code> submission.</p> <code>slurm_stderr</code> <code>Path</code> <p>Path of SLURM stderr file; see <code>slurm_stdout</code> concerning <code>\"%j\"</code>.</p> <code>workerids</code> <code>tuple[str, ...]</code> <p>IDs that enter in the per-task input/output pickle files (one per task).</p> <code>input_pickle_files</code> <code>tuple[Path, ...]</code> <p>Input pickle files (one per task).</p> <code>output_pickle_files</code> <code>tuple[Path, ...]</code> <p>Output pickle files (one per task).</p> <code>slurm_config</code> <code>SlurmConfig</code> <p><code>SlurmConfig</code> object.</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>class SlurmJob:\n    \"\"\"\n    Collect information related to a FractalSlurmExecutor job\n\n    This includes three groups of attributes:\n\n    1. Attributes related to the (possibly multi-task) SLURM job, e.g.\n       submission-file path.\n    2. Attributes related to single tasks, e.g. the paths of their input/output\n       pickle files.\n    3. SLURM configuration options, encoded in a SlurmConfig object.\n\n    Note: A SlurmJob object is generally defined as a multi-task job. Jobs\n    coming from the `map` method must have `single_task_submission=False` (even\n    if `num_tasks_tot=1`), while jobs coming from `submit` must have it set to\n    `True`.\n\n    Attributes:\n        num_tasks_tot:\n            Total number of tasks to be executed as part of this SLURM job.\n        single_task_submission:\n            This must be `True` for jobs submitted as part of the `submit`\n            method, and `False` for jobs coming from the `map` method.\n        slurm_file_prefix:\n            Prefix for SLURM-job related files (submission script and SLURM\n            stdout/stderr); this is also needed in the\n            `_copy_files_from_user_to_server` method.\n        wftask_file_prefixes:\n            Prefix for files that are created as part of the functions\n            submitted for execution on the `FractalSlurmExecutor`; this is\n            needed in the `_copy_files_from_user_to_server` method, and also to\n            construct the names of per-task input/output pickle files.\n        slurm_script:\n            Path of SLURM submission script.\n        slurm_stdout:\n            Path of SLURM stdout file; if this includes `\"%j\"`, then this\n            string will be replaced by the SLURM job ID upon `sbatch`\n            submission.\n        slurm_stderr:\n            Path of SLURM stderr file; see `slurm_stdout` concerning `\"%j\"`.\n        workerids:\n            IDs that enter in the per-task input/output pickle files (one per\n            task).\n        input_pickle_files:\n            Input pickle files (one per task).\n        output_pickle_files:\n            Output pickle files (one per task).\n        slurm_config:\n            `SlurmConfig` object.\n    \"\"\"\n\n    # Job-related attributes\n    num_tasks_tot: int\n    single_task_submission: bool\n    slurm_file_prefix: str\n    slurm_script: Path\n    slurm_stdout: Path\n    slurm_stderr: Path\n    # Per-task attributes\n    workerids: tuple[str, ...]\n    wftask_file_prefixes: tuple[str, ...]\n    input_pickle_files: tuple[Path, ...]\n    output_pickle_files: tuple[Path, ...]\n    # Slurm configuration\n    slurm_config: SlurmConfig\n\n    def __init__(\n        self,\n        num_tasks_tot: int,\n        slurm_config: SlurmConfig,\n        workflow_task_file_prefix: Optional[str] = None,\n        slurm_file_prefix: Optional[str] = None,\n        wftask_file_prefixes: Optional[tuple[str, ...]] = None,\n        single_task_submission: bool = False,\n    ):\n        if single_task_submission and num_tasks_tot &gt; 1:\n            raise ValueError(\n                \"Trying to initialize SlurmJob with\"\n                f\"{single_task_submission=} and {num_tasks_tot=}.\"\n            )\n        self.num_tasks_tot = num_tasks_tot\n        self.single_task_submission = single_task_submission\n        self.slurm_file_prefix = slurm_file_prefix or \"default_slurm_prefix\"\n        if wftask_file_prefixes is None:\n            self.wftask_file_prefixes = tuple(\n                \"default_wftask_prefix\" for i in range(self.num_tasks_tot)\n            )\n        else:\n            self.wftask_file_prefixes = wftask_file_prefixes\n        self.workerids = tuple(\n            random_string() for i in range(self.num_tasks_tot)\n        )\n        self.slurm_config = slurm_config\n\n    def get_clean_output_pickle_files(self) -&gt; tuple[str, ...]:\n        \"\"\"\n        Transform all pathlib.Path objects in self.output_pickle_files to\n        strings\n        \"\"\"\n        return tuple(str(f.as_posix()) for f in self.output_pickle_files)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor.SlurmJob.get_clean_output_pickle_files","title":"<code>get_clean_output_pickle_files()</code>","text":"<p>Transform all pathlib.Path objects in self.output_pickle_files to strings</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def get_clean_output_pickle_files(self) -&gt; tuple[str, ...]:\n    \"\"\"\n    Transform all pathlib.Path objects in self.output_pickle_files to\n    strings\n    \"\"\"\n    return tuple(str(f.as_posix()) for f in self.output_pickle_files)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/executor/#fractal_server.app.runner.executors.slurm.executor._subprocess_run_or_raise","title":"<code>_subprocess_run_or_raise(full_command)</code>","text":"<p>Wrap <code>subprocess.run</code> and raise  appropriate <code>JobExecutionError</code> if needed.</p> <p>Parameters:</p> Name Type Description Default <code>full_command</code> <code>str</code> <p>Full string of the command to execute.</p> required <p>Raises:</p> Type Description <code>JobExecutionError</code> <p>If <code>subprocess.run</code> raises a <code>CalledProcessError</code>.</p> <p>Returns:</p> Type Description <code>Optional[CompletedProcess]</code> <p>The actual <code>CompletedProcess</code> output of <code>subprocess.run</code>.</p> Source code in <code>fractal_server/app/runner/executors/slurm/executor.py</code> <pre><code>def _subprocess_run_or_raise(full_command: str) -&gt; Optional[CompletedProcess]:\n    \"\"\"\n    Wrap `subprocess.run` and raise  appropriate `JobExecutionError` if needed.\n\n    Args:\n        full_command: Full string of the command to execute.\n\n    Raises:\n        JobExecutionError: If `subprocess.run` raises a `CalledProcessError`.\n\n    Returns:\n        The actual `CompletedProcess` output of `subprocess.run`.\n    \"\"\"\n    try:\n        output = subprocess.run(  # nosec\n            shlex.split(full_command),\n            capture_output=True,\n            check=True,\n            encoding=\"utf-8\",\n        )\n        return output\n    except subprocess.CalledProcessError as e:\n        error_msg = (\n            f\"Submit command `{full_command}` failed. \"\n            f\"Original error:\\n{str(e)}\\n\"\n            f\"Original stdout:\\n{e.stdout}\\n\"\n            f\"Original stderr:\\n{e.stderr}\\n\"\n        )\n        logger.error(error_msg)\n        raise JobExecutionError(info=error_msg)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/remote/","title":"remote","text":"<p>This module provides a simple self-standing script that executes arbitrary python code received via pickled files on a cluster node.</p>"},{"location":"reference/fractal_server/app/runner/executors/slurm/remote/#fractal_server.app.runner.executors.slurm.remote.ExceptionProxy","title":"<code>ExceptionProxy</code>","text":"<p>Proxy class to serialise exceptions</p> <p>In general exceptions are not serialisable. This proxy class saves the serialisable content of an exception. On the receiving end, it can be used to reconstruct a TaskExecutionError.</p> <p>Attributes:</p> Name Type Description <code>exc_type_name</code> <code>str</code> <p>Name of the exception type</p> <code>tb</code> <code>str</code> <p>TBD</p> <code>args</code> <p>TBD</p> <code>kwargs</code> <code>dict</code> <p>TBD</p> Source code in <code>fractal_server/app/runner/executors/slurm/remote.py</code> <pre><code>class ExceptionProxy:\n    \"\"\"\n    Proxy class to serialise exceptions\n\n    In general exceptions are not serialisable. This proxy class saves the\n    serialisable content of an exception. On the receiving end, it can be used\n    to reconstruct a TaskExecutionError.\n\n    Attributes:\n        exc_type_name: Name of the exception type\n        tb: TBD\n        args: TBD\n        kwargs: TBD\n    \"\"\"\n\n    def __init__(\n        self, exc_type: Type[BaseException], tb: str, *args, **kwargs\n    ):\n        self.exc_type_name: str = exc_type.__name__\n        self.tb: str = tb\n        self.args = args\n        self.kwargs: dict = kwargs\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/remote/#fractal_server.app.runner.executors.slurm.remote.FractalVersionMismatch","title":"<code>FractalVersionMismatch</code>","text":"<p>             Bases: <code>RuntimeError</code></p> <p>Custom exception for version mismatch</p> Source code in <code>fractal_server/app/runner/executors/slurm/remote.py</code> <pre><code>class FractalVersionMismatch(RuntimeError):\n    \"\"\"\n    Custom exception for version mismatch\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/remote/#fractal_server.app.runner.executors.slurm.remote._check_versions_mismatch","title":"<code>_check_versions_mismatch(server_versions)</code>","text":"<p>Compare the server {python,cloudpickle,fractal_server} versions with the ones available to the current worker</p> <p>Parameters:</p> Name Type Description Default <code>server_versions</code> <code>dict[Literal['python', 'fractal_server', 'cloudpickle'], Union[str, tuple[int]]]</code> <p>The version used in the fractal-server instance that created the cloudpickle file</p> required <p>Raises:</p> Type Description <code>FractalVersionMismatch</code> <p>If the cloudpickle or fractal_server versions                     do not match with the ones on the server</p> Source code in <code>fractal_server/app/runner/executors/slurm/remote.py</code> <pre><code>def _check_versions_mismatch(\n    server_versions: dict[\n        Literal[\"python\", \"fractal_server\", \"cloudpickle\"],\n        Union[str, tuple[int]],\n    ]\n):\n    \"\"\"\n    Compare the server {python,cloudpickle,fractal_server} versions with the\n    ones available to the current worker\n\n    Arguments:\n        server_versions:\n            The version used in the fractal-server instance that created the\n            cloudpickle file\n\n    Raises:\n        FractalVersionMismatch: If the cloudpickle or fractal_server versions\n                                do not match with the ones on the server\n    \"\"\"\n\n    server_python_version = server_versions[\"python\"]\n    worker_python_version = sys.version_info[:3]\n    if worker_python_version != server_python_version:\n        # FIXME: turn this into an error, after fixing a broader CI issue, see\n        # https://github.com/fractal-analytics-platform/fractal-server/issues/375\n        logging.critical(\n            f\"{server_python_version=} but {worker_python_version=}. \"\n            \"cloudpickle is not guaranteed to correctly load \"\n            \"pickle files created with different python versions. \"\n            \"Note, however, that if you reached this line it means that \"\n            \"the pickle file was likely loaded correctly.\"\n        )\n\n    server_cloudpickle_version = server_versions[\"cloudpickle\"]\n    worker_cloudpickle_version = cloudpickle.__version__\n    if worker_cloudpickle_version != server_cloudpickle_version:\n        raise FractalVersionMismatch(\n            f\"{server_cloudpickle_version=} but \"\n            f\"{worker_cloudpickle_version=}\"\n        )\n\n    server_fractal_server_version = server_versions[\"fractal_server\"]\n    worker_fractal_server_version = __VERSION__\n    if worker_fractal_server_version != server_fractal_server_version:\n        raise FractalVersionMismatch(\n            f\"{server_fractal_server_version=} but \"\n            f\"{worker_fractal_server_version=}\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/runner/executors/slurm/remote/#fractal_server.app.runner.executors.slurm.remote.worker","title":"<code>worker(*, in_fname, out_fname, extra_import_paths=None)</code>","text":"<p>Execute a job, possibly on a remote node.</p> <p>Parameters:</p> Name Type Description Default <code>in_fname</code> <code>str</code> <p>Absolute path to the input pickle file (must be readable).</p> required <code>out_fname</code> <code>str</code> <p>Absolute path of the output pickle file (must be writeable).</p> required <code>extra_import_paths</code> <code>Optional[str]</code> <p>Additional import paths</p> <code>None</code> Source code in <code>fractal_server/app/runner/executors/slurm/remote.py</code> <pre><code>def worker(\n    *,\n    in_fname: str,\n    out_fname: str,\n    extra_import_paths: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Execute a job, possibly on a remote node.\n\n    Arguments:\n        in_fname: Absolute path to the input pickle file (must be readable).\n        out_fname: Absolute path of the output pickle file (must be writeable).\n        extra_import_paths: Additional import paths\n    \"\"\"\n\n    # Create output folder, if missing\n    out_dir = os.path.dirname(out_fname)\n    if not os.path.exists(out_dir):\n        logging.debug(f\"_slurm.remote.worker: create {out_dir=}\")\n        os.mkdir(out_dir)\n\n    if extra_import_paths:\n        _extra_import_paths = extra_import_paths.split(\":\")\n        sys.path[:0] = _extra_import_paths\n\n    # Execute the job and catpure exceptions\n    try:\n        with open(in_fname, \"rb\") as f:\n            indata = f.read()\n        server_versions, fun, args, kwargs = cloudpickle.loads(indata)\n        _check_versions_mismatch(server_versions)\n\n        result = True, fun(*args, **kwargs)\n        out = cloudpickle.dumps(result)\n    except Exception as e:\n        import traceback\n\n        typ, value, tb = sys.exc_info()\n        tb = tb.tb_next\n        exc_proxy = ExceptionProxy(\n            typ,\n            \"\".join(traceback.format_exception(typ, value, tb)),\n            *e.args,\n            **e.__dict__,\n        )\n\n        result = False, exc_proxy\n        out = cloudpickle.dumps(result)\n\n    # Write the output pickle file\n    tempfile = out_fname + \".tmp\"\n    with open(tempfile, \"wb\") as f:\n        f.write(out)\n    os.rename(tempfile, out_fname)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/","title":"v1","text":"<p>Runner backend subsystem root</p> <p>This module is the single entry point to the runner backend subsystem. Other subystems should only import this module and not its submodules or the individual backends.</p>"},{"location":"reference/fractal_server/app/runner/v1/#fractal_server.app.runner.v1.submit_workflow","title":"<code>submit_workflow(*, workflow_id, input_dataset_id, output_dataset_id, job_id, worker_init=None, slurm_user=None, user_cache_dir=None)</code>  <code>async</code>","text":"<p>Prepares a workflow and applies it to a dataset</p> <p>This function wraps the process_workflow one, which is different for each backend (e.g. local or slurm backend).</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>int</code> <p>ID of the workflow being applied</p> required <code>input_dataset_id</code> <code>int</code> <p>Input dataset ID</p> required <code>output_dataset_id</code> <code>int</code> <p>ID of the destination dataset of the workflow.</p> required <code>job_id</code> <code>int</code> <p>Id of the job record which stores the state for the current workflow application.</p> required <code>worker_init</code> <code>Optional[str]</code> <p>Custom executor parameters that get parsed before the execution of each task.</p> <code>None</code> <code>user_cache_dir</code> <code>Optional[str]</code> <p>Cache directory (namely a path where the user can write); for the slurm backend, this is used as a base directory for <code>job.working_dir_user</code>.</p> <code>None</code> <code>slurm_user</code> <code>Optional[str]</code> <p>The username to impersonate for the workflow execution, for the slurm backend.</p> <code>None</code> Source code in <code>fractal_server/app/runner/v1/__init__.py</code> <pre><code>async def submit_workflow(\n    *,\n    workflow_id: int,\n    input_dataset_id: int,\n    output_dataset_id: int,\n    job_id: int,\n    worker_init: Optional[str] = None,\n    slurm_user: Optional[str] = None,\n    user_cache_dir: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Prepares a workflow and applies it to a dataset\n\n    This function wraps the process_workflow one, which is different for each\n    backend (e.g. local or slurm backend).\n\n    Args:\n        workflow_id:\n            ID of the workflow being applied\n        input_dataset_id:\n            Input dataset ID\n        output_dataset_id:\n            ID of the destination dataset of the workflow.\n        job_id:\n            Id of the job record which stores the state for the current\n            workflow application.\n        worker_init:\n            Custom executor parameters that get parsed before the execution of\n            each task.\n        user_cache_dir:\n            Cache directory (namely a path where the user can write); for the\n            slurm backend, this is used as a base directory for\n            `job.working_dir_user`.\n        slurm_user:\n            The username to impersonate for the workflow execution, for the\n            slurm backend.\n    \"\"\"\n\n    # Declare runner backend and set `process_workflow` function\n    settings = Inject(get_settings)\n    FRACTAL_RUNNER_BACKEND = settings.FRACTAL_RUNNER_BACKEND\n    if FRACTAL_RUNNER_BACKEND == \"local\":\n        process_workflow = local_process_workflow\n    elif FRACTAL_RUNNER_BACKEND == \"slurm\":\n        process_workflow = slurm_process_workflow\n    else:\n        raise RuntimeError(f\"Invalid runner backend {FRACTAL_RUNNER_BACKEND=}\")\n\n    with next(DB.get_sync_db()) as db_sync:\n\n        job: ApplyWorkflow = db_sync.get(ApplyWorkflow, job_id)\n        if not job:\n            raise ValueError(f\"Cannot fetch job {job_id} from database\")\n\n        input_dataset: Dataset = db_sync.get(Dataset, input_dataset_id)\n        output_dataset: Dataset = db_sync.get(Dataset, output_dataset_id)\n        workflow: Workflow = db_sync.get(Workflow, workflow_id)\n        if not (input_dataset and output_dataset and workflow):\n            log_msg = \"\"\n            if not input_dataset:\n                log_msg += (\n                    f\"Cannot fetch input_dataset {input_dataset_id} \"\n                    \"from database\\n\"\n                )\n            if not output_dataset:\n                log_msg += (\n                    f\"Cannot fetch output_dataset {output_dataset_id} \"\n                    \"from database\\n\"\n                )\n            if not workflow:\n                log_msg += (\n                    f\"Cannot fetch workflow {workflow_id} from database\\n\"\n                )\n            job.status = JobStatusTypeV1.FAILED\n            job.end_timestamp = get_timestamp()\n            job.log = log_msg\n            db_sync.merge(job)\n            db_sync.commit()\n            db_sync.close()\n            return\n\n        # Prepare some of process_workflow arguments\n        input_paths = input_dataset.paths\n        output_path = output_dataset.paths[0]\n\n        # Define and create server-side working folder\n        project_id = workflow.project_id\n        timestamp_string = get_timestamp().strftime(\"%Y%m%d_%H%M%S\")\n        WORKFLOW_DIR = (\n            settings.FRACTAL_RUNNER_WORKING_BASE_DIR\n            / (\n                f\"proj_{project_id:07d}_wf_{workflow_id:07d}_job_{job_id:07d}\"\n                f\"_{timestamp_string}\"\n            )\n        ).resolve()\n\n        if WORKFLOW_DIR.exists():\n            raise RuntimeError(f\"Workflow dir {WORKFLOW_DIR} already exists.\")\n\n        # Create WORKFLOW_DIR with 755 permissions\n        original_umask = os.umask(0)\n        WORKFLOW_DIR.mkdir(parents=True, mode=0o755)\n        os.umask(original_umask)\n\n        # Define and create user-side working folder, if needed\n        if FRACTAL_RUNNER_BACKEND == \"local\":\n            WORKFLOW_DIR_USER = WORKFLOW_DIR\n        elif FRACTAL_RUNNER_BACKEND == \"slurm\":\n\n            from ..executors.slurm._subprocess_run_as_user import (\n                _mkdir_as_user,\n            )\n\n            WORKFLOW_DIR_USER = (\n                Path(user_cache_dir) / f\"{WORKFLOW_DIR.name}\"\n            ).resolve()\n            _mkdir_as_user(folder=str(WORKFLOW_DIR_USER), user=slurm_user)\n        else:\n            raise ValueError(f\"{FRACTAL_RUNNER_BACKEND=} not supported\")\n\n        # Update db\n        job.working_dir = WORKFLOW_DIR.as_posix()\n        job.working_dir_user = WORKFLOW_DIR_USER.as_posix()\n        db_sync.merge(job)\n        db_sync.commit()\n\n        # After Session.commit() is called, either explicitly or when using a\n        # context manager, all objects associated with the Session are expired.\n        # https://docs.sqlalchemy.org/en/14/orm/\n        #   session_basics.html#opening-and-closing-a-session\n        # https://docs.sqlalchemy.org/en/14/orm/\n        #   session_state_management.html#refreshing-expiring\n\n        # See issue #928:\n        # https://github.com/fractal-analytics-platform/\n        #   fractal-server/issues/928\n\n        db_sync.refresh(input_dataset)\n        db_sync.refresh(output_dataset)\n        db_sync.refresh(workflow)\n\n        # Write logs\n        logger_name = f\"WF{workflow_id}_job{job_id}\"\n        log_file_path = WORKFLOW_DIR / WORKFLOW_LOG_FILENAME\n        logger = set_logger(\n            logger_name=logger_name,\n            log_file_path=log_file_path,\n        )\n        logger.info(\n            f'Start execution of workflow \"{workflow.name}\"; '\n            f\"more logs at {str(log_file_path)}\"\n        )\n        logger.debug(f\"fractal_server.__VERSION__: {__VERSION__}\")\n        logger.debug(f\"FRACTAL_RUNNER_BACKEND: {FRACTAL_RUNNER_BACKEND}\")\n        logger.debug(f\"slurm_user: {slurm_user}\")\n        logger.debug(f\"slurm_account: {job.slurm_account}\")\n        logger.debug(f\"worker_init: {worker_init}\")\n        logger.debug(f\"input metadata keys: {list(input_dataset.meta.keys())}\")\n        logger.debug(f\"input_paths: {input_paths}\")\n        logger.debug(f\"output_path: {output_path}\")\n        logger.debug(f\"job.id: {job.id}\")\n        logger.debug(f\"job.working_dir: {job.working_dir}\")\n        logger.debug(f\"job.working_dir_user: {job.working_dir_user}\")\n        logger.debug(f\"job.first_task_index: {job.first_task_index}\")\n        logger.debug(f\"job.last_task_index: {job.last_task_index}\")\n        logger.debug(f'START workflow \"{workflow.name}\"')\n\n    try:\n        # \"The Session.close() method does not prevent the Session from being\n        # used again. The Session itself does not actually have a distinct\n        # \u201cclosed\u201d state; it merely means the Session will release all database\n        # connections and ORM objects.\"\n        # (https://docs.sqlalchemy.org/en/20/orm/session_api.html#sqlalchemy.orm.Session.close).\n        #\n        # We close the session before the (possibly long) process_workflow\n        # call, to make sure all DB connections are released. The reason why we\n        # are not using a context manager within the try block is that we also\n        # need access to db_sync in the except branches.\n        db_sync = next(DB.get_sync_db())\n        db_sync.close()\n\n        output_dataset_meta_hist = await process_workflow(\n            workflow=workflow,\n            input_paths=input_paths,\n            output_path=output_path,\n            input_metadata=input_dataset.meta,\n            input_history=input_dataset.history,\n            slurm_user=slurm_user,\n            slurm_account=job.slurm_account,\n            user_cache_dir=user_cache_dir,\n            workflow_dir=WORKFLOW_DIR,\n            workflow_dir_user=WORKFLOW_DIR_USER,\n            logger_name=logger_name,\n            worker_init=worker_init,\n            first_task_index=job.first_task_index,\n            last_task_index=job.last_task_index,\n        )\n\n        logger.info(\n            f'End execution of workflow \"{workflow.name}\"; '\n            f\"more logs at {str(log_file_path)}\"\n        )\n        logger.debug(f'END workflow \"{workflow.name}\"')\n\n        # Replace output_dataset.meta and output_dataset.history with their\n        # up-to-date versions, obtained within process_workflow\n        output_dataset.history = output_dataset_meta_hist.pop(\"history\")\n        output_dataset.meta = output_dataset_meta_hist.pop(\"metadata\")\n\n        db_sync.merge(output_dataset)\n\n        # Update job DB entry\n        job.status = JobStatusTypeV1.DONE\n        job.end_timestamp = get_timestamp()\n        with log_file_path.open(\"r\") as f:\n            logs = f.read()\n        job.log = logs\n        db_sync.merge(job)\n        close_job_logger(logger)\n        db_sync.commit()\n\n    except TaskExecutionError as e:\n\n        logger.debug(f'FAILED workflow \"{workflow.name}\", TaskExecutionError.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (TaskExecutionError).')\n\n        # Assemble output_dataset.meta based on the last successful task, i.e.\n        # based on METADATA_FILENAME\n        output_dataset.meta = assemble_meta_failed_job(job, output_dataset)\n\n        # Assemble new history and assign it to output_dataset.meta\n        failed_wftask = db_sync.get(WorkflowTask, e.workflow_task_id)\n        output_dataset.history = assemble_history_failed_job(\n            job,\n            output_dataset,\n            workflow,\n            logger,\n            failed_wftask=failed_wftask,\n        )\n\n        db_sync.merge(output_dataset)\n\n        job.status = JobStatusTypeV1.FAILED\n        job.end_timestamp = get_timestamp()\n\n        exception_args_string = \"\\n\".join(e.args)\n        job.log = (\n            f\"TASK ERROR: \"\n            f\"Task name: {e.task_name}, \"\n            f\"position in Workflow: {e.workflow_task_order}\\n\"\n            f\"TRACEBACK:\\n{exception_args_string}\"\n        )\n        db_sync.merge(job)\n        close_job_logger(logger)\n        db_sync.commit()\n\n    except JobExecutionError as e:\n\n        logger.debug(f'FAILED workflow \"{workflow.name}\", JobExecutionError.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (JobExecutionError).')\n\n        # Assemble output_dataset.meta based on the last successful task, i.e.\n        # based on METADATA_FILENAME\n        output_dataset.meta = assemble_meta_failed_job(job, output_dataset)\n\n        # Assemble new history and assign it to output_dataset.meta\n        output_dataset.history = assemble_history_failed_job(\n            job,\n            output_dataset,\n            workflow,\n            logger,\n        )\n\n        db_sync.merge(output_dataset)\n\n        job.status = JobStatusTypeV1.FAILED\n        job.end_timestamp = get_timestamp()\n        error = e.assemble_error()\n        job.log = f\"JOB ERROR in Fractal job {job.id}:\\nTRACEBACK:\\n{error}\"\n        db_sync.merge(job)\n        close_job_logger(logger)\n        db_sync.commit()\n\n    except Exception:\n\n        logger.debug(f'FAILED workflow \"{workflow.name}\", unknown error.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (unkwnon error).')\n\n        current_traceback = traceback.format_exc()\n\n        # Assemble output_dataset.meta based on the last successful task, i.e.\n        # based on METADATA_FILENAME\n        output_dataset.meta = assemble_meta_failed_job(job, output_dataset)\n\n        # Assemble new history and assign it to output_dataset.meta\n        output_dataset.history = assemble_history_failed_job(\n            job,\n            output_dataset,\n            workflow,\n            logger,\n        )\n\n        db_sync.merge(output_dataset)\n\n        job.status = JobStatusTypeV1.FAILED\n        job.end_timestamp = get_timestamp()\n        job.log = (\n            f\"UNKNOWN ERROR in Fractal job {job.id}\\n\"\n            f\"TRACEBACK:\\n{current_traceback}\"\n        )\n        db_sync.merge(job)\n        close_job_logger(logger)\n        db_sync.commit()\n    finally:\n        db_sync.close()\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_common/","title":"_common","text":"<p>Common utilities and routines for runner backends (private API)</p> <p>This module includes utilities and routines that are of use to implement runner backends and that should not be exposed outside of the runner subsystem.</p>"},{"location":"reference/fractal_server/app/runner/v1/_common/#fractal_server.app.runner.v1._common._call_command_wrapper","title":"<code>_call_command_wrapper(cmd, stdout, stderr)</code>","text":"<p>Call a command and write its stdout and stderr to files</p> <p>Raises:</p> Type Description <code>TaskExecutionError</code> <p>If the <code>subprocess.run</code> call returns a positive                 exit code</p> <code>JobExecutionError</code> <p>If the <code>subprocess.run</code> call returns a negative                 exit code (e.g. due to the subprocess receiving a                 TERM or KILL signal)</p> Source code in <code>fractal_server/app/runner/v1/_common.py</code> <pre><code>def _call_command_wrapper(cmd: str, stdout: Path, stderr: Path) -&gt; None:\n    \"\"\"\n    Call a command and write its stdout and stderr to files\n\n    Raises:\n        TaskExecutionError: If the `subprocess.run` call returns a positive\n                            exit code\n        JobExecutionError:  If the `subprocess.run` call returns a negative\n                            exit code (e.g. due to the subprocess receiving a\n                            TERM or KILL signal)\n    \"\"\"\n\n    # Verify that task command is executable\n    if shutil.which(shlex_split(cmd)[0]) is None:\n        msg = (\n            f'Command \"{shlex_split(cmd)[0]}\" is not valid. '\n            \"Hint: make sure that it is executable.\"\n        )\n        raise TaskExecutionError(msg)\n\n    fp_stdout = open(stdout, \"w\")\n    fp_stderr = open(stderr, \"w\")\n    try:\n        result = subprocess.run(  # nosec\n            shlex_split(cmd),\n            stderr=fp_stderr,\n            stdout=fp_stdout,\n        )\n    except Exception as e:\n        raise e\n    finally:\n        fp_stdout.close()\n        fp_stderr.close()\n\n    if result.returncode &gt; 0:\n        with stderr.open(\"r\") as fp_stderr:\n            err = fp_stderr.read()\n        raise TaskExecutionError(err)\n    elif result.returncode &lt; 0:\n        raise JobExecutionError(\n            info=f\"Task failed with returncode={result.returncode}\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_common/#fractal_server.app.runner.v1._common._task_needs_image_list","title":"<code>_task_needs_image_list(_task)</code>","text":"<p>Whether a task requires <code>metadata[\"image\"]</code> in its <code>args.json</code> file.</p> <p>For details see https://github.com/fractal-analytics-platform/fractal-server/issues/1237</p> <p>Parameters:</p> Name Type Description Default <code>_task</code> <code>Task</code> <p>The task to be checked.</p> required Source code in <code>fractal_server/app/runner/v1/_common.py</code> <pre><code>def _task_needs_image_list(_task: Task) -&gt; bool:\n    \"\"\"\n    Whether a task requires `metadata[\"image\"]` in its `args.json` file.\n\n    For details see\n    https://github.com/fractal-analytics-platform/fractal-server/issues/1237\n\n    Args:\n        _task: The task to be checked.\n    \"\"\"\n    settings = Inject(get_settings)\n    exception_task_names = settings.FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE.split(\n        \";\"\n    )\n    if _task.name in exception_task_names:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_common/#fractal_server.app.runner.v1._common.call_parallel_task","title":"<code>call_parallel_task(*, executor, wftask, task_pars_depend, workflow_dir, workflow_dir_user=None, submit_setup_call=no_op_submit_setup_call, logger_name=None)</code>","text":"<p>Collect results from the parallel instances of a parallel task</p> <p>Prepare and submit for execution all the single calls of a parallel task, and return a single TaskParameters instance to be passed on to the next task.</p> <p>NOTE: this function is executed by the same user that runs <code>fractal-server</code>, and therefore may not have access to some of user's files.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Executor</code> <p>The <code>concurrent.futures.Executor</code>-compatible executor that will run the task.</p> required <code>wftask</code> <code>WorkflowTask</code> <p>The parallel task to run.</p> required <code>task_pars_depend</code> <code>TaskParameters</code> <p>The task parameters to be passed on to the parallel task.</p> required <code>workflow_dir</code> <code>Path</code> <p>The server-side working directory for workflow execution.</p> required <code>workflow_dir_user</code> <code>Optional[Path]</code> <p>The user-side working directory for workflow execution (only relevant for multi-user executors).</p> <code>None</code> <code>submit_setup_call</code> <code>Callable</code> <p>An optional function that computes configuration parameters for the executor.</p> <code>no_op_submit_setup_call</code> <code>logger_name</code> <code>Optional[str]</code> <p>Name of the logger</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_task_parameters</code> <code>TaskParameters</code> <p>The output task parameters of the parallel task execution, ready to be passed on to the next task.</p> Source code in <code>fractal_server/app/runner/v1/_common.py</code> <pre><code>def call_parallel_task(\n    *,\n    executor: Executor,\n    wftask: WorkflowTask,\n    task_pars_depend: TaskParameters,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    submit_setup_call: Callable = no_op_submit_setup_call,\n    logger_name: Optional[str] = None,\n) -&gt; TaskParameters:\n    \"\"\"\n    Collect results from the parallel instances of a parallel task\n\n    Prepare and submit for execution all the single calls of a parallel task,\n    and return a single TaskParameters instance to be passed on to the\n    next task.\n\n    **NOTE**: this function is executed by the same user that runs\n    `fractal-server`, and therefore may not have access to some of user's\n    files.\n\n    Args:\n        executor:\n            The `concurrent.futures.Executor`-compatible executor that will\n            run the task.\n        wftask:\n            The parallel task to run.\n        task_pars_depend:\n            The task parameters to be passed on to the parallel task.\n        workflow_dir:\n            The server-side working directory for workflow execution.\n        workflow_dir_user:\n            The user-side working directory for workflow execution (only\n            relevant for multi-user executors).\n        submit_setup_call:\n            An optional function that computes configuration parameters for\n            the executor.\n        logger_name:\n            Name of the logger\n\n    Returns:\n        out_task_parameters:\n            The output task parameters of the parallel task execution, ready to\n            be passed on to the next task.\n    \"\"\"\n    logger = get_logger(logger_name)\n\n    if not workflow_dir_user:\n        workflow_dir_user = workflow_dir\n\n    try:\n        component_list = task_pars_depend.metadata[\n            wftask.parallelization_level\n        ]\n    except KeyError:\n        keys = list(task_pars_depend.metadata.keys())\n        raise RuntimeError(\n            \"WorkflowTask parallelization_level \"\n            f\"('{wftask.parallelization_level}') is missing \"\n            f\"in metadata keys ({keys}).\"\n        )\n\n    # Backend-specific configuration\n    try:\n        extra_setup = submit_setup_call(\n            wftask=wftask,\n            workflow_dir=workflow_dir,\n            workflow_dir_user=workflow_dir_user,\n        )\n    except Exception as e:\n        tb = \"\".join(traceback.format_tb(e.__traceback__))\n        raise RuntimeError(\n            f\"{type(e)} error in {submit_setup_call=}\\n\"\n            f\"Original traceback:\\n{tb}\"\n        )\n\n    # Preliminary steps\n    actual_task_pars_depend = trim_TaskParameters(\n        task_pars_depend, wftask.task\n    )\n\n    partial_call_task = partial(\n        call_single_parallel_task,\n        wftask=wftask,\n        task_pars=actual_task_pars_depend,\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n    )\n\n    # Submit tasks for execution. Note that `for _ in map_iter:\n    # pass` explicitly calls the .result() method for each future, and\n    # therefore is blocking until the task are complete.\n    map_iter = executor.map(partial_call_task, component_list, **extra_setup)\n\n    # Wait for execution of parallel tasks, and aggregate updated metadata (ref\n    # https://github.com/fractal-analytics-platform/fractal-server/issues/802).\n    # NOTE: Even if we remove the need of aggregating metadata, we must keep\n    # the iteration over `map_iter` (e.g. as in `for _ in map_iter: pass`), to\n    # make this call blocking. This is required *also* because otherwise the\n    # shutdown of a FractalSlurmExecutor while running map() may not work\n    aggregated_metadata_update: dict[str, Any] = {}\n    for this_meta_update in map_iter:\n        # Cover the case where the task wrote `null`, rather than a\n        # valid dictionary (ref fractal-server issue #878), or where the\n        # metadiff file was missing.\n        if this_meta_update is None:\n            this_meta_update = {}\n        # Include this_meta_update into aggregated_metadata_update\n        for key, val in this_meta_update.items():\n            aggregated_metadata_update.setdefault(key, []).append(val)\n    if aggregated_metadata_update:\n        logger.warning(\n            \"Aggregating parallel-taks updated metadata (with keys \"\n            f\"{list(aggregated_metadata_update.keys())}).\\n\"\n            \"This feature is experimental and it may change in \"\n            \"future releases.\"\n        )\n\n    # Prepare updated_metadata\n    updated_metadata = task_pars_depend.metadata.copy()\n    updated_metadata.update(aggregated_metadata_update)\n\n    # Prepare updated_history (note: the expected type for history items is\n    # defined in `_DatasetHistoryItem`)\n    wftask_dump = wftask.model_dump(exclude={\"task\"})\n    wftask_dump[\"task\"] = wftask.task.model_dump()\n    new_history_item = dict(\n        workflowtask=wftask_dump,\n        status=WorkflowTaskStatusTypeV1.DONE,\n        parallelization=dict(\n            parallelization_level=wftask.parallelization_level,\n            component_list=component_list,\n        ),\n    )\n    updated_history = task_pars_depend.history.copy()\n    updated_history.append(new_history_item)\n\n    # Assemble a TaskParameter object\n    out_task_parameters = TaskParameters(\n        input_paths=[task_pars_depend.output_path],\n        output_path=task_pars_depend.output_path,\n        metadata=updated_metadata,\n        history=updated_history,\n    )\n\n    return out_task_parameters\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_common/#fractal_server.app.runner.v1._common.call_single_parallel_task","title":"<code>call_single_parallel_task(component, *, wftask, task_pars, workflow_dir, workflow_dir_user=None)</code>","text":"<p>Call a single instance of a parallel task</p> <p>Parallel tasks need to run in several instances across the parallelization parameters. This function is responsible of running each single one of those instances.</p> Note <p>This function is directly submitted to a <code>concurrent.futures</code>-compatible executor, roughly as in</p> <pre><code>some_future = executor.map(call_single_parallel_task, ...)\n</code></pre> <p>If the executor then impersonates another user (as in the <code>FractalSlurmExecutor</code>), this function is run by that user.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>str</code> <p>The parallelization parameter.</p> required <code>wftask</code> <code>WorkflowTask</code> <p>The task to execute.</p> required <code>task_pars</code> <code>TaskParameters</code> <p>The parameters to pass on to the task.</p> required <code>workflow_dir</code> <code>Path</code> <p>The server-side working directory for workflow execution.</p> required <code>workflow_dir_user</code> <code>Optional[Path]</code> <p>The user-side working directory for workflow execution (only relevant for multi-user executors).</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The <code>json.load</code>-ed contents of the metadiff output file, or <code>None</code> if the file is missing.</p> <p>Raises:</p> Type Description <code>TaskExecutionError</code> <p>If the wrapped task raises a task-related error.                 This function is responsible of adding debugging                 information to the TaskExecutionError, such as task                 order and name.</p> <code>JobExecutionError</code> <p>If the wrapped task raises a job-related error.</p> <code>RuntimeError</code> <p>If the <code>workflow_dir</code> is falsy.</p> Source code in <code>fractal_server/app/runner/v1/_common.py</code> <pre><code>def call_single_parallel_task(\n    component: str,\n    *,\n    wftask: WorkflowTask,\n    task_pars: TaskParameters,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n) -&gt; Any:\n    \"\"\"\n    Call a single instance of a parallel task\n\n    Parallel tasks need to run in several instances across the parallelization\n    parameters. This function is responsible of running each single one of\n    those instances.\n\n    Note:\n        This function is directly submitted to a\n        `concurrent.futures`-compatible executor, roughly as in\n\n            some_future = executor.map(call_single_parallel_task, ...)\n\n        If the executor then impersonates another user (as in the\n        `FractalSlurmExecutor`), this function is run by that user.\n\n    Args:\n        component:\n            The parallelization parameter.\n        wftask:\n            The task to execute.\n        task_pars:\n            The parameters to pass on to the task.\n        workflow_dir:\n            The server-side working directory for workflow execution.\n        workflow_dir_user:\n            The user-side working directory for workflow execution (only\n            relevant for multi-user executors).\n\n    Returns:\n        The `json.load`-ed contents of the metadiff output file, or `None` if\n            the file is missing.\n\n    Raises:\n        TaskExecutionError: If the wrapped task raises a task-related error.\n                            This function is responsible of adding debugging\n                            information to the TaskExecutionError, such as task\n                            order and name.\n        JobExecutionError: If the wrapped task raises a job-related error.\n        RuntimeError: If the `workflow_dir` is falsy.\n    \"\"\"\n    if not workflow_dir:\n        raise RuntimeError\n    if not workflow_dir_user:\n        workflow_dir_user = workflow_dir\n\n    task_files = get_task_file_paths(\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n        task_order=wftask.order,\n        component=component,\n    )\n\n    # write args file (by assembling task_pars, wftask.args and component)\n    write_args_file(\n        task_pars.dict(exclude={\"history\"}),\n        wftask.args or {},\n        dict(component=component),\n        path=task_files.args,\n    )\n\n    # assemble full command\n    cmd = (\n        f\"{wftask.task.command} -j {task_files.args} \"\n        f\"--metadata-out {task_files.metadiff}\"\n    )\n\n    try:\n        _call_command_wrapper(\n            cmd, stdout=task_files.out, stderr=task_files.err\n        )\n    except TaskExecutionError as e:\n        e.workflow_task_order = wftask.order\n        e.workflow_task_id = wftask.id\n        e.task_name = wftask.task.name\n        raise e\n\n    # JSON-load metadiff file and return its contents (or None)\n    try:\n        with task_files.metadiff.open(\"r\") as f:\n            this_meta_update = json.load(f)\n    except FileNotFoundError:\n        this_meta_update = None\n\n    return this_meta_update\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_common/#fractal_server.app.runner.v1._common.call_single_task","title":"<code>call_single_task(*, wftask, task_pars, workflow_dir, workflow_dir_user=None, logger_name=None)</code>","text":"<p>Call a single task</p> <p>This assembles the runner arguments (input_paths, output_path, ...) and wftask arguments (i.e., arguments that are specific to the WorkflowTask, such as message or index in the dummy task), writes them to file, call the task executable command passing the arguments file as an input and assembles the output.</p> <p>Note: This function is directly submitted to a <code>concurrent.futures</code>-compatible executor, as in</p> <pre><code>some_future = executor.submit(call_single_task, ...)\n</code></pre> <p>If the executor then impersonates another user (as in the <code>FractalSlurmExecutor</code>), this function is run by that user.  For this reason, it should not write any file to workflow_dir, or it may yield permission errors.</p> <p>Parameters:</p> Name Type Description Default <code>wftask</code> <code>WorkflowTask</code> <p>The workflow task to be called. This includes task specific arguments via the wftask.args attribute.</p> required <code>task_pars</code> <code>TaskParameters</code> <p>The parameters required to run the task which are not specific to the task, e.g., I/O paths.</p> required <code>workflow_dir</code> <code>Path</code> <p>The server-side working directory for workflow execution.</p> required <code>workflow_dir_user</code> <code>Optional[Path]</code> <p>The user-side working directory for workflow execution (only relevant for multi-user executors). If <code>None</code>, it is set to be equal to <code>workflow_dir</code>.</p> <code>None</code> <code>logger_name</code> <code>Optional[str]</code> <p>Name of the logger</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_task_parameters</code> <code>TaskParameters</code> <p>A TaskParameters in which the previous output becomes the input and where metadata is the metadata dictionary returned by the task being called.</p> <p>Raises:</p> Type Description <code>TaskExecutionError</code> <p>If the wrapped task raises a task-related error.                 This function is responsible of adding debugging                 information to the TaskExecutionError, such as task                 order and name.</p> <code>JobExecutionError</code> <p>If the wrapped task raises a job-related error.</p> <code>RuntimeError</code> <p>If the <code>workflow_dir</code> is falsy.</p> Source code in <code>fractal_server/app/runner/v1/_common.py</code> <pre><code>def call_single_task(\n    *,\n    wftask: WorkflowTask,\n    task_pars: TaskParameters,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    logger_name: Optional[str] = None,\n) -&gt; TaskParameters:\n    \"\"\"\n    Call a single task\n\n    This assembles the runner arguments (input_paths, output_path, ...) and\n    wftask arguments (i.e., arguments that are specific to the WorkflowTask,\n    such as message or index in the dummy task), writes them to file, call the\n    task executable command passing the arguments file as an input and\n    assembles the output.\n\n    **Note**: This function is directly submitted to a\n    `concurrent.futures`-compatible executor, as in\n\n        some_future = executor.submit(call_single_task, ...)\n\n    If the executor then impersonates another user (as in the\n    `FractalSlurmExecutor`), this function is run by that user.  For this\n    reason, it should not write any file to workflow_dir, or it may yield\n    permission errors.\n\n    Args:\n        wftask:\n            The workflow task to be called. This includes task specific\n            arguments via the wftask.args attribute.\n        task_pars:\n            The parameters required to run the task which are not specific to\n            the task, e.g., I/O paths.\n        workflow_dir:\n            The server-side working directory for workflow execution.\n        workflow_dir_user:\n            The user-side working directory for workflow execution (only\n            relevant for multi-user executors). If `None`, it is set to be\n            equal to `workflow_dir`.\n        logger_name:\n            Name of the logger\n\n    Returns:\n        out_task_parameters:\n            A TaskParameters in which the previous output becomes the input\n            and where metadata is the metadata dictionary returned by the task\n            being called.\n\n    Raises:\n        TaskExecutionError: If the wrapped task raises a task-related error.\n                            This function is responsible of adding debugging\n                            information to the TaskExecutionError, such as task\n                            order and name.\n        JobExecutionError: If the wrapped task raises a job-related error.\n        RuntimeError: If the `workflow_dir` is falsy.\n    \"\"\"\n\n    logger = get_logger(logger_name)\n\n    if not workflow_dir_user:\n        workflow_dir_user = workflow_dir\n\n    task_files = get_task_file_paths(\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n        task_order=wftask.order,\n    )\n\n    # write args file (by assembling task_pars and wftask.args)\n    write_args_file(\n        task_pars.dict(exclude={\"history\"}),\n        wftask.args or {},\n        path=task_files.args,\n    )\n\n    # assemble full command\n    cmd = (\n        f\"{wftask.task.command} -j {task_files.args} \"\n        f\"--metadata-out {task_files.metadiff}\"\n    )\n\n    try:\n        _call_command_wrapper(\n            cmd, stdout=task_files.out, stderr=task_files.err\n        )\n    except TaskExecutionError as e:\n        e.workflow_task_order = wftask.order\n        e.workflow_task_id = wftask.id\n        e.task_name = wftask.task.name\n        raise e\n\n    # This try/except block covers the case of a task that ran successfully but\n    # did not write the expected metadiff file (ref fractal-server issue #854).\n    try:\n        with task_files.metadiff.open(\"r\") as f_metadiff:\n            diff_metadata = json.load(f_metadiff)\n    except FileNotFoundError as e:\n        logger.warning(\n            f\"Skip collection of updated metadata. Original error: {str(e)}\"\n        )\n        diff_metadata = {}\n\n    # Cover the case where the task wrote `null`, rather than a valid\n    # dictionary (ref fractal-server issue #878).\n    if diff_metadata is None:\n        diff_metadata = {}\n\n    # Prepare updated_metadata\n    updated_metadata = task_pars.metadata.copy()\n    updated_metadata.update(diff_metadata)\n    # Prepare updated_history (note: the expected type for history items is\n    # defined in `_DatasetHistoryItem`)\n    wftask_dump = wftask.model_dump(exclude={\"task\"})\n    wftask_dump[\"task\"] = wftask.task.model_dump()\n    new_history_item = dict(\n        workflowtask=wftask_dump,\n        status=WorkflowTaskStatusTypeV1.DONE,\n        parallelization=None,\n    )\n    updated_history = task_pars.history.copy()\n    updated_history.append(new_history_item)\n\n    # Assemble a TaskParameter object\n    out_task_parameters = TaskParameters(\n        input_paths=[task_pars.output_path],\n        output_path=task_pars.output_path,\n        metadata=updated_metadata,\n        history=updated_history,\n    )\n\n    return out_task_parameters\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_common/#fractal_server.app.runner.v1._common.execute_tasks","title":"<code>execute_tasks(*, executor, task_list, task_pars, workflow_dir, workflow_dir_user=None, submit_setup_call=no_op_submit_setup_call, logger_name)</code>","text":"<p>Submit a list of WorkflowTasks for execution</p> <p>Note: At the end of each task, write current metadata to <code>working_dir / METADATA_FILENAME</code>, so that they can be read as part of the <code>get_job</code> endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Executor</code> <p>The <code>concurrent.futures.Executor</code>-compatible executor that will run the task.</p> required <code>task_list</code> <code>list[WorkflowTask]</code> <p>The list of wftasks to be run</p> required <code>task_pars</code> <code>TaskParameters</code> <p>The task parameters to be passed on to the first task of the list.</p> required <code>workflow_dir</code> <code>Path</code> <p>The server-side working directory for workflow execution.</p> required <code>workflow_dir_user</code> <code>Optional[Path]</code> <p>The user-side working directory for workflow execution (only relevant for multi-user executors). If <code>None</code>, it is set to be equal to <code>workflow_dir</code>.</p> <code>None</code> <code>submit_setup_call</code> <code>Callable</code> <p>An optional function that computes configuration parameters for the executor.</p> <code>no_op_submit_setup_call</code> <code>logger_name</code> <code>str</code> <p>Name of the logger</p> required <p>Returns:</p> Name Type Description <code>current_task_pars</code> <code>TaskParameters</code> <p>A TaskParameters object which constitutes the output of the last task in the list.</p> Source code in <code>fractal_server/app/runner/v1/_common.py</code> <pre><code>def execute_tasks(\n    *,\n    executor: Executor,\n    task_list: list[WorkflowTask],\n    task_pars: TaskParameters,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    submit_setup_call: Callable = no_op_submit_setup_call,\n    logger_name: str,\n) -&gt; TaskParameters:\n    \"\"\"\n    Submit a list of WorkflowTasks for execution\n\n    **Note:** At the end of each task, write current metadata to `working_dir /\n    METADATA_FILENAME`, so that they can be read as part of the [`get_job`\n    endpoint](../../api/v1/job/#fractal_server.app.routes.api.v1.job.get_job).\n\n    Arguments:\n        executor:\n            The `concurrent.futures.Executor`-compatible executor that will\n            run the task.\n        task_list:\n            The list of wftasks to be run\n        task_pars:\n            The task parameters to be passed on to the first task of the list.\n        workflow_dir:\n            The server-side working directory for workflow execution.\n        workflow_dir_user:\n            The user-side working directory for workflow execution (only\n            relevant for multi-user executors). If `None`, it is set to be\n            equal to `workflow_dir`.\n        submit_setup_call:\n            An optional function that computes configuration parameters for\n            the executor.\n        logger_name:\n            Name of the logger\n\n    Returns:\n        current_task_pars:\n            A TaskParameters object which constitutes the output of the last\n            task in the list.\n    \"\"\"\n    if not workflow_dir_user:\n        workflow_dir_user = workflow_dir\n\n    logger = get_logger(logger_name)\n\n    current_task_pars = task_pars.copy()\n\n    for this_wftask in task_list:\n        logger.debug(\n            f\"SUBMIT {this_wftask.order}-th task \"\n            f'(name=\"{this_wftask.task.name}\")'\n        )\n        if this_wftask.is_parallel:\n            current_task_pars = call_parallel_task(\n                executor=executor,\n                wftask=this_wftask,\n                task_pars_depend=current_task_pars,\n                workflow_dir=workflow_dir,\n                workflow_dir_user=workflow_dir_user,\n                submit_setup_call=submit_setup_call,\n                logger_name=logger_name,\n            )\n        else:\n            # Call backend-specific submit_setup_call\n            try:\n                extra_setup = submit_setup_call(\n                    wftask=this_wftask,\n                    workflow_dir=workflow_dir,\n                    workflow_dir_user=workflow_dir_user,\n                )\n            except Exception as e:\n                tb = \"\".join(traceback.format_tb(e.__traceback__))\n                raise RuntimeError(\n                    f\"{type(e)} error in {submit_setup_call=}\\n\"\n                    f\"Original traceback:\\n{tb}\"\n                )\n            # NOTE: executor.submit(call_single_task, ...) is non-blocking,\n            # i.e. the returned future may have `this_wftask_future.done() =\n            # False`. We make it blocking right away, by calling `.result()`\n            # NOTE: do not use trim_TaskParameters for non-parallel tasks,\n            # since the `task_pars` argument in `call_single_task` is also used\n            # as a basis for new `metadata`.\n            this_wftask_future = executor.submit(\n                call_single_task,\n                wftask=this_wftask,\n                task_pars=current_task_pars,\n                workflow_dir=workflow_dir,\n                workflow_dir_user=workflow_dir_user,\n                logger_name=logger_name,\n                **extra_setup,\n            )\n            # Wait for the future result (blocking)\n            current_task_pars = this_wftask_future.result()\n        logger.debug(\n            f\"END    {this_wftask.order}-th task \"\n            f'(name=\"{this_wftask.task.name}\")'\n        )\n\n        # Write most recent metadata to METADATA_FILENAME\n        with open(workflow_dir / METADATA_FILENAME, \"w\") as f:\n            json.dump(current_task_pars.metadata, f, indent=2)\n\n        # Write most recent metadata to HISTORY_FILENAME\n        with open(workflow_dir / HISTORY_FILENAME, \"w\") as f:\n            json.dump(current_task_pars.history, f, indent=2)\n\n    return current_task_pars\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_common/#fractal_server.app.runner.v1._common.no_op_submit_setup_call","title":"<code>no_op_submit_setup_call(*, wftask, workflow_dir, workflow_dir_user)</code>","text":"<p>Default (no-operation) interface of submit_setup_call.</p> Source code in <code>fractal_server/app/runner/v1/_common.py</code> <pre><code>def no_op_submit_setup_call(\n    *,\n    wftask: WorkflowTask,\n    workflow_dir: Path,\n    workflow_dir_user: Path,\n) -&gt; dict:\n    \"\"\"\n    Default (no-operation) interface of submit_setup_call.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_common/#fractal_server.app.runner.v1._common.trim_TaskParameters","title":"<code>trim_TaskParameters(task_params, _task)</code>","text":"<p>Return a smaller copy of a TaskParameter object.</p> <p>Remove metadata[\"image\"] key/value pair - see issues 1237 and 1242. (https://github.com/fractal-analytics-platform/fractal-server/issues/1237) This applies only to parallel tasks with names different from the ones defined in <code>_task_needs_image_list</code>.</p> Source code in <code>fractal_server/app/runner/v1/_common.py</code> <pre><code>def trim_TaskParameters(\n    task_params: TaskParameters,\n    _task: Task,\n) -&gt; TaskParameters:\n    \"\"\"\n    Return a smaller copy of a TaskParameter object.\n\n    Remove metadata[\"image\"] key/value pair - see issues 1237 and 1242.\n    (https://github.com/fractal-analytics-platform/fractal-server/issues/1237)\n    This applies only to parallel tasks with names different from the ones\n    defined in `_task_needs_image_list`.\n    \"\"\"\n    task_params_slim = deepcopy(task_params)\n    if not _task_needs_image_list(_task) and _task.is_parallel:\n        if \"image\" in task_params_slim.metadata.keys():\n            task_params_slim.metadata.pop(\"image\")\n        task_params_slim.history = []\n    return task_params_slim\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/common/","title":"common","text":"<p>Common utilities and routines for runner backends (public API)</p> <p>This module includes utilities and routines that are of use to implement runner backends but that should also be exposed to the other components of <code>Fractal Server</code>.</p>"},{"location":"reference/fractal_server/app/runner/v1/common/#fractal_server.app.runner.v1.common.TaskParameterEncoder","title":"<code>TaskParameterEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>Convenience JSONEncoder that serialises <code>Path</code>s as strings</p> Source code in <code>fractal_server/app/runner/v1/common.py</code> <pre><code>class TaskParameterEncoder(JSONEncoder):\n    \"\"\"\n    Convenience JSONEncoder that serialises `Path`s as strings\n    \"\"\"\n\n    def default(self, value):\n        if isinstance(value, Path):\n            return value.as_posix()\n        return JSONEncoder.default(self, value)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/common/#fractal_server.app.runner.v1.common.TaskParameters","title":"<code>TaskParameters</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Wrapper for task input parameters</p> <p>Instances of this class are used to pass parameters from the output of a task to the input of the next one.</p> <p>Attributes:</p> Name Type Description <code>input_paths</code> <code>list[Path]</code> <p>Input paths as derived by the input dataset.</p> <code>output_paths</code> <code>list[Path]</code> <p>Output path as derived from the output dataset.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Dataset metadata, as found in the input dataset or as updated by the previous task.</p> <code>history</code> <code>list[dict[str, Any]]</code> <p>Dataset history, as found in the input dataset or as updated by the previous task.</p> Source code in <code>fractal_server/app/runner/v1/common.py</code> <pre><code>class TaskParameters(BaseModel):\n    \"\"\"\n    Wrapper for task input parameters\n\n    Instances of this class are used to pass parameters from the output of a\n    task to the input of the next one.\n\n    Attributes:\n        input_paths:\n            Input paths as derived by the input dataset.\n        output_paths:\n            Output path as derived from the output dataset.\n        metadata:\n            Dataset metadata, as found in the input dataset or as updated by\n            the previous task.\n        history:\n            Dataset history, as found in the input dataset or as updated by\n            the previous task.\n    \"\"\"\n\n    input_paths: list[Path]\n    output_path: Path\n    metadata: dict[str, Any]\n    history: list[dict[str, Any]]\n\n    class Config:\n        arbitrary_types_allowed = True\n        extra = \"forbid\"\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/common/#fractal_server.app.runner.v1.common.validate_workflow_compatibility","title":"<code>validate_workflow_compatibility(*, input_dataset, workflow, output_dataset, first_task_index, last_task_index)</code>","text":"<p>Check compatibility of workflow and input / ouptut dataset</p> Source code in <code>fractal_server/app/runner/v1/common.py</code> <pre><code>def validate_workflow_compatibility(\n    *,\n    input_dataset: Dataset,\n    workflow: Workflow,\n    output_dataset: Dataset,\n    first_task_index: int,\n    last_task_index: int,\n) -&gt; None:\n    \"\"\"\n    Check compatibility of workflow and input / ouptut dataset\n    \"\"\"\n    # Check input_dataset type\n    workflow_input_type = workflow.task_list[first_task_index].task.input_type\n    if (\n        workflow_input_type != \"Any\"\n        and workflow_input_type != input_dataset.type\n    ):\n        raise TypeError(\n            f\"Incompatible types `{workflow_input_type}` of workflow \"\n            f\"`{workflow.name}` and `{input_dataset.type}` of dataset \"\n            f\"`{input_dataset.name}`\"\n        )\n\n    # Check output_dataset type\n    workflow_output_type = workflow.task_list[last_task_index].task.output_type\n    if (\n        workflow_output_type != \"Any\"\n        and workflow_output_type != output_dataset.type\n    ):\n        raise TypeError(\n            f\"Incompatible types `{workflow_output_type}` of workflow \"\n            f\"`{workflow.name}` and `{output_dataset.type}` of dataset \"\n            f\"`{output_dataset.name}`\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/common/#fractal_server.app.runner.v1.common.write_args_file","title":"<code>write_args_file(*args, path)</code>","text":"<p>Merge arbitrary dictionaries and write to file</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>dict[str, Any]</code> <p>One or more dictionaries that will be merged into one respecting the order with which they are passed in, i.e., last in overrides previous ones.</p> <code>()</code> <code>path</code> <code>Path</code> <p>Destination for serialised file.</p> required Source code in <code>fractal_server/app/runner/v1/common.py</code> <pre><code>def write_args_file(\n    *args: dict[str, Any],\n    path: Path,\n):\n    \"\"\"\n    Merge arbitrary dictionaries and write to file\n\n    Args:\n        *args:\n            One or more dictionaries that will be merged into one respecting\n            the order with which they are passed in, i.e., last in overrides\n            previous ones.\n        path:\n            Destination for serialised file.\n    \"\"\"\n    out = {}\n    for d in args:\n        out.update(d)\n\n    with open(path, \"w\") as f:\n        json.dump(out, f, cls=TaskParameterEncoder, indent=4)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/handle_failed_job/","title":"handle_failed_job","text":"<p>Helper functions to handle Dataset history.</p>"},{"location":"reference/fractal_server/app/runner/v1/handle_failed_job/#fractal_server.app.runner.v1.handle_failed_job.assemble_history_failed_job","title":"<code>assemble_history_failed_job(job, output_dataset, workflow, logger, failed_wftask=None)</code>","text":"<p>Assemble <code>history</code> after a workflow-execution job fails.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>ApplyWorkflow</code> <p>The failed <code>ApplyWorkflow</code> object.</p> required <code>output_dataset</code> <code>Dataset</code> <p>The <code>output_dataset</code> associated to <code>job</code>.</p> required <code>workflow</code> <code>Workflow</code> <p>The <code>workflow</code> associated to <code>job</code>.</p> required <code>logger</code> <code>Logger</code> <p>A logger instance.</p> required <code>failed_wftask</code> <code>Optional[WorkflowTask]</code> <p>If set, append it to <code>history</code> during step 3; if <code>None</code>, infer it by comparing the job task list and the one in <code>tmp_metadata_file</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>The new value of <code>history</code>, to be merged into</p> <code>list[dict[str, Any]]</code> <p><code>output_dataset.meta</code>.</p> Source code in <code>fractal_server/app/runner/v1/handle_failed_job.py</code> <pre><code>def assemble_history_failed_job(\n    job: ApplyWorkflow,\n    output_dataset: Dataset,\n    workflow: Workflow,\n    logger: logging.Logger,\n    failed_wftask: Optional[WorkflowTask] = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Assemble `history` after a workflow-execution job fails.\n\n    Args:\n        job:\n            The failed `ApplyWorkflow` object.\n        output_dataset:\n            The `output_dataset` associated to `job`.\n        workflow:\n            The `workflow` associated to `job`.\n        logger: A logger instance.\n        failed_wftask:\n            If set, append it to `history` during step 3; if `None`, infer\n            it by comparing the job task list and the one in\n            `tmp_metadata_file`.\n\n    Returns:\n        The new value of `history`, to be merged into\n        `output_dataset.meta`.\n    \"\"\"\n\n    # The final value of the history attribute should include up to three\n    # parts, coming from: the database, the temporary file, the failed-task\n    # information.\n\n    # Part 1: Read exising history from DB\n    new_history = output_dataset.history\n\n    # Part 2: Extend history based on tmp_metadata_file\n    tmp_history_file = Path(job.working_dir) / HISTORY_FILENAME\n    try:\n        with tmp_history_file.open(\"r\") as f:\n            tmp_file_history = json.load(f)\n            new_history.extend(tmp_file_history)\n    except FileNotFoundError:\n        tmp_file_history = []\n\n    # Part 3/A: Identify failed task, if needed\n    if failed_wftask is None:\n        job_wftasks = workflow.task_list[\n            job.first_task_index : (job.last_task_index + 1)  # noqa\n        ]\n        tmp_file_wftasks = [\n            history_item[\"workflowtask\"] for history_item in tmp_file_history\n        ]\n        if len(job_wftasks) &lt;= len(tmp_file_wftasks):\n            n_tasks_job = len(job_wftasks)\n            n_tasks_tmp = len(tmp_file_wftasks)\n            logger.error(\n                \"Cannot identify the failed task based on job task list \"\n                f\"(length {n_tasks_job}) and temporary-file task list \"\n                f\"(length {n_tasks_tmp}).\"\n            )\n            logger.error(\"Failed task not appended to history.\")\n        else:\n            failed_wftask = job_wftasks[len(tmp_file_wftasks)]\n\n    # Part 3/B: Append failed task to history\n    if failed_wftask is not None:\n        failed_wftask_dump = failed_wftask.model_dump(exclude={\"task\"})\n        failed_wftask_dump[\"task\"] = failed_wftask.task.model_dump()\n        new_history_item = dict(\n            workflowtask=failed_wftask_dump,\n            status=WorkflowTaskStatusTypeV1.FAILED,\n            parallelization=dict(\n                parallelization_level=failed_wftask.parallelization_level,\n            ),\n        )\n        new_history.append(new_history_item)\n\n    return new_history\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/handle_failed_job/#fractal_server.app.runner.v1.handle_failed_job.assemble_meta_failed_job","title":"<code>assemble_meta_failed_job(job, output_dataset)</code>","text":"<p>Assemble <code>Dataset.meta</code> (history excluded) for a failed workflow-execution.</p> <p>Assemble new value of <code>output_dataset.meta</code> based on the last successful task, i.e. based on the content of the temporary <code>METADATA_FILENAME</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>ApplyWorkflow</code> <p>The failed <code>ApplyWorkflow</code> object.</p> required <code>output_dataset</code> <code>Dataset</code> <p>The <code>output_dataset</code> associated to <code>job</code>.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The new value of <code>output_dataset.meta</code>, apart from its <code>history</code> key.</p> Source code in <code>fractal_server/app/runner/v1/handle_failed_job.py</code> <pre><code>def assemble_meta_failed_job(\n    job: ApplyWorkflow,\n    output_dataset: Dataset,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Assemble `Dataset.meta` (history excluded) for a failed workflow-execution.\n\n    Assemble new value of `output_dataset.meta` based on the last successful\n    task, i.e. based on the content of the temporary `METADATA_FILENAME` file.\n\n    Args:\n        job:\n            The failed `ApplyWorkflow` object.\n        output_dataset:\n            The `output_dataset` associated to `job`.\n\n    Returns:\n        The new value of `output_dataset.meta`, apart from its `history` key.\n    \"\"\"\n\n    new_meta = deepcopy(output_dataset.meta)\n    metadata_file = Path(job.working_dir) / METADATA_FILENAME\n    try:\n        with metadata_file.open(\"r\") as f:\n            metadata_update = json.load(f)\n        for key, value in metadata_update.items():\n            new_meta[key] = value\n    except FileNotFoundError:\n        pass\n\n    return new_meta\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/","title":"_local","text":"<p>Local Bakend</p> <p>This backend runs Fractal workflows using <code>FractalThreadPoolExecutor</code> (a custom version of Python ThreadPoolExecutor) to run tasks in several threads. Incidentally, it also represents the reference implementation for a backend.</p>"},{"location":"reference/fractal_server/app/runner/v1/_local/#fractal_server.app.runner.v1._local._process_workflow","title":"<code>_process_workflow(*, workflow, input_paths, output_path, input_metadata, input_history, logger_name, workflow_dir, first_task_index, last_task_index)</code>","text":"<p>Internal processing routine</p> <p>Schedules the workflow using a <code>FractalThreadPoolExecutor</code>.</p> <p>Cf. process_workflow for the call signature.</p> Source code in <code>fractal_server/app/runner/v1/_local/__init__.py</code> <pre><code>def _process_workflow(\n    *,\n    workflow: Workflow,\n    input_paths: list[Path],\n    output_path: Path,\n    input_metadata: dict[str, Any],\n    input_history: list[dict[str, Any]],\n    logger_name: str,\n    workflow_dir: Path,\n    first_task_index: int,\n    last_task_index: int,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Internal processing routine\n\n    Schedules the workflow using a `FractalThreadPoolExecutor`.\n\n    Cf.\n    [process_workflow][fractal_server.app.runner.v1._local.process_workflow]\n    for the call signature.\n    \"\"\"\n\n    with FractalThreadPoolExecutor() as executor:\n        output_task_pars = execute_tasks(\n            executor=executor,\n            task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)  # noqa\n            ],  # noqa\n            task_pars=TaskParameters(\n                input_paths=input_paths,\n                output_path=output_path,\n                metadata=input_metadata,\n                history=input_history,\n            ),\n            workflow_dir=workflow_dir,\n            workflow_dir_user=workflow_dir,\n            logger_name=logger_name,\n            submit_setup_call=_local_submit_setup,\n        )\n    output_dataset_metadata_history = dict(\n        metadata=output_task_pars.metadata, history=output_task_pars.history\n    )\n    return output_dataset_metadata_history\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/#fractal_server.app.runner.v1._local.process_workflow","title":"<code>process_workflow(*, workflow, input_paths, output_path, input_metadata, input_history, logger_name, workflow_dir, workflow_dir_user=None, slurm_user=None, slurm_account=None, user_cache_dir=None, worker_init=None, first_task_index=None, last_task_index=None)</code>  <code>async</code>","text":"<p>Run a workflow</p> <p>This function is responsible for running a workflow on some input data, saving the output and taking care of any exception raised during the run.</p> <p>NOTE: This is the <code>local</code> backend's public interface, which also works as a reference implementation for other backends.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>Workflow</code> <p>The workflow to be run</p> required <code>input_paths</code> <code>list[Path]</code> <p>The paths to the input files to pass to the first task of the workflow</p> required <code>output_path</code> <code>Path</code> <p>The destination path for the last task of the workflow</p> required <code>input_metadata</code> <code>dict[str, Any]</code> <p>Initial metadata, passed to the first task</p> required <code>logger_name</code> <code>str</code> <p>Name of the logger to log information on the run to</p> required <code>workflow_dir</code> <code>Path</code> <p>Working directory for this run.</p> required <code>workflow_dir_user</code> <code>Optional[Path]</code> <p>Working directory for this run, on the user side. This argument is present for compatibility with the standard backend interface, but for the <code>local</code> backend it cannot be different from <code>workflow_dir</code>.</p> <code>None</code> <code>slurm_user</code> <code>Optional[str]</code> <p>Username to impersonate to run the workflow. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>slurm_account</code> <code>Optional[str]</code> <p>SLURM account to use when running the workflow. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>user_cache_dir</code> <code>Optional[str]</code> <p>Cache directory of the user who will run the workflow. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>worker_init</code> <code>Optional[str]</code> <p>Any additional, usually backend specific, information to be passed to the backend executor. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>first_task_index</code> <code>Optional[int]</code> <p>Positional index of the first task to execute; if <code>None</code>, start from <code>0</code>.</p> <code>None</code> <code>last_task_index</code> <code>Optional[int]</code> <p>Positional index of the last task to execute; if <code>None</code>, proceed until the last task.</p> <code>None</code> <p>Raises:</p> Type Description <code>TaskExecutionError</code> <p>wrapper for errors raised during tasks' execution                 (positive exit codes).</p> <code>JobExecutionError</code> <p>wrapper for errors raised by the tasks' executors                (negative exit codes).</p> <p>Returns:</p> Name Type Description <code>output_dataset_metadata</code> <code>dict[str, Any]</code> <p>The updated metadata for the dataset, as returned by the last task of the workflow</p> Source code in <code>fractal_server/app/runner/v1/_local/__init__.py</code> <pre><code>async def process_workflow(\n    *,\n    workflow: Workflow,\n    input_paths: list[Path],\n    output_path: Path,\n    input_metadata: dict[str, Any],\n    input_history: list[dict[str, Any]],\n    logger_name: str,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    slurm_user: Optional[str] = None,\n    slurm_account: Optional[str] = None,\n    user_cache_dir: Optional[str] = None,\n    worker_init: Optional[str] = None,\n    first_task_index: Optional[int] = None,\n    last_task_index: Optional[int] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Run a workflow\n\n    This function is responsible for running a workflow on some input data,\n    saving the output and taking care of any exception raised during the run.\n\n    NOTE: This is the `local` backend's public interface, which also works as\n    a reference implementation for other backends.\n\n    Args:\n        workflow:\n            The workflow to be run\n        input_paths:\n            The paths to the input files to pass to the first task of the\n            workflow\n        output_path:\n            The destination path for the last task of the workflow\n        input_metadata:\n            Initial metadata, passed to the first task\n        logger_name:\n            Name of the logger to log information on the run to\n        workflow_dir:\n            Working directory for this run.\n        workflow_dir_user:\n            Working directory for this run, on the user side. This argument is\n            present for compatibility with the standard backend interface, but\n            for the `local` backend it cannot be different from `workflow_dir`.\n        slurm_user:\n            Username to impersonate to run the workflow. This argument is\n            present for compatibility with the standard backend interface, but\n            is ignored in the `local` backend.\n        slurm_account:\n            SLURM account to use when running the workflow. This argument is\n            present for compatibility with the standard backend interface, but\n            is ignored in the `local` backend.\n        user_cache_dir:\n            Cache directory of the user who will run the workflow. This\n            argument is present for compatibility with the standard backend\n            interface, but is ignored in the `local` backend.\n        worker_init:\n            Any additional, usually backend specific, information to be passed\n            to the backend executor. This argument is present for compatibility\n            with the standard backend interface, but is ignored in the `local`\n            backend.\n        first_task_index:\n            Positional index of the first task to execute; if `None`, start\n            from `0`.\n        last_task_index:\n            Positional index of the last task to execute; if `None`, proceed\n            until the last task.\n\n    Raises:\n        TaskExecutionError: wrapper for errors raised during tasks' execution\n                            (positive exit codes).\n        JobExecutionError: wrapper for errors raised by the tasks' executors\n                           (negative exit codes).\n\n    Returns:\n        output_dataset_metadata:\n            The updated metadata for the dataset, as returned by the last task\n            of the workflow\n    \"\"\"\n\n    if workflow_dir_user and (workflow_dir_user != workflow_dir):\n        raise NotImplementedError(\n            \"Local backend does not support different directories \"\n            f\"{workflow_dir=} and {workflow_dir_user=}\"\n        )\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    output_dataset_metadata_history = await async_wrap(_process_workflow)(\n        workflow=workflow,\n        input_paths=input_paths,\n        output_path=output_path,\n        input_metadata=input_metadata,\n        input_history=input_history,\n        logger_name=logger_name,\n        workflow_dir=workflow_dir,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n    return output_dataset_metadata_history\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/_local_config/","title":"_local_config","text":"<p>Submodule to handle the local-backend configuration for a WorkflowTask</p>"},{"location":"reference/fractal_server/app/runner/v1/_local/_local_config/#fractal_server.app.runner.v1._local._local_config.LocalBackendConfig","title":"<code>LocalBackendConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Specifications of the local-backend configuration</p> <p>Attributes:</p> Name Type Description <code>parallel_tasks_per_job</code> <code>Optional[int]</code> <p>Maximum number of tasks to be run in parallel as part of a call to <code>FractalThreadPoolExecutor.map</code>; if <code>None</code>, then all tasks will start at the same time.</p> Source code in <code>fractal_server/app/runner/v1/_local/_local_config.py</code> <pre><code>class LocalBackendConfig(BaseModel, extra=Extra.forbid):\n    \"\"\"\n    Specifications of the local-backend configuration\n\n    Attributes:\n        parallel_tasks_per_job:\n            Maximum number of tasks to be run in parallel as part of a call to\n            `FractalThreadPoolExecutor.map`; if `None`, then all tasks will\n            start at the same time.\n    \"\"\"\n\n    parallel_tasks_per_job: Optional[int]\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/_local_config/#fractal_server.app.runner.v1._local._local_config.LocalBackendConfigError","title":"<code>LocalBackendConfigError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Local-backend configuration error</p> Source code in <code>fractal_server/app/runner/v1/_local/_local_config.py</code> <pre><code>class LocalBackendConfigError(ValueError):\n    \"\"\"\n    Local-backend configuration error\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/_local_config/#fractal_server.app.runner.v1._local._local_config.get_default_local_backend_config","title":"<code>get_default_local_backend_config()</code>","text":"<p>Return a default <code>LocalBackendConfig</code> configuration object</p> Source code in <code>fractal_server/app/runner/v1/_local/_local_config.py</code> <pre><code>def get_default_local_backend_config():\n    \"\"\"\n    Return a default `LocalBackendConfig` configuration object\n    \"\"\"\n    return LocalBackendConfig(parallel_tasks_per_job=None)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/_local_config/#fractal_server.app.runner.v1._local._local_config.get_local_backend_config","title":"<code>get_local_backend_config(wftask, config_path=None)</code>","text":"<p>Prepare a <code>LocalBackendConfig</code> configuration object</p> <p>The sources for <code>parallel_tasks_per_job</code> attributes, starting from the highest-priority one, are</p> <ol> <li>Properties in <code>wftask.meta</code>;</li> <li>The general content of the local-backend configuration file;</li> <li>The default value (<code>None</code>).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>wftask</code> <code>WorkflowTask</code> <p>WorkflowTask (V1) for which the backend configuration should be prepared.</p> required <code>config_path</code> <code>Optional[Path]</code> <p>Path of local-backend configuration file; if <code>None</code>, use <code>FRACTAL_LOCAL_CONFIG_FILE</code> variable from settings.</p> <code>None</code> <p>Returns:</p> Type Description <code>LocalBackendConfig</code> <p>A local-backend configuration object</p> Source code in <code>fractal_server/app/runner/v1/_local/_local_config.py</code> <pre><code>def get_local_backend_config(\n    wftask: WorkflowTask,\n    config_path: Optional[Path] = None,\n) -&gt; LocalBackendConfig:\n    \"\"\"\n    Prepare a `LocalBackendConfig` configuration object\n\n    The sources for `parallel_tasks_per_job` attributes, starting from the\n    highest-priority one, are\n\n    1. Properties in `wftask.meta`;\n    2. The general content of the local-backend configuration file;\n    3. The default value (`None`).\n\n    Arguments:\n        wftask:\n            WorkflowTask (V1) for which the backend configuration should\n            be prepared.\n        config_path:\n            Path of local-backend configuration file; if `None`, use\n            `FRACTAL_LOCAL_CONFIG_FILE` variable from settings.\n\n    Returns:\n        A local-backend configuration object\n    \"\"\"\n\n    key = \"parallel_tasks_per_job\"\n    default = None\n\n    if wftask.meta and key in wftask.meta:\n        parallel_tasks_per_job = wftask.meta[key]\n    else:\n        if not config_path:\n            settings = Inject(get_settings)\n            config_path = settings.FRACTAL_LOCAL_CONFIG_FILE\n        if config_path is None:\n            parallel_tasks_per_job = default\n        else:\n            with config_path.open(\"r\") as f:\n                env = json.load(f)\n            try:\n                _ = LocalBackendConfig(**env)\n            except ValidationError as e:\n                raise LocalBackendConfigError(\n                    f\"Error while loading {config_path=}. \"\n                    f\"Original error:\\n{str(e)}\"\n                )\n\n            parallel_tasks_per_job = env.get(key, default)\n    return LocalBackendConfig(parallel_tasks_per_job=parallel_tasks_per_job)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/_submit_setup/","title":"_submit_setup","text":"<p>Submodule to define _local_submit_setup</p>"},{"location":"reference/fractal_server/app/runner/v1/_local/_submit_setup/#fractal_server.app.runner.v1._local._submit_setup._local_submit_setup","title":"<code>_local_submit_setup(*, wftask, workflow_dir=None, workflow_dir_user=None)</code>","text":"<p>Collect WorfklowTask-specific configuration parameters from different sources, and inject them for execution.</p> <p>Parameters:</p> Name Type Description Default <code>wftask</code> <code>WorkflowTask</code> <p>WorkflowTask for which the configuration is to be assembled</p> required <code>workflow_dir</code> <code>Optional[Path]</code> <p>Not used in this function.</p> <code>None</code> <code>workflow_dir_user</code> <code>Optional[Path]</code> <p>Not used in this function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>submit_setup_dict</code> <code>dict[str, object]</code> <p>A dictionary that will be passed on to <code>FractalThreadPoolExecutor.submit</code> and <code>FractalThreadPoolExecutor.map</code>, so as to set extra options.</p> Source code in <code>fractal_server/app/runner/v1/_local/_submit_setup.py</code> <pre><code>def _local_submit_setup(\n    *,\n    wftask: WorkflowTask,\n    workflow_dir: Optional[Path] = None,\n    workflow_dir_user: Optional[Path] = None,\n) -&gt; dict[str, object]:\n    \"\"\"\n    Collect WorfklowTask-specific configuration parameters from different\n    sources, and inject them for execution.\n\n    Arguments:\n        wftask:\n            WorkflowTask for which the configuration is to be assembled\n        workflow_dir:\n            Not used in this function.\n        workflow_dir_user:\n            Not used in this function.\n\n    Returns:\n        submit_setup_dict:\n            A dictionary that will be passed on to\n            `FractalThreadPoolExecutor.submit` and\n            `FractalThreadPoolExecutor.map`, so as to set extra options.\n    \"\"\"\n\n    local_backend_config = get_local_backend_config(wftask=wftask)\n\n    return dict(local_backend_config=local_backend_config)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/executor/","title":"executor","text":"<p>Custom version of Python ThreadPoolExecutor).</p>"},{"location":"reference/fractal_server/app/runner/v1/_local/executor/#fractal_server.app.runner.v1._local.executor.FractalThreadPoolExecutor","title":"<code>FractalThreadPoolExecutor</code>","text":"<p>             Bases: <code>ThreadPoolExecutor</code></p> <p>Custom version of ThreadPoolExecutor) that overrides the <code>submit</code> and <code>map</code> methods</p> Source code in <code>fractal_server/app/runner/v1/_local/executor.py</code> <pre><code>class FractalThreadPoolExecutor(ThreadPoolExecutor):\n    \"\"\"\n    Custom version of\n    [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor))\n    that overrides the `submit` and `map` methods\n    \"\"\"\n\n    def submit(\n        self,\n        *args,\n        local_backend_config: Optional[LocalBackendConfig] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Compared to the `ThreadPoolExecutor` method, here we accept an addition\n        keyword argument (`local_backend_config`), which is then simply\n        ignored.\n        \"\"\"\n        return super().submit(*args, **kwargs)\n\n    def map(\n        self,\n        fn: Callable,\n        *iterables: Sequence[Iterable],\n        local_backend_config: Optional[LocalBackendConfig] = None,\n    ):\n        \"\"\"\n        Custom version of the `Executor.map` method\n\n        The main change with the respect to the original `map` method is that\n        the list of tasks to be executed is split into chunks, and then\n        `super().map` is called (sequentially) on each chunk. The goal of this\n        change is to limit parallelism, e.g. due to limited computational\n        resources.\n\n        Other changes from the `concurrent.futures` `map` method:\n\n        1. Removed `timeout` argument;\n        2. Removed `chunksize`;\n        3. All iterators (both inputs and output ones) are transformed into\n           lists.\n\n        Args:\n            fn: A callable function.\n            iterables: The argument iterables (one iterable per argument of\n                       `fn`).\n           local_backend_config: The backend configuration, needed to extract\n                                 `parallel_tasks_per_job`.\n        \"\"\"\n\n        # Preliminary check\n        iterable_lengths = [len(it) for it in iterables]\n        if not len(set(iterable_lengths)) == 1:\n            raise ValueError(\"Iterables have different lengths.\")\n\n        # Set total number of arguments\n        n_elements = len(iterables[0])\n\n        # Set parallel_tasks_per_job\n        if local_backend_config is None:\n            local_backend_config = get_default_local_backend_config()\n        parallel_tasks_per_job = local_backend_config.parallel_tasks_per_job\n        if parallel_tasks_per_job is None:\n            parallel_tasks_per_job = n_elements\n\n        # Execute tasks, in chunks of size parallel_tasks_per_job\n        results = []\n        for ind_chunk in range(0, n_elements, parallel_tasks_per_job):\n            chunk_iterables = [\n                it[ind_chunk : ind_chunk + parallel_tasks_per_job]  # noqa\n                for it in iterables\n            ]\n            map_iter = super().map(fn, *chunk_iterables)\n            results.extend(list(map_iter))\n\n        return iter(results)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/executor/#fractal_server.app.runner.v1._local.executor.FractalThreadPoolExecutor.map","title":"<code>map(fn, *iterables, local_backend_config=None)</code>","text":"<p>Custom version of the <code>Executor.map</code> method</p> <p>The main change with the respect to the original <code>map</code> method is that the list of tasks to be executed is split into chunks, and then <code>super().map</code> is called (sequentially) on each chunk. The goal of this change is to limit parallelism, e.g. due to limited computational resources.</p> <p>Other changes from the <code>concurrent.futures</code> <code>map</code> method:</p> <ol> <li>Removed <code>timeout</code> argument;</li> <li>Removed <code>chunksize</code>;</li> <li>All iterators (both inputs and output ones) are transformed into    lists.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>A callable function.</p> required <code>iterables</code> <code>Sequence[Iterable]</code> <p>The argument iterables (one iterable per argument of        <code>fn</code>).</p> <code>()</code> <p>local_backend_config: The backend configuration, needed to extract                          <code>parallel_tasks_per_job</code>.</p> Source code in <code>fractal_server/app/runner/v1/_local/executor.py</code> <pre><code>def map(\n    self,\n    fn: Callable,\n    *iterables: Sequence[Iterable],\n    local_backend_config: Optional[LocalBackendConfig] = None,\n):\n    \"\"\"\n    Custom version of the `Executor.map` method\n\n    The main change with the respect to the original `map` method is that\n    the list of tasks to be executed is split into chunks, and then\n    `super().map` is called (sequentially) on each chunk. The goal of this\n    change is to limit parallelism, e.g. due to limited computational\n    resources.\n\n    Other changes from the `concurrent.futures` `map` method:\n\n    1. Removed `timeout` argument;\n    2. Removed `chunksize`;\n    3. All iterators (both inputs and output ones) are transformed into\n       lists.\n\n    Args:\n        fn: A callable function.\n        iterables: The argument iterables (one iterable per argument of\n                   `fn`).\n       local_backend_config: The backend configuration, needed to extract\n                             `parallel_tasks_per_job`.\n    \"\"\"\n\n    # Preliminary check\n    iterable_lengths = [len(it) for it in iterables]\n    if not len(set(iterable_lengths)) == 1:\n        raise ValueError(\"Iterables have different lengths.\")\n\n    # Set total number of arguments\n    n_elements = len(iterables[0])\n\n    # Set parallel_tasks_per_job\n    if local_backend_config is None:\n        local_backend_config = get_default_local_backend_config()\n    parallel_tasks_per_job = local_backend_config.parallel_tasks_per_job\n    if parallel_tasks_per_job is None:\n        parallel_tasks_per_job = n_elements\n\n    # Execute tasks, in chunks of size parallel_tasks_per_job\n    results = []\n    for ind_chunk in range(0, n_elements, parallel_tasks_per_job):\n        chunk_iterables = [\n            it[ind_chunk : ind_chunk + parallel_tasks_per_job]  # noqa\n            for it in iterables\n        ]\n        map_iter = super().map(fn, *chunk_iterables)\n        results.extend(list(map_iter))\n\n    return iter(results)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_local/executor/#fractal_server.app.runner.v1._local.executor.FractalThreadPoolExecutor.submit","title":"<code>submit(*args, local_backend_config=None, **kwargs)</code>","text":"<p>Compared to the <code>ThreadPoolExecutor</code> method, here we accept an addition keyword argument (<code>local_backend_config</code>), which is then simply ignored.</p> Source code in <code>fractal_server/app/runner/v1/_local/executor.py</code> <pre><code>def submit(\n    self,\n    *args,\n    local_backend_config: Optional[LocalBackendConfig] = None,\n    **kwargs,\n):\n    \"\"\"\n    Compared to the `ThreadPoolExecutor` method, here we accept an addition\n    keyword argument (`local_backend_config`), which is then simply\n    ignored.\n    \"\"\"\n    return super().submit(*args, **kwargs)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_slurm/","title":"_slurm","text":"<p>Slurm Bakend</p> <p>This backend runs fractal workflows in a SLURM cluster using Clusterfutures Executor objects.</p>"},{"location":"reference/fractal_server/app/runner/v1/_slurm/#fractal_server.app.runner.v1._slurm._process_workflow","title":"<code>_process_workflow(*, workflow, input_paths, output_path, input_metadata, input_history, logger_name, workflow_dir, workflow_dir_user, first_task_index, last_task_index, slurm_user=None, slurm_account=None, user_cache_dir, worker_init=None)</code>","text":"<p>Internal processing routine for the SLURM backend</p> <p>This function initialises the a FractalSlurmExecutor, setting logging, workflow working dir and user to impersonate. It then schedules the workflow tasks and returns the output dataset metadata.</p> <p>Cf. process_workflow</p> <p>Returns:</p> Name Type Description <code>output_dataset_metadata</code> <code>dict[str, Any]</code> <p>Metadata of the output dataset</p> Source code in <code>fractal_server/app/runner/v1/_slurm/__init__.py</code> <pre><code>def _process_workflow(\n    *,\n    workflow: Workflow,\n    input_paths: list[Path],\n    output_path: Path,\n    input_metadata: dict[str, Any],\n    input_history: list[dict[str, Any]],\n    logger_name: str,\n    workflow_dir: Path,\n    workflow_dir_user: Path,\n    first_task_index: int,\n    last_task_index: int,\n    slurm_user: Optional[str] = None,\n    slurm_account: Optional[str] = None,\n    user_cache_dir: str,\n    worker_init: Optional[Union[str, list[str]]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Internal processing routine for the SLURM backend\n\n    This function initialises the a FractalSlurmExecutor, setting logging,\n    workflow working dir and user to impersonate. It then schedules the\n    workflow tasks and returns the output dataset metadata.\n\n    Cf.\n    [process_workflow][fractal_server.app.runner.v1._local.process_workflow]\n\n    Returns:\n        output_dataset_metadata: Metadata of the output dataset\n    \"\"\"\n\n    if not slurm_user:\n        raise RuntimeError(\n            \"slurm_user argument is required, for slurm backend\"\n        )\n\n    if isinstance(worker_init, str):\n        worker_init = worker_init.split(\"\\n\")\n\n    with FractalSlurmExecutor(\n        debug=True,\n        keep_logs=True,\n        slurm_user=slurm_user,\n        user_cache_dir=user_cache_dir,\n        working_dir=workflow_dir,\n        working_dir_user=workflow_dir_user,\n        common_script_lines=worker_init,\n        slurm_account=slurm_account,\n    ) as executor:\n        output_task_pars = execute_tasks(\n            executor=executor,\n            task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)  # noqa\n            ],  # noqa\n            task_pars=TaskParameters(\n                input_paths=input_paths,\n                output_path=output_path,\n                metadata=input_metadata,\n                history=input_history,\n            ),\n            workflow_dir=workflow_dir,\n            workflow_dir_user=workflow_dir_user,\n            submit_setup_call=_slurm_submit_setup,\n            logger_name=logger_name,\n        )\n    output_dataset_metadata_history = dict(\n        metadata=output_task_pars.metadata, history=output_task_pars.history\n    )\n    return output_dataset_metadata_history\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_slurm/#fractal_server.app.runner.v1._slurm.process_workflow","title":"<code>process_workflow(*, workflow, input_paths, output_path, input_metadata, input_history, logger_name, workflow_dir, workflow_dir_user=None, user_cache_dir=None, slurm_user=None, slurm_account=None, worker_init=None, first_task_index=None, last_task_index=None)</code>  <code>async</code>","text":"<p>Process workflow (SLURM backend public interface)</p> <p>Cf. process_workflow</p> Source code in <code>fractal_server/app/runner/v1/_slurm/__init__.py</code> <pre><code>async def process_workflow(\n    *,\n    workflow: Workflow,\n    input_paths: list[Path],\n    output_path: Path,\n    input_metadata: dict[str, Any],\n    input_history: list[dict[str, Any]],\n    logger_name: str,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    user_cache_dir: Optional[str] = None,\n    slurm_user: Optional[str] = None,\n    slurm_account: Optional[str] = None,\n    worker_init: Optional[str] = None,\n    first_task_index: Optional[int] = None,\n    last_task_index: Optional[int] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Process workflow (SLURM backend public interface)\n\n    Cf.\n    [process_workflow][fractal_server.app.runner.v1._local.process_workflow]\n    \"\"\"\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    output_dataset_metadata_history = await async_wrap(_process_workflow)(\n        workflow=workflow,\n        input_paths=input_paths,\n        output_path=output_path,\n        input_metadata=input_metadata,\n        input_history=input_history,\n        logger_name=logger_name,\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n        slurm_user=slurm_user,\n        slurm_account=slurm_account,\n        user_cache_dir=user_cache_dir,\n        worker_init=worker_init,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n    return output_dataset_metadata_history\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_slurm/_submit_setup/","title":"_submit_setup","text":"<p>Submodule to define _slurm_submit_setup, which is also the reference implementation of <code>submit_setup_call</code> in fractal_server.app.runner.v1._common).</p>"},{"location":"reference/fractal_server/app/runner/v1/_slurm/_submit_setup/#fractal_server.app.runner.v1._slurm._submit_setup._slurm_submit_setup","title":"<code>_slurm_submit_setup(*, wftask, workflow_dir, workflow_dir_user)</code>","text":"<p>Collect WorfklowTask-specific configuration parameters from different sources, and inject them for execution.</p> <p>Here goes all the logic for reading attributes from the appropriate sources and transforming them into an appropriate <code>SlurmConfig</code> object (encoding SLURM configuration) and <code>TaskFiles</code> object (with details e.g. about file paths or filename prefixes).</p> <p>For now, this is the reference implementation for the argument <code>submit_setup_call</code> of fractal_server.app.runner.v1._common.execute_tasks.</p> <p>Parameters:</p> Name Type Description Default <code>wftask</code> <code>WorkflowTask</code> <p>WorkflowTask for which the configuration is to be assembled</p> required <code>workflow_dir</code> <code>Path</code> <p>Server-owned directory to store all task-execution-related relevant files (inputs, outputs, errors, and all meta files related to the job execution). Note: users cannot write directly to this folder.</p> required <code>workflow_dir_user</code> <code>Path</code> <p>User-side directory with the same scope as <code>workflow_dir</code>, and where a user can write.</p> required <p>Returns:</p> Name Type Description <code>submit_setup_dict</code> <code>dict[str, object]</code> <p>A dictionary that will be passed on to <code>FractalSlurmExecutor.submit</code> and <code>FractalSlurmExecutor.map</code>, so as to set extra options.</p> Source code in <code>fractal_server/app/runner/v1/_slurm/_submit_setup.py</code> <pre><code>def _slurm_submit_setup(\n    *,\n    wftask: WorkflowTask,\n    workflow_dir: Path,\n    workflow_dir_user: Path,\n) -&gt; dict[str, object]:\n    \"\"\"\n    Collect WorfklowTask-specific configuration parameters from different\n    sources, and inject them for execution.\n\n    Here goes all the logic for reading attributes from the appropriate sources\n    and transforming them into an appropriate `SlurmConfig` object (encoding\n    SLURM configuration) and `TaskFiles` object (with details e.g. about file\n    paths or filename prefixes).\n\n    For now, this is the reference implementation for the argument\n    `submit_setup_call` of\n    [fractal_server.app.runner.v1._common.execute_tasks][].\n\n    Arguments:\n        wftask:\n            WorkflowTask for which the configuration is to be assembled\n        workflow_dir:\n            Server-owned directory to store all task-execution-related relevant\n            files (inputs, outputs, errors, and all meta files related to the\n            job execution). Note: users cannot write directly to this folder.\n        workflow_dir_user:\n            User-side directory with the same scope as `workflow_dir`, and\n            where a user can write.\n\n    Returns:\n        submit_setup_dict:\n            A dictionary that will be passed on to\n            `FractalSlurmExecutor.submit` and `FractalSlurmExecutor.map`, so\n            as to set extra options.\n    \"\"\"\n\n    # Get SlurmConfig object\n    slurm_config = get_slurm_config(\n        wftask=wftask,\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n    )\n\n    # Get TaskFiles object\n    task_files = get_task_file_paths(\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n        task_order=wftask.order,\n    )\n\n    # Prepare and return output dictionary\n    submit_setup_dict = dict(\n        slurm_config=slurm_config,\n        task_files=task_files,\n    )\n    return submit_setup_dict\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v1/_slurm/get_slurm_config/","title":"get_slurm_config","text":""},{"location":"reference/fractal_server/app/runner/v1/_slurm/get_slurm_config/#fractal_server.app.runner.v1._slurm.get_slurm_config.get_slurm_config","title":"<code>get_slurm_config(wftask, workflow_dir, workflow_dir_user, config_path=None)</code>","text":"<p>Prepare a <code>SlurmConfig</code> configuration object</p> <p>The sources for <code>SlurmConfig</code> attributes, in increasing priority order, are</p> <ol> <li>The general content of the Fractal SLURM configuration file.</li> <li>The GPU-specific content of the Fractal SLURM configuration file, if     appropriate.</li> <li>Properties in <code>wftask.meta</code> (which, for <code>WorkflowTask</code>s added through    <code>Workflow.insert_task</code>, also includes <code>wftask.task.meta</code>);</li> </ol> <p>Note: <code>wftask.meta</code> may be <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>wftask</code> <code>WorkflowTask</code> <p>WorkflowTask for which the SLURM configuration is is to be prepared.</p> required <code>workflow_dir</code> <code>Path</code> <p>Server-owned directory to store all task-execution-related relevant files (inputs, outputs, errors, and all meta files related to the job execution). Note: users cannot write directly to this folder.</p> required <code>workflow_dir_user</code> <code>Path</code> <p>User-side directory with the same scope as <code>workflow_dir</code>, and where a user can write.</p> required <code>config_path</code> <code>Optional[Path]</code> <p>Path of aFractal  SLURM configuration file; if <code>None</code>, use <code>FRACTAL_SLURM_CONFIG_FILE</code> variable from settings.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>slurm_config</code> <code>SlurmConfig</code> <p>The SlurmConfig object</p> Source code in <code>fractal_server/app/runner/v1/_slurm/get_slurm_config.py</code> <pre><code>def get_slurm_config(\n    wftask: WorkflowTask,\n    workflow_dir: Path,\n    workflow_dir_user: Path,\n    config_path: Optional[Path] = None,\n) -&gt; SlurmConfig:\n    \"\"\"\n    Prepare a `SlurmConfig` configuration object\n\n    The sources for `SlurmConfig` attributes, in increasing priority order, are\n\n    1. The general content of the Fractal SLURM configuration file.\n    2. The GPU-specific content of the Fractal SLURM configuration file, if\n        appropriate.\n    3. Properties in `wftask.meta` (which, for `WorkflowTask`s added through\n       `Workflow.insert_task`, also includes `wftask.task.meta`);\n\n    Note: `wftask.meta` may be `None`.\n\n    Arguments:\n        wftask:\n            WorkflowTask for which the SLURM configuration is is to be\n            prepared.\n        workflow_dir:\n            Server-owned directory to store all task-execution-related relevant\n            files (inputs, outputs, errors, and all meta files related to the\n            job execution). Note: users cannot write directly to this folder.\n        workflow_dir_user:\n            User-side directory with the same scope as `workflow_dir`, and\n            where a user can write.\n        config_path:\n            Path of aFractal  SLURM configuration file; if `None`, use\n            `FRACTAL_SLURM_CONFIG_FILE` variable from settings.\n\n    Returns:\n        slurm_config:\n            The SlurmConfig object\n    \"\"\"\n\n    logger.debug(\n        \"[get_slurm_config] WorkflowTask meta attribute: {wftask.meta=}\"\n    )\n\n    # Incorporate slurm_env.default_slurm_config\n    slurm_env = load_slurm_config_file(config_path=config_path)\n    slurm_dict = slurm_env.default_slurm_config.dict(\n        exclude_unset=True, exclude={\"mem\"}\n    )\n    if slurm_env.default_slurm_config.mem:\n        slurm_dict[\"mem_per_task_MB\"] = slurm_env.default_slurm_config.mem\n\n    # Incorporate slurm_env.batching_config\n    for key, value in slurm_env.batching_config.dict().items():\n        slurm_dict[key] = value\n\n    # Incorporate slurm_env.user_local_exports\n    slurm_dict[\"user_local_exports\"] = slurm_env.user_local_exports\n\n    logger.debug(\n        \"[get_slurm_config] Fractal SLURM configuration file: \"\n        f\"{slurm_env.dict()=}\"\n    )\n\n    # GPU-related options\n    # Notes about priority:\n    # 1. This block of definitions takes priority over other definitions from\n    #    slurm_env which are not under the `needs_gpu` subgroup\n    # 2. This block of definitions has lower priority than whatever comes next\n    #    (i.e. from WorkflowTask.meta).\n    if wftask.meta is not None:\n        needs_gpu = wftask.meta.get(\"needs_gpu\", False)\n    else:\n        needs_gpu = False\n    logger.debug(f\"[get_slurm_config] {needs_gpu=}\")\n    if needs_gpu:\n        for key, value in slurm_env.gpu_slurm_config.dict(\n            exclude_unset=True, exclude={\"mem\"}\n        ).items():\n            slurm_dict[key] = value\n        if slurm_env.gpu_slurm_config.mem:\n            slurm_dict[\"mem_per_task_MB\"] = slurm_env.gpu_slurm_config.mem\n\n    # Number of CPUs per task, for multithreading\n    if wftask.meta is not None and \"cpus_per_task\" in wftask.meta:\n        cpus_per_task = int(wftask.meta[\"cpus_per_task\"])\n        slurm_dict[\"cpus_per_task\"] = cpus_per_task\n\n    # Required memory per task, in MB\n    if wftask.meta is not None and \"mem\" in wftask.meta:\n        raw_mem = wftask.meta[\"mem\"]\n        mem_per_task_MB = _parse_mem_value(raw_mem)\n        slurm_dict[\"mem_per_task_MB\"] = mem_per_task_MB\n\n    # Job name\n    job_name = wftask.task.name.replace(\" \", \"_\")\n    slurm_dict[\"job_name\"] = job_name\n\n    # Optional SLURM arguments and extra lines\n    if wftask.meta is not None:\n        account = wftask.meta.get(\"account\", None)\n        if account is not None:\n            error_msg = (\n                f\"Invalid {account=} property in WorkflowTask `meta` \"\n                \"attribute.\\n\"\n                \"SLURM account must be set in the request body of the \"\n                \"apply-workflow endpoint, or by modifying the user properties.\"\n            )\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        for key in [\"time\", \"gres\", \"constraint\"]:\n            value = wftask.meta.get(key, None)\n            if value:\n                slurm_dict[key] = value\n    if wftask.meta is not None:\n        extra_lines = wftask.meta.get(\"extra_lines\", [])\n    else:\n        extra_lines = []\n    extra_lines = slurm_dict.get(\"extra_lines\", []) + extra_lines\n    if len(set(extra_lines)) != len(extra_lines):\n        logger.debug(\n            \"[get_slurm_config] Removing repeated elements \"\n            f\"from {extra_lines=}.\"\n        )\n        extra_lines = list(set(extra_lines))\n    slurm_dict[\"extra_lines\"] = extra_lines\n\n    # Job-batching parameters (if None, they will be determined heuristically)\n    if wftask.meta is not None:\n        tasks_per_job = wftask.meta.get(\"tasks_per_job\", None)\n        parallel_tasks_per_job = wftask.meta.get(\n            \"parallel_tasks_per_job\", None\n        )\n    else:\n        tasks_per_job = None\n        parallel_tasks_per_job = None\n    slurm_dict[\"tasks_per_job\"] = tasks_per_job\n    slurm_dict[\"parallel_tasks_per_job\"] = parallel_tasks_per_job\n\n    # Put everything together\n    logger.debug(\n        \"[get_slurm_config] Now create a SlurmConfig object based \"\n        f\"on {slurm_dict=}\"\n    )\n    slurm_config = SlurmConfig(**slurm_dict)\n\n    return slurm_config\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/","title":"v2","text":"<p>Runner backend subsystem root V2</p> <p>This module is the single entry point to the runner backend subsystem V2. Other subystems should only import this module and not its submodules or the individual backends.</p>"},{"location":"reference/fractal_server/app/runner/v2/#fractal_server.app.runner.v2.submit_workflow","title":"<code>submit_workflow(*, workflow_id, dataset_id, job_id, worker_init=None, slurm_user=None, user_cache_dir=None)</code>  <code>async</code>","text":"<p>Prepares a workflow and applies it to a dataset</p> <p>This function wraps the process_workflow one, which is different for each backend (e.g. local or slurm backend).</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>int</code> <p>ID of the workflow being applied</p> required <code>dataset_id</code> <code>int</code> <p>Dataset ID</p> required <code>job_id</code> <code>int</code> <p>Id of the job record which stores the state for the current workflow application.</p> required <code>worker_init</code> <code>Optional[str]</code> <p>Custom executor parameters that get parsed before the execution of each task.</p> <code>None</code> <code>user_cache_dir</code> <code>Optional[str]</code> <p>Cache directory (namely a path where the user can write); for the slurm backend, this is used as a base directory for <code>job.working_dir_user</code>.</p> <code>None</code> <code>slurm_user</code> <code>Optional[str]</code> <p>The username to impersonate for the workflow execution, for the slurm backend.</p> <code>None</code> Source code in <code>fractal_server/app/runner/v2/__init__.py</code> <pre><code>async def submit_workflow(\n    *,\n    workflow_id: int,\n    dataset_id: int,\n    job_id: int,\n    worker_init: Optional[str] = None,\n    slurm_user: Optional[str] = None,\n    user_cache_dir: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Prepares a workflow and applies it to a dataset\n\n    This function wraps the process_workflow one, which is different for each\n    backend (e.g. local or slurm backend).\n\n    Args:\n        workflow_id:\n            ID of the workflow being applied\n        dataset_id:\n            Dataset ID\n        job_id:\n            Id of the job record which stores the state for the current\n            workflow application.\n        worker_init:\n            Custom executor parameters that get parsed before the execution of\n            each task.\n        user_cache_dir:\n            Cache directory (namely a path where the user can write); for the\n            slurm backend, this is used as a base directory for\n            `job.working_dir_user`.\n        slurm_user:\n            The username to impersonate for the workflow execution, for the\n            slurm backend.\n    \"\"\"\n\n    # Declare runner backend and set `process_workflow` function\n    settings = Inject(get_settings)\n    FRACTAL_RUNNER_BACKEND = settings.FRACTAL_RUNNER_BACKEND\n    if FRACTAL_RUNNER_BACKEND == \"local\":\n        process_workflow = local_process_workflow\n    elif FRACTAL_RUNNER_BACKEND == \"slurm\":\n        process_workflow = slurm_process_workflow\n    else:\n        raise RuntimeError(f\"Invalid runner backend {FRACTAL_RUNNER_BACKEND=}\")\n\n    with next(DB.get_sync_db()) as db_sync:\n\n        job: JobV2 = db_sync.get(JobV2, job_id)\n        if not job:\n            raise ValueError(f\"Cannot fetch job {job_id} from database\")\n\n        dataset: DatasetV2 = db_sync.get(DatasetV2, dataset_id)\n        workflow: WorkflowV2 = db_sync.get(WorkflowV2, workflow_id)\n        if not (dataset and workflow):\n            log_msg = \"\"\n            if not dataset:\n                log_msg += f\"Cannot fetch dataset {dataset_id} from database\\n\"\n            if not workflow:\n                log_msg += (\n                    f\"Cannot fetch workflow {workflow_id} from database\\n\"\n                )\n            job.status = JobStatusTypeV2.FAILED\n            job.end_timestamp = get_timestamp()\n            job.log = log_msg\n            db_sync.merge(job)\n            db_sync.commit()\n            db_sync.close()\n            return\n\n        # Define and create server-side working folder\n        project_id = workflow.project_id\n        timestamp_string = get_timestamp().strftime(\"%Y%m%d_%H%M%S\")\n        WORKFLOW_DIR = (\n            settings.FRACTAL_RUNNER_WORKING_BASE_DIR\n            / (\n                f\"proj_{project_id:07d}_wf_{workflow_id:07d}_job_{job_id:07d}\"\n                f\"_{timestamp_string}\"\n            )\n        ).resolve()\n\n        if WORKFLOW_DIR.exists():\n            raise RuntimeError(f\"Workflow dir {WORKFLOW_DIR} already exists.\")\n\n        # Create WORKFLOW_DIR with 755 permissions\n        original_umask = os.umask(0)\n        WORKFLOW_DIR.mkdir(parents=True, mode=0o755)\n        os.umask(original_umask)\n\n        # Define and create user-side working folder, if needed\n        if FRACTAL_RUNNER_BACKEND == \"local\":\n            WORKFLOW_DIR_USER = WORKFLOW_DIR\n        elif FRACTAL_RUNNER_BACKEND == \"slurm\":\n\n            from ..executors.slurm._subprocess_run_as_user import (\n                _mkdir_as_user,\n            )\n\n            WORKFLOW_DIR_USER = (\n                Path(user_cache_dir) / f\"{WORKFLOW_DIR.name}\"\n            ).resolve()\n            _mkdir_as_user(folder=str(WORKFLOW_DIR_USER), user=slurm_user)\n        else:\n            raise ValueError(f\"{FRACTAL_RUNNER_BACKEND=} not supported\")\n\n        # Update db\n        job.working_dir = WORKFLOW_DIR.as_posix()\n        job.working_dir_user = WORKFLOW_DIR_USER.as_posix()\n        db_sync.merge(job)\n        db_sync.commit()\n\n        # After Session.commit() is called, either explicitly or when using a\n        # context manager, all objects associated with the Session are expired.\n        # https://docs.sqlalchemy.org/en/14/orm/\n        #   session_basics.html#opening-and-closing-a-session\n        # https://docs.sqlalchemy.org/en/14/orm/\n        #   session_state_management.html#refreshing-expiring\n\n        # See issue #928:\n        # https://github.com/fractal-analytics-platform/\n        #   fractal-server/issues/928\n\n        db_sync.refresh(dataset)\n        db_sync.refresh(workflow)\n\n        # Write logs\n        logger_name = f\"WF{workflow_id}_job{job_id}\"\n        log_file_path = WORKFLOW_DIR / WORKFLOW_LOG_FILENAME\n        logger = set_logger(\n            logger_name=logger_name,\n            log_file_path=log_file_path,\n        )\n        logger.info(\n            f'Start execution of workflow \"{workflow.name}\"; '\n            f\"more logs at {str(log_file_path)}\"\n        )\n        logger.debug(f\"fractal_server.__VERSION__: {__VERSION__}\")\n        logger.debug(f\"FRACTAL_RUNNER_BACKEND: {FRACTAL_RUNNER_BACKEND}\")\n        logger.debug(f\"slurm_user: {slurm_user}\")\n        logger.debug(f\"slurm_account: {job.slurm_account}\")\n        logger.debug(f\"worker_init: {worker_init}\")\n        logger.debug(f\"job.id: {job.id}\")\n        logger.debug(f\"job.working_dir: {job.working_dir}\")\n        logger.debug(f\"job.working_dir_user: {job.working_dir_user}\")\n        logger.debug(f\"job.first_task_index: {job.first_task_index}\")\n        logger.debug(f\"job.last_task_index: {job.last_task_index}\")\n        logger.debug(f'START workflow \"{workflow.name}\"')\n\n    try:\n        # \"The Session.close() method does not prevent the Session from being\n        # used again. The Session itself does not actually have a distinct\n        # \u201cclosed\u201d state; it merely means the Session will release all database\n        # connections and ORM objects.\"\n        # (https://docs.sqlalchemy.org/en/20/orm/session_api.html#sqlalchemy.orm.Session.close).\n        #\n        # We close the session before the (possibly long) process_workflow\n        # call, to make sure all DB connections are released. The reason why we\n        # are not using a context manager within the try block is that we also\n        # need access to db_sync in the except branches.\n        db_sync = next(DB.get_sync_db())\n        db_sync.close()\n\n        new_dataset_attributes = await process_workflow(\n            workflow=workflow,\n            dataset=dataset,\n            slurm_user=slurm_user,\n            slurm_account=job.slurm_account,\n            user_cache_dir=user_cache_dir,\n            workflow_dir=WORKFLOW_DIR,\n            workflow_dir_user=WORKFLOW_DIR_USER,\n            logger_name=logger_name,\n            worker_init=worker_init,\n            first_task_index=job.first_task_index,\n            last_task_index=job.last_task_index,\n        )\n\n        logger.info(\n            f'End execution of workflow \"{workflow.name}\"; '\n            f\"more logs at {str(log_file_path)}\"\n        )\n        logger.debug(f'END workflow \"{workflow.name}\"')\n\n        # Update dataset attributes, in case of successful execution\n        dataset.history.extend(new_dataset_attributes[\"history\"])\n        dataset.filters = new_dataset_attributes[\"filters\"]\n        dataset.images = new_dataset_attributes[\"images\"]\n        for attribute_name in [\"filters\", \"history\", \"images\"]:\n            flag_modified(dataset, attribute_name)\n        db_sync.merge(dataset)\n\n        # Update job DB entry\n        job.status = JobStatusTypeV2.DONE\n        job.end_timestamp = get_timestamp()\n        with log_file_path.open(\"r\") as f:\n            logs = f.read()\n        job.log = logs\n        db_sync.merge(job)\n        db_sync.commit()\n\n    except TaskExecutionError as e:\n\n        logger.debug(f'FAILED workflow \"{workflow.name}\", TaskExecutionError.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (TaskExecutionError).')\n\n        # Read dataset attributes produced by the last successful task, and\n        # update the DB dataset accordingly\n        failed_wftask = db_sync.get(WorkflowTaskV2, e.workflow_task_id)\n        dataset.history = assemble_history_failed_job(\n            job,\n            dataset,\n            workflow,\n            logger_name=logger_name,\n            failed_wftask=failed_wftask,\n        )\n        latest_filters = assemble_filters_failed_job(job)\n        if latest_filters is not None:\n            dataset.filters = latest_filters\n        latest_images = assemble_images_failed_job(job)\n        if latest_images is not None:\n            dataset.images = latest_images\n        db_sync.merge(dataset)\n\n        job.status = JobStatusTypeV2.FAILED\n        job.end_timestamp = get_timestamp()\n\n        exception_args_string = \"\\n\".join(e.args)\n        job.log = (\n            f\"TASK ERROR: \"\n            f\"Task name: {e.task_name}, \"\n            f\"position in Workflow: {e.workflow_task_order}\\n\"\n            f\"TRACEBACK:\\n{exception_args_string}\"\n        )\n        db_sync.merge(job)\n        db_sync.commit()\n\n    except JobExecutionError as e:\n\n        logger.debug(f'FAILED workflow \"{workflow.name}\", JobExecutionError.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (JobExecutionError).')\n\n        # Read dataset attributes produced by the last successful task, and\n        # update the DB dataset accordingly\n        dataset.history = assemble_history_failed_job(\n            job,\n            dataset,\n            workflow,\n            logger_name=logger_name,\n        )\n        latest_filters = assemble_filters_failed_job(job)\n        if latest_filters is not None:\n            dataset.filters = latest_filters\n        latest_images = assemble_images_failed_job(job)\n        if latest_images is not None:\n            dataset.images = latest_images\n        db_sync.merge(dataset)\n\n        job.status = JobStatusTypeV2.FAILED\n        job.end_timestamp = get_timestamp()\n        error = e.assemble_error()\n        job.log = f\"JOB ERROR in Fractal job {job.id}:\\nTRACEBACK:\\n{error}\"\n        db_sync.merge(job)\n        db_sync.commit()\n\n    except Exception:\n\n        logger.debug(f'FAILED workflow \"{workflow.name}\", unknown error.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (unkwnon error).')\n\n        current_traceback = traceback.format_exc()\n\n        # Read dataset attributes produced by the last successful task, and\n        # update the DB dataset accordingly\n        dataset.history = assemble_history_failed_job(\n            job,\n            dataset,\n            workflow,\n            logger_name=logger_name,\n        )\n        latest_filters = assemble_filters_failed_job(job)\n        if latest_filters is not None:\n            dataset.filters = latest_filters\n        latest_images = assemble_images_failed_job(job)\n        if latest_images is not None:\n            dataset.images = latest_images\n        db_sync.merge(dataset)\n\n        job.status = JobStatusTypeV2.FAILED\n        job.end_timestamp = get_timestamp()\n        job.log = (\n            f\"UNKNOWN ERROR in Fractal job {job.id}\\n\"\n            f\"TRACEBACK:\\n{current_traceback}\"\n        )\n        db_sync.merge(job)\n        db_sync.commit()\n    finally:\n        close_logger(logger)\n        db_sync.close()\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/deduplicate_list/","title":"deduplicate_list","text":""},{"location":"reference/fractal_server/app/runner/v2/deduplicate_list/#fractal_server.app.runner.v2.deduplicate_list.deduplicate_list","title":"<code>deduplicate_list(this_list)</code>","text":"<p>Custom replacement for <code>set(this_list)</code>, when items are non-hashable.</p> Source code in <code>fractal_server/app/runner/v2/deduplicate_list.py</code> <pre><code>def deduplicate_list(\n    this_list: list[T],\n) -&gt; list[T]:\n    \"\"\"\n    Custom replacement for `set(this_list)`, when items are non-hashable.\n    \"\"\"\n    new_list_dict = []\n    new_list_objs = []\n    for this_obj in this_list:\n        this_dict = this_obj.dict()\n        if this_dict not in new_list_dict:\n            new_list_dict.append(this_dict)\n            new_list_objs.append(this_obj)\n    return new_list_objs\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/handle_failed_job/","title":"handle_failed_job","text":"<p>Helper functions to handle Dataset history.</p>"},{"location":"reference/fractal_server/app/runner/v2/handle_failed_job/#fractal_server.app.runner.v2.handle_failed_job.assemble_filters_failed_job","title":"<code>assemble_filters_failed_job(job)</code>","text":"<p>Assemble <code>DatasetV2.filters</code> for a failed workflow-execution.</p> <p>Assemble new value of <code>filters</code> based on the last successful task, i.e. based on the content of the temporary <code>FILTERS_FILENAME</code> file. If the file is missing, return <code>None</code>.</p> Argumentss <p>job:     The failed <code>JobV2</code> object.</p> <p>Returns:</p> Type Description <code>Optional[dict[str, Any]]</code> <p>The new value of <code>dataset.filters</code>, or <code>None</code> if <code>FILTERS_FILENAME</code></p> <code>Optional[dict[str, Any]]</code> <p>is missing.</p> Source code in <code>fractal_server/app/runner/v2/handle_failed_job.py</code> <pre><code>def assemble_filters_failed_job(job: JobV2) -&gt; Optional[dict[str, Any]]:\n    \"\"\"\n    Assemble `DatasetV2.filters` for a failed workflow-execution.\n\n    Assemble new value of `filters` based on the last successful task, i.e.\n    based on the content of the temporary `FILTERS_FILENAME` file. If the file\n    is missing, return `None`.\n\n    Argumentss:\n        job:\n            The failed `JobV2` object.\n\n    Returns:\n        The new value of `dataset.filters`, or `None` if `FILTERS_FILENAME`\n        is missing.\n    \"\"\"\n    tmp_file = Path(job.working_dir) / FILTERS_FILENAME\n    try:\n        with tmp_file.open(\"r\") as f:\n            new_filters = json.load(f)\n        return new_filters\n    except FileNotFoundError:\n        return None\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/handle_failed_job/#fractal_server.app.runner.v2.handle_failed_job.assemble_history_failed_job","title":"<code>assemble_history_failed_job(job, dataset, workflow, logger_name=None, failed_wftask=None)</code>","text":"<p>Assemble <code>history</code> after a workflow-execution job fails.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobV2</code> <p>The failed <code>JobV2</code> object.</p> required <code>dataset</code> <code>DatasetV2</code> <p>The <code>DatasetV2</code> object associated to <code>job</code>.</p> required <code>workflow</code> <code>WorkflowV2</code> <p>The <code>WorkflowV2</code> object associated to <code>job</code>.</p> required <code>logger_name</code> <code>Optional[str]</code> <p>A logger name.</p> <code>None</code> <code>failed_wftask</code> <code>Optional[WorkflowTaskV2]</code> <p>If set, append it to <code>history</code> during step 3; if <code>None</code>, infer it by comparing the job task list and the one in <code>HISTORY_FILENAME</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>The new value of <code>history</code>, to be merged into</p> <code>list[dict[str, Any]]</code> <p><code>dataset.meta</code>.</p> Source code in <code>fractal_server/app/runner/v2/handle_failed_job.py</code> <pre><code>def assemble_history_failed_job(\n    job: JobV2,\n    dataset: DatasetV2,\n    workflow: WorkflowV2,\n    logger_name: Optional[str] = None,\n    failed_wftask: Optional[WorkflowTaskV2] = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Assemble `history` after a workflow-execution job fails.\n\n    Args:\n        job:\n            The failed `JobV2` object.\n        dataset:\n            The `DatasetV2` object associated to `job`.\n        workflow:\n            The `WorkflowV2` object associated to `job`.\n        logger_name: A logger name.\n        failed_wftask:\n            If set, append it to `history` during step 3; if `None`, infer\n            it by comparing the job task list and the one in\n            `HISTORY_FILENAME`.\n\n    Returns:\n        The new value of `history`, to be merged into\n        `dataset.meta`.\n    \"\"\"\n\n    logger = logging.getLogger(logger_name)\n\n    # The final value of the history attribute should include up to three\n    # parts, coming from: the database, the temporary file, the failed-task\n    # information.\n\n    # Part 1: Read exising history from DB\n    new_history = dataset.history\n\n    # Part 2: Extend history based on temporary-file contents\n    tmp_history_file = Path(job.working_dir) / HISTORY_FILENAME\n    try:\n        with tmp_history_file.open(\"r\") as f:\n            tmp_file_history = json.load(f)\n            new_history.extend(tmp_file_history)\n    except FileNotFoundError:\n        tmp_file_history = []\n\n    # Part 3/A: Identify failed task, if needed\n    if failed_wftask is None:\n        job_wftasks = workflow.task_list[\n            job.first_task_index : (job.last_task_index + 1)  # noqa\n        ]\n        tmp_file_wftasks = [\n            history_item[\"workflowtask\"] for history_item in tmp_file_history\n        ]\n        if len(job_wftasks) &lt;= len(tmp_file_wftasks):\n            n_tasks_job = len(job_wftasks)\n            n_tasks_tmp = len(tmp_file_wftasks)\n            logger.error(\n                \"Cannot identify the failed task based on job task list \"\n                f\"(length {n_tasks_job}) and temporary-file task list \"\n                f\"(length {n_tasks_tmp}).\"\n            )\n            logger.error(\"Failed task not appended to history.\")\n        else:\n            failed_wftask = job_wftasks[len(tmp_file_wftasks)]\n\n    # Part 3/B: Append failed task to history\n    if failed_wftask is not None:\n        failed_wftask_dump = failed_wftask.model_dump(\n            exclude={\"task\", \"task_legacy\"}\n        )\n        if failed_wftask.is_legacy_task:\n            failed_wftask_dump[\n                \"task_legacy\"\n            ] = failed_wftask.task_legacy.model_dump()\n        else:\n            failed_wftask_dump[\"task\"] = failed_wftask.task.model_dump()\n        new_history_item = dict(\n            workflowtask=failed_wftask_dump,\n            status=WorkflowTaskStatusTypeV2.FAILED,\n            parallelization=dict(),  # FIXME: re-include parallelization\n        )\n        new_history.append(new_history_item)\n\n    return new_history\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/handle_failed_job/#fractal_server.app.runner.v2.handle_failed_job.assemble_images_failed_job","title":"<code>assemble_images_failed_job(job)</code>","text":"<p>Assemble <code>DatasetV2.images</code> for a failed workflow-execution.</p> <p>Assemble new value of <code>images</code> based on the last successful task, i.e. based on the content of the temporary <code>IMAGES_FILENAME</code> file. If the file is missing, return <code>None</code>.</p> Argumentss <p>job:     The failed <code>JobV2</code> object.</p> <p>Returns:</p> Type Description <code>Optional[dict[str, Any]]</code> <p>The new value of <code>dataset.images</code>, or <code>None</code> if <code>IMAGES_FILENAME</code></p> <code>Optional[dict[str, Any]]</code> <p>is missing.</p> Source code in <code>fractal_server/app/runner/v2/handle_failed_job.py</code> <pre><code>def assemble_images_failed_job(job: JobV2) -&gt; Optional[dict[str, Any]]:\n    \"\"\"\n    Assemble `DatasetV2.images` for a failed workflow-execution.\n\n    Assemble new value of `images` based on the last successful task, i.e.\n    based on the content of the temporary `IMAGES_FILENAME` file. If the file\n    is missing, return `None`.\n\n    Argumentss:\n        job:\n            The failed `JobV2` object.\n\n    Returns:\n        The new value of `dataset.images`, or `None` if `IMAGES_FILENAME`\n        is missing.\n    \"\"\"\n    tmp_file = Path(job.working_dir) / IMAGES_FILENAME\n    try:\n        with tmp_file.open(\"r\") as f:\n            new_images = json.load(f)\n        return new_images\n    except FileNotFoundError:\n        return None\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/merge_outputs/","title":"merge_outputs","text":""},{"location":"reference/fractal_server/app/runner/v2/runner/","title":"runner","text":""},{"location":"reference/fractal_server/app/runner/v2/runner_functions/","title":"runner_functions","text":""},{"location":"reference/fractal_server/app/runner/v2/runner_functions/#fractal_server.app.runner.v2.runner_functions.no_op_submit_setup_call","title":"<code>no_op_submit_setup_call(*, wftask, workflow_dir, workflow_dir_user, which_type)</code>","text":"<p>Default (no-operation) interface of submit_setup_call in V2.</p> Source code in <code>fractal_server/app/runner/v2/runner_functions.py</code> <pre><code>def no_op_submit_setup_call(\n    *,\n    wftask: WorkflowTaskV2,\n    workflow_dir: Path,\n    workflow_dir_user: Path,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n) -&gt; dict:\n    \"\"\"\n    Default (no-operation) interface of submit_setup_call in V2.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/runner_functions/#fractal_server.app.runner.v2.runner_functions.run_v2_task_non_parallel","title":"<code>run_v2_task_non_parallel(*, images, zarr_dir, task, wftask, workflow_dir, workflow_dir_user=None, executor, logger_name=None, submit_setup_call=no_op_submit_setup_call)</code>","text":"<p>This runs server-side (see <code>executor</code> argument)</p> Source code in <code>fractal_server/app/runner/v2/runner_functions.py</code> <pre><code>def run_v2_task_non_parallel(\n    *,\n    images: list[dict[str, Any]],\n    zarr_dir: str,\n    task: TaskV2,\n    wftask: WorkflowTaskV2,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    executor: Executor,\n    logger_name: Optional[str] = None,\n    submit_setup_call: Callable = no_op_submit_setup_call,\n) -&gt; TaskOutput:\n    \"\"\"\n    This runs server-side (see `executor` argument)\n    \"\"\"\n\n    if workflow_dir_user is None:\n        workflow_dir_user = workflow_dir\n        logging.warning(\n            \"In `run_single_task`, workflow_dir_user=None. Is this right?\"\n        )\n        workflow_dir_user = workflow_dir\n\n    executor_options = _get_executor_options(\n        wftask=wftask,\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n        submit_setup_call=submit_setup_call,\n        which_type=\"non_parallel\",\n    )\n\n    function_kwargs = dict(\n        zarr_urls=[image[\"zarr_url\"] for image in images],\n        zarr_dir=zarr_dir,\n        **(wftask.args_non_parallel or {}),\n    )\n    future = executor.submit(\n        functools.partial(\n            run_single_task,\n            wftask=wftask,\n            command=task.command_non_parallel,\n            workflow_dir=workflow_dir,\n            workflow_dir_user=workflow_dir_user,\n        ),\n        function_kwargs,\n        **executor_options,\n    )\n    output = future.result()\n    # FIXME V2: handle validation errors\n    if output is None:\n        return TaskOutput()\n    else:\n        validated_output = TaskOutput(**output)\n        return validated_output\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/runner_functions_low_level/","title":"runner_functions_low_level","text":""},{"location":"reference/fractal_server/app/runner/v2/runner_functions_low_level/#fractal_server.app.runner.v2.runner_functions_low_level._call_command_wrapper","title":"<code>_call_command_wrapper(cmd, log_path)</code>","text":"<p>Call a command and write its stdout and stderr to files</p> <p>Raises:</p> Type Description <code>TaskExecutionError</code> <p>If the <code>subprocess.run</code> call returns a positive                 exit code</p> <code>JobExecutionError</code> <p>If the <code>subprocess.run</code> call returns a negative                 exit code (e.g. due to the subprocess receiving a                 TERM or KILL signal)</p> Source code in <code>fractal_server/app/runner/v2/runner_functions_low_level.py</code> <pre><code>def _call_command_wrapper(cmd: str, log_path: Path) -&gt; None:\n    \"\"\"\n    Call a command and write its stdout and stderr to files\n\n    Raises:\n        TaskExecutionError: If the `subprocess.run` call returns a positive\n                            exit code\n        JobExecutionError:  If the `subprocess.run` call returns a negative\n                            exit code (e.g. due to the subprocess receiving a\n                            TERM or KILL signal)\n    \"\"\"\n\n    # Verify that task command is executable\n    if shutil.which(shlex_split(cmd)[0]) is None:\n        msg = (\n            f'Command \"{shlex_split(cmd)[0]}\" is not valid. '\n            \"Hint: make sure that it is executable.\"\n        )\n        raise TaskExecutionError(msg)\n\n    fp_log = open(log_path, \"w\")\n    try:\n        result = subprocess.run(  # nosec\n            shlex_split(cmd),\n            stderr=fp_log,\n            stdout=fp_log,\n        )\n    except Exception as e:\n        raise e\n    finally:\n        fp_log.close()\n\n    if result.returncode &gt; 0:\n        with log_path.open(\"r\") as fp_stderr:\n            err = fp_stderr.read()\n        raise TaskExecutionError(err)\n    elif result.returncode &lt; 0:\n        raise JobExecutionError(\n            info=f\"Task failed with returncode={result.returncode}\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/runner_functions_low_level/#fractal_server.app.runner.v2.runner_functions_low_level.run_single_task","title":"<code>run_single_task(args, command, wftask, workflow_dir, workflow_dir_user=None, logger_name=None, is_task_v1=False)</code>","text":"<p>Runs within an executor.</p> Source code in <code>fractal_server/app/runner/v2/runner_functions_low_level.py</code> <pre><code>def run_single_task(\n    args: dict[str, Any],\n    command: str,\n    wftask: WorkflowTaskV2,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    logger_name: Optional[str] = None,\n    is_task_v1: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Runs within an executor.\n    \"\"\"\n\n    logger = logging.getLogger(logger_name)\n    logger.debug(f\"Now start running {command=}\")\n\n    if not workflow_dir_user:\n        workflow_dir_user = workflow_dir\n\n    component = args.pop(_COMPONENT_KEY_, None)\n    if component is None:\n        task_files = get_task_file_paths(\n            workflow_dir=workflow_dir,\n            workflow_dir_user=workflow_dir_user,\n            task_order=wftask.order,\n        )\n    else:\n        task_files = get_task_file_paths(\n            workflow_dir=workflow_dir,\n            workflow_dir_user=workflow_dir_user,\n            task_order=wftask.order,\n            component=component,\n        )\n\n    # Write arguments to args.json file\n    with task_files.args.open(\"w\") as f:\n        json.dump(args, f, indent=2)\n\n    # Assemble full command\n    if is_task_v1:\n        full_command = (\n            f\"{command} \"\n            f\"--json {task_files.args.as_posix()} \"\n            f\"--metadata-out {task_files.metadiff.as_posix()}\"\n        )\n    else:\n        full_command = (\n            f\"{command} \"\n            f\"--args-json {task_files.args.as_posix()} \"\n            f\"--out-json {task_files.metadiff.as_posix()}\"\n        )\n\n    try:\n        _call_command_wrapper(\n            full_command,\n            log_path=task_files.log,\n        )\n    except TaskExecutionError as e:\n        e.workflow_task_order = wftask.order\n        e.workflow_task_id = wftask.id\n        if wftask.is_legacy_task:\n            e.task_name = wftask.task_legacy.name\n        else:\n            e.task_name = wftask.task.name\n        raise e\n\n    try:\n        with task_files.metadiff.open(\"r\") as f:\n            out_meta = json.load(f)\n    except FileNotFoundError as e:\n        logger.debug(\n            \"Task did not produce output metadata. \"\n            f\"Original FileNotFoundError: {str(e)}\"\n        )\n        out_meta = None\n\n    if out_meta == {}:\n        return None\n    return out_meta\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/task_interface/","title":"task_interface","text":""},{"location":"reference/fractal_server/app/runner/v2/v1_compat/","title":"v1_compat","text":""},{"location":"reference/fractal_server/app/runner/v2/_local/","title":"_local","text":"<p>Local Bakend</p> <p>This backend runs Fractal workflows using <code>FractalThreadPoolExecutor</code> (a custom version of Python ThreadPoolExecutor) to run tasks in several threads. Incidentally, it also represents the reference implementation for a backend.</p>"},{"location":"reference/fractal_server/app/runner/v2/_local/#fractal_server.app.runner.v2._local._process_workflow","title":"<code>_process_workflow(*, workflow, dataset, logger_name, workflow_dir, first_task_index, last_task_index)</code>","text":"<p>Internal processing routine</p> <p>Schedules the workflow using a <code>FractalThreadPoolExecutor</code>.</p> <p>Cf. process_workflow for the call signature.</p> Source code in <code>fractal_server/app/runner/v2/_local/__init__.py</code> <pre><code>def _process_workflow(\n    *,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    logger_name: str,\n    workflow_dir: Path,\n    first_task_index: int,\n    last_task_index: int,\n) -&gt; dict:\n    \"\"\"\n    Internal processing routine\n\n    Schedules the workflow using a `FractalThreadPoolExecutor`.\n\n    Cf.\n    [process_workflow][fractal_server.app.runner.v2._local.process_workflow]\n    for the call signature.\n    \"\"\"\n\n    with FractalThreadPoolExecutor() as executor:\n        new_dataset_attributes = execute_tasks_v2(\n            wf_task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)  # noqa\n            ],  # noqa\n            dataset=dataset,\n            executor=executor,\n            workflow_dir=workflow_dir,\n            workflow_dir_user=workflow_dir,\n            logger_name=logger_name,\n            submit_setup_call=_local_submit_setup,\n        )\n    return new_dataset_attributes\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_local/#fractal_server.app.runner.v2._local.process_workflow","title":"<code>process_workflow(*, workflow, dataset, workflow_dir, workflow_dir_user=None, first_task_index=None, last_task_index=None, logger_name, user_cache_dir=None, slurm_user=None, slurm_account=None, worker_init=None)</code>  <code>async</code>","text":"<p>Run a workflow</p> <p>This function is responsible for running a workflow on some input data, saving the output and taking care of any exception raised during the run.</p> <p>NOTE: This is the <code>local</code> backend's public interface, which also works as a reference implementation for other backends.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>WorkflowV2</code> <p>The workflow to be run</p> required <code>dataset</code> <code>DatasetV2</code> <p>Initial dataset.</p> required <code>workflow_dir</code> <code>Path</code> <p>Working directory for this run.</p> required <code>workflow_dir_user</code> <code>Optional[Path]</code> <p>Working directory for this run, on the user side. This argument is present for compatibility with the standard backend interface, but for the <code>local</code> backend it cannot be different from <code>workflow_dir</code>.</p> <code>None</code> <code>first_task_index</code> <code>Optional[int]</code> <p>Positional index of the first task to execute; if <code>None</code>, start from <code>0</code>.</p> <code>None</code> <code>last_task_index</code> <code>Optional[int]</code> <p>Positional index of the last task to execute; if <code>None</code>, proceed until the last task.</p> <code>None</code> <code>logger_name</code> <code>str</code> <p>Logger name</p> required <code>slurm_user</code> <code>Optional[str]</code> <p>Username to impersonate to run the workflow. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>slurm_account</code> <code>Optional[str]</code> <p>SLURM account to use when running the workflow. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>user_cache_dir</code> <code>Optional[str]</code> <p>Cache directory of the user who will run the workflow. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <code>worker_init</code> <code>Optional[str]</code> <p>Any additional, usually backend specific, information to be passed to the backend executor. This argument is present for compatibility with the standard backend interface, but is ignored in the <code>local</code> backend.</p> <code>None</code> <p>Raises:</p> Type Description <code>TaskExecutionError</code> <p>wrapper for errors raised during tasks' execution                 (positive exit codes).</p> <code>JobExecutionError</code> <p>wrapper for errors raised by the tasks' executors                (negative exit codes).</p> <p>Returns:</p> Name Type Description <code>output_dataset_metadata</code> <code>dict</code> <p>The updated metadata for the dataset, as returned by the last task of the workflow</p> Source code in <code>fractal_server/app/runner/v2/_local/__init__.py</code> <pre><code>async def process_workflow(\n    *,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    first_task_index: Optional[int] = None,\n    last_task_index: Optional[int] = None,\n    logger_name: str,\n    # Slurm-specific\n    user_cache_dir: Optional[str] = None,\n    slurm_user: Optional[str] = None,\n    slurm_account: Optional[str] = None,\n    worker_init: Optional[str] = None,\n) -&gt; dict:\n    \"\"\"\n    Run a workflow\n\n    This function is responsible for running a workflow on some input data,\n    saving the output and taking care of any exception raised during the run.\n\n    NOTE: This is the `local` backend's public interface, which also works as\n    a reference implementation for other backends.\n\n    Args:\n        workflow:\n            The workflow to be run\n        dataset:\n            Initial dataset.\n        workflow_dir:\n            Working directory for this run.\n        workflow_dir_user:\n            Working directory for this run, on the user side. This argument is\n            present for compatibility with the standard backend interface, but\n            for the `local` backend it cannot be different from `workflow_dir`.\n        first_task_index:\n            Positional index of the first task to execute; if `None`, start\n            from `0`.\n        last_task_index:\n            Positional index of the last task to execute; if `None`, proceed\n            until the last task.\n        logger_name: Logger name\n        slurm_user:\n            Username to impersonate to run the workflow. This argument is\n            present for compatibility with the standard backend interface, but\n            is ignored in the `local` backend.\n        slurm_account:\n            SLURM account to use when running the workflow. This argument is\n            present for compatibility with the standard backend interface, but\n            is ignored in the `local` backend.\n        user_cache_dir:\n            Cache directory of the user who will run the workflow. This\n            argument is present for compatibility with the standard backend\n            interface, but is ignored in the `local` backend.\n        worker_init:\n            Any additional, usually backend specific, information to be passed\n            to the backend executor. This argument is present for compatibility\n            with the standard backend interface, but is ignored in the `local`\n            backend.\n\n    Raises:\n        TaskExecutionError: wrapper for errors raised during tasks' execution\n                            (positive exit codes).\n        JobExecutionError: wrapper for errors raised by the tasks' executors\n                           (negative exit codes).\n\n    Returns:\n        output_dataset_metadata:\n            The updated metadata for the dataset, as returned by the last task\n            of the workflow\n    \"\"\"\n\n    if workflow_dir_user and (workflow_dir_user != workflow_dir):\n        raise NotImplementedError(\n            \"Local backend does not support different directories \"\n            f\"{workflow_dir=} and {workflow_dir_user=}\"\n        )\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    new_dataset_attributes = await async_wrap(_process_workflow)(\n        workflow=workflow,\n        dataset=dataset,\n        logger_name=logger_name,\n        workflow_dir=workflow_dir,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n    return new_dataset_attributes\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_local/_local_config/","title":"_local_config","text":"<p>Submodule to handle the local-backend configuration for a WorkflowTask</p>"},{"location":"reference/fractal_server/app/runner/v2/_local/_local_config/#fractal_server.app.runner.v2._local._local_config.LocalBackendConfig","title":"<code>LocalBackendConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Specifications of the local-backend configuration</p> <p>Attributes:</p> Name Type Description <code>parallel_tasks_per_job</code> <code>Optional[int]</code> <p>Maximum number of tasks to be run in parallel as part of a call to <code>FractalThreadPoolExecutor.map</code>; if <code>None</code>, then all tasks will start at the same time.</p> Source code in <code>fractal_server/app/runner/v2/_local/_local_config.py</code> <pre><code>class LocalBackendConfig(BaseModel, extra=Extra.forbid):\n    \"\"\"\n    Specifications of the local-backend configuration\n\n    Attributes:\n        parallel_tasks_per_job:\n            Maximum number of tasks to be run in parallel as part of a call to\n            `FractalThreadPoolExecutor.map`; if `None`, then all tasks will\n            start at the same time.\n    \"\"\"\n\n    parallel_tasks_per_job: Optional[int]\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_local/_local_config/#fractal_server.app.runner.v2._local._local_config.LocalBackendConfigError","title":"<code>LocalBackendConfigError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Local-backend configuration error</p> Source code in <code>fractal_server/app/runner/v2/_local/_local_config.py</code> <pre><code>class LocalBackendConfigError(ValueError):\n    \"\"\"\n    Local-backend configuration error\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_local/_local_config/#fractal_server.app.runner.v2._local._local_config.get_default_local_backend_config","title":"<code>get_default_local_backend_config()</code>","text":"<p>Return a default <code>LocalBackendConfig</code> configuration object</p> Source code in <code>fractal_server/app/runner/v2/_local/_local_config.py</code> <pre><code>def get_default_local_backend_config():\n    \"\"\"\n    Return a default `LocalBackendConfig` configuration object\n    \"\"\"\n    return LocalBackendConfig(parallel_tasks_per_job=None)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_local/_local_config/#fractal_server.app.runner.v2._local._local_config.get_local_backend_config","title":"<code>get_local_backend_config(wftask, which_type, config_path=None)</code>","text":"<p>Prepare a <code>LocalBackendConfig</code> configuration object</p> <p>The sources for <code>parallel_tasks_per_job</code> attributes, starting from the highest-priority one, are</p> <ol> <li>Properties in <code>wftask.meta_parallel</code> or <code>wftask.meta_non_parallel</code>    (depending on <code>which_type</code>);</li> <li>The general content of the local-backend configuration file;</li> <li>The default value (<code>None</code>).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>wftask</code> <code>WorkflowTaskV2</code> <p>WorkflowTaskV2 for which the backend configuration should be prepared.</p> required <code>config_path</code> <code>Optional[Path]</code> <p>Path of local-backend configuration file; if <code>None</code>, use <code>FRACTAL_LOCAL_CONFIG_FILE</code> variable from settings.</p> <code>None</code> <p>Returns:</p> Type Description <code>LocalBackendConfig</code> <p>A local-backend configuration object</p> Source code in <code>fractal_server/app/runner/v2/_local/_local_config.py</code> <pre><code>def get_local_backend_config(\n    wftask: WorkflowTaskV2,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n    config_path: Optional[Path] = None,\n) -&gt; LocalBackendConfig:\n    \"\"\"\n    Prepare a `LocalBackendConfig` configuration object\n\n    The sources for `parallel_tasks_per_job` attributes, starting from the\n    highest-priority one, are\n\n    1. Properties in `wftask.meta_parallel` or `wftask.meta_non_parallel`\n       (depending on `which_type`);\n    2. The general content of the local-backend configuration file;\n    3. The default value (`None`).\n\n    Arguments:\n        wftask:\n            WorkflowTaskV2 for which the backend configuration should\n            be prepared.\n        config_path:\n            Path of local-backend configuration file; if `None`, use\n            `FRACTAL_LOCAL_CONFIG_FILE` variable from settings.\n\n    Returns:\n        A local-backend configuration object\n    \"\"\"\n\n    key = \"parallel_tasks_per_job\"\n    default_value = None\n\n    if which_type == \"non_parallel\":\n        wftask_meta = wftask.meta_non_parallel\n    elif which_type == \"parallel\":\n        wftask_meta = wftask.meta_parallel\n    else:\n        raise ValueError(\n            \"`get_local_backend_config` received an invalid argument\"\n            f\" {which_type=}.\"\n        )\n\n    if wftask_meta and key in wftask_meta:\n        parallel_tasks_per_job = wftask.meta[key]\n    else:\n        if not config_path:\n            settings = Inject(get_settings)\n            config_path = settings.FRACTAL_LOCAL_CONFIG_FILE\n        if config_path is None:\n            parallel_tasks_per_job = default_value\n        else:\n            with config_path.open(\"r\") as f:\n                env = json.load(f)\n            try:\n                _ = LocalBackendConfig(**env)\n            except ValidationError as e:\n                raise LocalBackendConfigError(\n                    f\"Error while loading {config_path=}. \"\n                    f\"Original error:\\n{str(e)}\"\n                )\n\n            parallel_tasks_per_job = env.get(key, default_value)\n    return LocalBackendConfig(parallel_tasks_per_job=parallel_tasks_per_job)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_local/_submit_setup/","title":"_submit_setup","text":"<p>Submodule to define _local_submit_setup</p>"},{"location":"reference/fractal_server/app/runner/v2/_local/_submit_setup/#fractal_server.app.runner.v2._local._submit_setup._local_submit_setup","title":"<code>_local_submit_setup(*, wftask, workflow_dir=None, workflow_dir_user=None, which_type)</code>","text":"<p>Collect WorfklowTask-specific configuration parameters from different sources, and inject them for execution.</p> <p>Parameters:</p> Name Type Description Default <code>wftask</code> <code>WorkflowTaskV2</code> <p>WorkflowTask for which the configuration is to be assembled</p> required <code>workflow_dir</code> <code>Optional[Path]</code> <p>Not used in this function.</p> <code>None</code> <code>workflow_dir_user</code> <code>Optional[Path]</code> <p>Not used in this function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>submit_setup_dict</code> <code>dict[str, object]</code> <p>A dictionary that will be passed on to <code>FractalThreadPoolExecutor.submit</code> and <code>FractalThreadPoolExecutor.map</code>, so as to set extra options.</p> Source code in <code>fractal_server/app/runner/v2/_local/_submit_setup.py</code> <pre><code>def _local_submit_setup(\n    *,\n    wftask: WorkflowTaskV2,\n    workflow_dir: Optional[Path] = None,\n    workflow_dir_user: Optional[Path] = None,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n) -&gt; dict[str, object]:\n    \"\"\"\n    Collect WorfklowTask-specific configuration parameters from different\n    sources, and inject them for execution.\n\n    Arguments:\n        wftask:\n            WorkflowTask for which the configuration is to be assembled\n        workflow_dir:\n            Not used in this function.\n        workflow_dir_user:\n            Not used in this function.\n\n    Returns:\n        submit_setup_dict:\n            A dictionary that will be passed on to\n            `FractalThreadPoolExecutor.submit` and\n            `FractalThreadPoolExecutor.map`, so as to set extra options.\n    \"\"\"\n\n    local_backend_config = get_local_backend_config(\n        wftask=wftask, which_type=which_type\n    )\n\n    return dict(local_backend_config=local_backend_config)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_local/executor/","title":"executor","text":"<p>Custom version of Python ThreadPoolExecutor).</p>"},{"location":"reference/fractal_server/app/runner/v2/_local/executor/#fractal_server.app.runner.v2._local.executor.FractalThreadPoolExecutor","title":"<code>FractalThreadPoolExecutor</code>","text":"<p>             Bases: <code>ThreadPoolExecutor</code></p> <p>Custom version of ThreadPoolExecutor) that overrides the <code>submit</code> and <code>map</code> methods</p> Source code in <code>fractal_server/app/runner/v2/_local/executor.py</code> <pre><code>class FractalThreadPoolExecutor(ThreadPoolExecutor):\n    \"\"\"\n    Custom version of\n    [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor))\n    that overrides the `submit` and `map` methods\n    \"\"\"\n\n    def submit(\n        self,\n        *args,\n        local_backend_config: Optional[LocalBackendConfig] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Compared to the `ThreadPoolExecutor` method, here we accept an addition\n        keyword argument (`local_backend_config`), which is then simply\n        ignored.\n        \"\"\"\n        return super().submit(*args, **kwargs)\n\n    def map(\n        self,\n        fn: Callable,\n        *iterables: Sequence[Iterable],\n        local_backend_config: Optional[LocalBackendConfig] = None,\n    ):\n        \"\"\"\n        Custom version of the `Executor.map` method\n\n        The main change with the respect to the original `map` method is that\n        the list of tasks to be executed is split into chunks, and then\n        `super().map` is called (sequentially) on each chunk. The goal of this\n        change is to limit parallelism, e.g. due to limited computational\n        resources.\n\n        Other changes from the `concurrent.futures` `map` method:\n\n        1. Removed `timeout` argument;\n        2. Removed `chunksize`;\n        3. All iterators (both inputs and output ones) are transformed into\n           lists.\n\n        Args:\n            fn: A callable function.\n            iterables: The argument iterables (one iterable per argument of\n                       `fn`).\n           local_backend_config: The backend configuration, needed to extract\n                                 `parallel_tasks_per_job`.\n        \"\"\"\n\n        # Preliminary check\n        iterable_lengths = [len(it) for it in iterables]\n        if not len(set(iterable_lengths)) == 1:\n            raise ValueError(\"Iterables have different lengths.\")\n\n        # Set total number of arguments\n        n_elements = len(iterables[0])\n\n        # Set parallel_tasks_per_job\n        if local_backend_config is None:\n            local_backend_config = get_default_local_backend_config()\n        parallel_tasks_per_job = local_backend_config.parallel_tasks_per_job\n        if parallel_tasks_per_job is None:\n            parallel_tasks_per_job = n_elements\n\n        # Execute tasks, in chunks of size parallel_tasks_per_job\n        results = []\n        for ind_chunk in range(0, n_elements, parallel_tasks_per_job):\n            chunk_iterables = [\n                it[ind_chunk : ind_chunk + parallel_tasks_per_job]  # noqa\n                for it in iterables\n            ]\n            map_iter = super().map(fn, *chunk_iterables)\n            results.extend(list(map_iter))\n\n        return iter(results)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_local/executor/#fractal_server.app.runner.v2._local.executor.FractalThreadPoolExecutor.map","title":"<code>map(fn, *iterables, local_backend_config=None)</code>","text":"<p>Custom version of the <code>Executor.map</code> method</p> <p>The main change with the respect to the original <code>map</code> method is that the list of tasks to be executed is split into chunks, and then <code>super().map</code> is called (sequentially) on each chunk. The goal of this change is to limit parallelism, e.g. due to limited computational resources.</p> <p>Other changes from the <code>concurrent.futures</code> <code>map</code> method:</p> <ol> <li>Removed <code>timeout</code> argument;</li> <li>Removed <code>chunksize</code>;</li> <li>All iterators (both inputs and output ones) are transformed into    lists.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>A callable function.</p> required <code>iterables</code> <code>Sequence[Iterable]</code> <p>The argument iterables (one iterable per argument of        <code>fn</code>).</p> <code>()</code> <p>local_backend_config: The backend configuration, needed to extract                          <code>parallel_tasks_per_job</code>.</p> Source code in <code>fractal_server/app/runner/v2/_local/executor.py</code> <pre><code>def map(\n    self,\n    fn: Callable,\n    *iterables: Sequence[Iterable],\n    local_backend_config: Optional[LocalBackendConfig] = None,\n):\n    \"\"\"\n    Custom version of the `Executor.map` method\n\n    The main change with the respect to the original `map` method is that\n    the list of tasks to be executed is split into chunks, and then\n    `super().map` is called (sequentially) on each chunk. The goal of this\n    change is to limit parallelism, e.g. due to limited computational\n    resources.\n\n    Other changes from the `concurrent.futures` `map` method:\n\n    1. Removed `timeout` argument;\n    2. Removed `chunksize`;\n    3. All iterators (both inputs and output ones) are transformed into\n       lists.\n\n    Args:\n        fn: A callable function.\n        iterables: The argument iterables (one iterable per argument of\n                   `fn`).\n       local_backend_config: The backend configuration, needed to extract\n                             `parallel_tasks_per_job`.\n    \"\"\"\n\n    # Preliminary check\n    iterable_lengths = [len(it) for it in iterables]\n    if not len(set(iterable_lengths)) == 1:\n        raise ValueError(\"Iterables have different lengths.\")\n\n    # Set total number of arguments\n    n_elements = len(iterables[0])\n\n    # Set parallel_tasks_per_job\n    if local_backend_config is None:\n        local_backend_config = get_default_local_backend_config()\n    parallel_tasks_per_job = local_backend_config.parallel_tasks_per_job\n    if parallel_tasks_per_job is None:\n        parallel_tasks_per_job = n_elements\n\n    # Execute tasks, in chunks of size parallel_tasks_per_job\n    results = []\n    for ind_chunk in range(0, n_elements, parallel_tasks_per_job):\n        chunk_iterables = [\n            it[ind_chunk : ind_chunk + parallel_tasks_per_job]  # noqa\n            for it in iterables\n        ]\n        map_iter = super().map(fn, *chunk_iterables)\n        results.extend(list(map_iter))\n\n    return iter(results)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_local/executor/#fractal_server.app.runner.v2._local.executor.FractalThreadPoolExecutor.submit","title":"<code>submit(*args, local_backend_config=None, **kwargs)</code>","text":"<p>Compared to the <code>ThreadPoolExecutor</code> method, here we accept an addition keyword argument (<code>local_backend_config</code>), which is then simply ignored.</p> Source code in <code>fractal_server/app/runner/v2/_local/executor.py</code> <pre><code>def submit(\n    self,\n    *args,\n    local_backend_config: Optional[LocalBackendConfig] = None,\n    **kwargs,\n):\n    \"\"\"\n    Compared to the `ThreadPoolExecutor` method, here we accept an addition\n    keyword argument (`local_backend_config`), which is then simply\n    ignored.\n    \"\"\"\n    return super().submit(*args, **kwargs)\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_slurm/","title":"_slurm","text":"<p>Slurm Bakend</p> <p>This backend runs fractal workflows in a SLURM cluster using Clusterfutures Executor objects.</p>"},{"location":"reference/fractal_server/app/runner/v2/_slurm/#fractal_server.app.runner.v2._slurm._process_workflow","title":"<code>_process_workflow(*, workflow, dataset, logger_name, workflow_dir, workflow_dir_user, first_task_index, last_task_index, slurm_user=None, slurm_account=None, user_cache_dir, worker_init=None)</code>","text":"<p>Internal processing routine for the SLURM backend</p> <p>This function initialises the a FractalSlurmExecutor, setting logging, workflow working dir and user to impersonate. It then schedules the workflow tasks and returns the new dataset attributes</p> <p>Cf. process_workflow</p> <p>Returns:</p> Name Type Description <code>new_dataset_attributes</code> <code>dict[str, Any]</code> Source code in <code>fractal_server/app/runner/v2/_slurm/__init__.py</code> <pre><code>def _process_workflow(\n    *,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    logger_name: str,\n    workflow_dir: Path,\n    workflow_dir_user: Path,\n    first_task_index: int,\n    last_task_index: int,\n    slurm_user: Optional[str] = None,\n    slurm_account: Optional[str] = None,\n    user_cache_dir: str,\n    worker_init: Optional[Union[str, list[str]]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Internal processing routine for the SLURM backend\n\n    This function initialises the a FractalSlurmExecutor, setting logging,\n    workflow working dir and user to impersonate. It then schedules the\n    workflow tasks and returns the new dataset attributes\n\n    Cf.\n    [process_workflow][fractal_server.app.runner.v2._local.process_workflow]\n\n    Returns:\n        new_dataset_attributes:\n    \"\"\"\n\n    if not slurm_user:\n        raise RuntimeError(\n            \"slurm_user argument is required, for slurm backend\"\n        )\n\n    if isinstance(worker_init, str):\n        worker_init = worker_init.split(\"\\n\")\n\n    with FractalSlurmExecutor(\n        debug=True,\n        keep_logs=True,\n        slurm_user=slurm_user,\n        user_cache_dir=user_cache_dir,\n        working_dir=workflow_dir,\n        working_dir_user=workflow_dir_user,\n        common_script_lines=worker_init,\n        slurm_account=slurm_account,\n    ) as executor:\n        new_dataset_attributes = execute_tasks_v2(\n            wf_task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)  # noqa\n            ],  # noqa\n            dataset=dataset,\n            executor=executor,\n            workflow_dir=workflow_dir,\n            workflow_dir_user=workflow_dir_user,\n            logger_name=logger_name,\n            submit_setup_call=_slurm_submit_setup,\n        )\n    return new_dataset_attributes\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_slurm/#fractal_server.app.runner.v2._slurm.process_workflow","title":"<code>process_workflow(*, workflow, dataset, workflow_dir, workflow_dir_user=None, first_task_index=None, last_task_index=None, logger_name, user_cache_dir=None, slurm_user=None, slurm_account=None, worker_init=None)</code>  <code>async</code>","text":"<p>Process workflow (SLURM backend public interface)</p> <p>Cf. process_workflow</p> Source code in <code>fractal_server/app/runner/v2/_slurm/__init__.py</code> <pre><code>async def process_workflow(\n    *,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    workflow_dir: Path,\n    workflow_dir_user: Optional[Path] = None,\n    first_task_index: Optional[int] = None,\n    last_task_index: Optional[int] = None,\n    logger_name: str,\n    # Slurm-specific\n    user_cache_dir: Optional[str] = None,\n    slurm_user: Optional[str] = None,\n    slurm_account: Optional[str] = None,\n    worker_init: Optional[str] = None,\n) -&gt; dict:\n    \"\"\"\n    Process workflow (SLURM backend public interface)\n\n    Cf.\n    [process_workflow][fractal_server.app.runner.v2._local.process_workflow]\n    \"\"\"\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    new_dataset_attributes = await async_wrap(_process_workflow)(\n        workflow=workflow,\n        dataset=dataset,\n        logger_name=logger_name,\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n        user_cache_dir=user_cache_dir,\n        slurm_user=slurm_user,\n        slurm_account=slurm_account,\n        worker_init=worker_init,\n    )\n    return new_dataset_attributes\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_slurm/_submit_setup/","title":"_submit_setup","text":"<p>Submodule to define _slurm_submit_setup, which is also the reference implementation of <code>submit_setup_call</code>.</p>"},{"location":"reference/fractal_server/app/runner/v2/_slurm/_submit_setup/#fractal_server.app.runner.v2._slurm._submit_setup._slurm_submit_setup","title":"<code>_slurm_submit_setup(*, wftask, workflow_dir, workflow_dir_user, which_type)</code>","text":"<p>Collect WorfklowTask-specific configuration parameters from different sources, and inject them for execution.</p> <p>Here goes all the logic for reading attributes from the appropriate sources and transforming them into an appropriate <code>SlurmConfig</code> object (encoding SLURM configuration) and <code>TaskFiles</code> object (with details e.g. about file paths or filename prefixes).</p> <p>For now, this is the reference implementation for the argument <code>submit_setup_call</code> of fractal_server.app.runner.v2.runner.</p> <p>Parameters:</p> Name Type Description Default <code>wftask</code> <code>WorkflowTaskV2</code> <p>WorkflowTask for which the configuration is to be assembled</p> required <code>workflow_dir</code> <code>Path</code> <p>Server-owned directory to store all task-execution-related relevant files (inputs, outputs, errors, and all meta files related to the job execution). Note: users cannot write directly to this folder.</p> required <code>workflow_dir_user</code> <code>Path</code> <p>User-side directory with the same scope as <code>workflow_dir</code>, and where a user can write.</p> required <p>Returns:</p> Name Type Description <code>submit_setup_dict</code> <code>dict[str, object]</code> <p>A dictionary that will be passed on to <code>FractalSlurmExecutor.submit</code> and <code>FractalSlurmExecutor.map</code>, so as to set extra options.</p> Source code in <code>fractal_server/app/runner/v2/_slurm/_submit_setup.py</code> <pre><code>def _slurm_submit_setup(\n    *,\n    wftask: WorkflowTaskV2,\n    workflow_dir: Path,\n    workflow_dir_user: Path,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n) -&gt; dict[str, object]:\n    \"\"\"\n    Collect WorfklowTask-specific configuration parameters from different\n    sources, and inject them for execution.\n\n    Here goes all the logic for reading attributes from the appropriate sources\n    and transforming them into an appropriate `SlurmConfig` object (encoding\n    SLURM configuration) and `TaskFiles` object (with details e.g. about file\n    paths or filename prefixes).\n\n    For now, this is the reference implementation for the argument\n    `submit_setup_call` of\n    [fractal_server.app.runner.v2.runner][].\n\n    Arguments:\n        wftask:\n            WorkflowTask for which the configuration is to be assembled\n        workflow_dir:\n            Server-owned directory to store all task-execution-related relevant\n            files (inputs, outputs, errors, and all meta files related to the\n            job execution). Note: users cannot write directly to this folder.\n        workflow_dir_user:\n            User-side directory with the same scope as `workflow_dir`, and\n            where a user can write.\n\n    Returns:\n        submit_setup_dict:\n            A dictionary that will be passed on to\n            `FractalSlurmExecutor.submit` and `FractalSlurmExecutor.map`, so\n            as to set extra options.\n    \"\"\"\n\n    # Get SlurmConfig object\n    slurm_config = get_slurm_config(\n        wftask=wftask,\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n        which_type=which_type,\n    )\n\n    # Get TaskFiles object\n    task_files = get_task_file_paths(\n        workflow_dir=workflow_dir,\n        workflow_dir_user=workflow_dir_user,\n        task_order=wftask.order,\n    )\n\n    # Prepare and return output dictionary\n    submit_setup_dict = dict(\n        slurm_config=slurm_config,\n        task_files=task_files,\n    )\n    return submit_setup_dict\n</code></pre>"},{"location":"reference/fractal_server/app/runner/v2/_slurm/get_slurm_config/","title":"get_slurm_config","text":""},{"location":"reference/fractal_server/app/runner/v2/_slurm/get_slurm_config/#fractal_server.app.runner.v2._slurm.get_slurm_config.get_slurm_config","title":"<code>get_slurm_config(wftask, workflow_dir, workflow_dir_user, which_type, config_path=None)</code>","text":"<p>Prepare a <code>SlurmConfig</code> configuration object</p> <p>The argument <code>which_type</code> determines whether we use <code>wftask.meta_parallel</code> or <code>wftask.meta_non_parallel</code>. In the following descritpion, let us assume that <code>which_type=\"parallel\"</code>.</p> <p>The sources for <code>SlurmConfig</code> attributes, in increasing priority order, are</p> <ol> <li>The general content of the Fractal SLURM configuration file.</li> <li>The GPU-specific content of the Fractal SLURM configuration file, if     appropriate.</li> <li>Properties in <code>wftask.meta_parallel</code> (which typically include those in    <code>wftask.task.meta_parallel</code>). Note that <code>wftask.meta_parallel</code> may be    <code>None</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>wftask</code> <code>WorkflowTaskV2</code> <p>WorkflowTask for which the SLURM configuration is is to be prepared.</p> required <code>workflow_dir</code> <code>Path</code> <p>Server-owned directory to store all task-execution-related relevant files (inputs, outputs, errors, and all meta files related to the job execution). Note: users cannot write directly to this folder.</p> required <code>workflow_dir_user</code> <code>Path</code> <p>User-side directory with the same scope as <code>workflow_dir</code>, and where a user can write.</p> required <code>config_path</code> <code>Optional[Path]</code> <p>Path of a Fractal SLURM configuration file; if <code>None</code>, use <code>FRACTAL_SLURM_CONFIG_FILE</code> variable from settings.</p> <code>None</code> <code>which_type</code> <code>Literal['non_parallel', 'parallel']</code> <p>Determines whether to use <code>meta_parallel</code> or <code>meta_non_parallel</code>.</p> required <p>Returns:</p> Name Type Description <code>slurm_config</code> <code>SlurmConfig</code> <p>The SlurmConfig object</p> Source code in <code>fractal_server/app/runner/v2/_slurm/get_slurm_config.py</code> <pre><code>def get_slurm_config(\n    wftask: WorkflowTaskV2,\n    workflow_dir: Path,\n    workflow_dir_user: Path,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n    config_path: Optional[Path] = None,\n) -&gt; SlurmConfig:\n    \"\"\"\n    Prepare a `SlurmConfig` configuration object\n\n    The argument `which_type` determines whether we use `wftask.meta_parallel`\n    or `wftask.meta_non_parallel`. In the following descritpion, let us assume\n    that `which_type=\"parallel\"`.\n\n    The sources for `SlurmConfig` attributes, in increasing priority order, are\n\n    1. The general content of the Fractal SLURM configuration file.\n    2. The GPU-specific content of the Fractal SLURM configuration file, if\n        appropriate.\n    3. Properties in `wftask.meta_parallel` (which typically include those in\n       `wftask.task.meta_parallel`). Note that `wftask.meta_parallel` may be\n       `None`.\n\n    Arguments:\n        wftask:\n            WorkflowTask for which the SLURM configuration is is to be\n            prepared.\n        workflow_dir:\n            Server-owned directory to store all task-execution-related relevant\n            files (inputs, outputs, errors, and all meta files related to the\n            job execution). Note: users cannot write directly to this folder.\n        workflow_dir_user:\n            User-side directory with the same scope as `workflow_dir`, and\n            where a user can write.\n        config_path:\n            Path of a Fractal SLURM configuration file; if `None`, use\n            `FRACTAL_SLURM_CONFIG_FILE` variable from settings.\n        which_type:\n            Determines whether to use `meta_parallel` or `meta_non_parallel`.\n\n    Returns:\n        slurm_config:\n            The SlurmConfig object\n    \"\"\"\n\n    if which_type == \"non_parallel\":\n        wftask_meta = wftask.meta_non_parallel\n    elif which_type == \"parallel\":\n        wftask_meta = wftask.meta_parallel\n    else:\n        raise ValueError(\n            f\"get_slurm_config received invalid argument {which_type=}.\"\n        )\n\n    logger.debug(\n        \"[get_slurm_config] WorkflowTask meta attribute: {wftask_meta=}\"\n    )\n\n    # Incorporate slurm_env.default_slurm_config\n    slurm_env = load_slurm_config_file(config_path=config_path)\n    slurm_dict = slurm_env.default_slurm_config.dict(\n        exclude_unset=True, exclude={\"mem\"}\n    )\n    if slurm_env.default_slurm_config.mem:\n        slurm_dict[\"mem_per_task_MB\"] = slurm_env.default_slurm_config.mem\n\n    # Incorporate slurm_env.batching_config\n    for key, value in slurm_env.batching_config.dict().items():\n        slurm_dict[key] = value\n\n    # Incorporate slurm_env.user_local_exports\n    slurm_dict[\"user_local_exports\"] = slurm_env.user_local_exports\n\n    logger.debug(\n        \"[get_slurm_config] Fractal SLURM configuration file: \"\n        f\"{slurm_env.dict()=}\"\n    )\n\n    # GPU-related options\n    # Notes about priority:\n    # 1. This block of definitions takes priority over other definitions from\n    #    slurm_env which are not under the `needs_gpu` subgroup\n    # 2. This block of definitions has lower priority than whatever comes next\n    #    (i.e. from WorkflowTask.meta).\n    if wftask_meta is not None:\n        needs_gpu = wftask_meta.get(\"needs_gpu\", False)\n    else:\n        needs_gpu = False\n    logger.debug(f\"[get_slurm_config] {needs_gpu=}\")\n    if needs_gpu:\n        for key, value in slurm_env.gpu_slurm_config.dict(\n            exclude_unset=True, exclude={\"mem\"}\n        ).items():\n            slurm_dict[key] = value\n        if slurm_env.gpu_slurm_config.mem:\n            slurm_dict[\"mem_per_task_MB\"] = slurm_env.gpu_slurm_config.mem\n\n    # Number of CPUs per task, for multithreading\n    if wftask_meta is not None and \"cpus_per_task\" in wftask_meta:\n        cpus_per_task = int(wftask_meta[\"cpus_per_task\"])\n        slurm_dict[\"cpus_per_task\"] = cpus_per_task\n\n    # Required memory per task, in MB\n    if wftask_meta is not None and \"mem\" in wftask_meta:\n        raw_mem = wftask_meta[\"mem\"]\n        mem_per_task_MB = _parse_mem_value(raw_mem)\n        slurm_dict[\"mem_per_task_MB\"] = mem_per_task_MB\n\n    # Job name\n    if wftask.is_legacy_task:\n        job_name = wftask.task_legacy.name.replace(\" \", \"_\")\n    else:\n        job_name = wftask.task.name.replace(\" \", \"_\")\n    slurm_dict[\"job_name\"] = job_name\n\n    # Optional SLURM arguments and extra lines\n    if wftask_meta is not None:\n        account = wftask_meta.get(\"account\", None)\n        if account is not None:\n            error_msg = (\n                f\"Invalid {account=} property in WorkflowTask `meta` \"\n                \"attribute.\\n\"\n                \"SLURM account must be set in the request body of the \"\n                \"apply-workflow endpoint, or by modifying the user properties.\"\n            )\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        for key in [\"time\", \"gres\", \"constraint\"]:\n            value = wftask_meta.get(key, None)\n            if value:\n                slurm_dict[key] = value\n    if wftask_meta is not None:\n        extra_lines = wftask_meta.get(\"extra_lines\", [])\n    else:\n        extra_lines = []\n    extra_lines = slurm_dict.get(\"extra_lines\", []) + extra_lines\n    if len(set(extra_lines)) != len(extra_lines):\n        logger.debug(\n            \"[get_slurm_config] Removing repeated elements \"\n            f\"from {extra_lines=}.\"\n        )\n        extra_lines = list(set(extra_lines))\n    slurm_dict[\"extra_lines\"] = extra_lines\n\n    # Job-batching parameters (if None, they will be determined heuristically)\n    if wftask_meta is not None:\n        tasks_per_job = wftask_meta.get(\"tasks_per_job\", None)\n        parallel_tasks_per_job = wftask_meta.get(\n            \"parallel_tasks_per_job\", None\n        )\n    else:\n        tasks_per_job = None\n        parallel_tasks_per_job = None\n    slurm_dict[\"tasks_per_job\"] = tasks_per_job\n    slurm_dict[\"parallel_tasks_per_job\"] = parallel_tasks_per_job\n\n    # Put everything together\n    logger.debug(\n        \"[get_slurm_config] Now create a SlurmConfig object based \"\n        f\"on {slurm_dict=}\"\n    )\n    slurm_config = SlurmConfig(**slurm_dict)\n\n    return slurm_config\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/","title":"schemas","text":""},{"location":"reference/fractal_server/app/schemas/_validators/","title":"_validators","text":""},{"location":"reference/fractal_server/app/schemas/_validators/#fractal_server.app.schemas._validators.val_absolute_path","title":"<code>val_absolute_path(attribute)</code>","text":"<p>Check that a string attribute is an absolute path</p> Source code in <code>fractal_server/app/schemas/_validators.py</code> <pre><code>def val_absolute_path(attribute: str):\n    \"\"\"\n    Check that a string attribute is an absolute path\n    \"\"\"\n\n    def val(string: Optional[str]) -&gt; str:\n        if string is None:\n            raise ValueError(f\"String attribute '{attribute}' cannot be None\")\n        s = string.strip()\n        if not s:\n            raise ValueError(f\"String attribute '{attribute}' cannot be empty\")\n        if not os.path.isabs(s):\n            raise ValueError(\n                f\"String attribute '{attribute}' must be an absolute path \"\n                f\"(given '{s}').\"\n            )\n        return s\n\n    return val\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/_validators/#fractal_server.app.schemas._validators.valint","title":"<code>valint(attribute, min_val=1)</code>","text":"<p>Check that an integer attribute (e.g. if it is meant to be the ID of a database entry) is greater or equal to min_val.</p> Source code in <code>fractal_server/app/schemas/_validators.py</code> <pre><code>def valint(attribute: str, min_val: int = 1):\n    \"\"\"\n    Check that an integer attribute (e.g. if it is meant to be the ID of a\n    database entry) is greater or equal to min_val.\n    \"\"\"\n\n    def val(integer: Optional[int]) -&gt; Optional[int]:\n        if integer is None:\n            raise ValueError(f\"Integer attribute '{attribute}' cannot be None\")\n        if integer &lt; min_val:\n            raise ValueError(\n                f\"Integer attribute '{attribute}' cannot be less than \"\n                f\"{min_val} (given {integer})\"\n            )\n        return integer\n\n    return val\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/_validators/#fractal_server.app.schemas._validators.valstr","title":"<code>valstr(attribute, accept_none=False)</code>","text":"<p>Check that a string attribute is not an empty string, and remove the leading and trailing whitespace characters.</p> <p>If <code>accept_none</code>, the validator also accepts <code>None</code>.</p> Source code in <code>fractal_server/app/schemas/_validators.py</code> <pre><code>def valstr(attribute: str, accept_none: bool = False):\n    \"\"\"\n    Check that a string attribute is not an empty string, and remove the\n    leading and trailing whitespace characters.\n\n    If `accept_none`, the validator also accepts `None`.\n    \"\"\"\n\n    def val(string: Optional[str]) -&gt; Optional[str]:\n        if string is None:\n            if accept_none:\n                return string\n            else:\n                raise ValueError(\n                    f\"String attribute '{attribute}' cannot be None\"\n                )\n        s = string.strip()\n        if not s:\n            raise ValueError(f\"String attribute '{attribute}' cannot be empty\")\n        return s\n\n    return val\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/state/","title":"state","text":""},{"location":"reference/fractal_server/app/schemas/state/#fractal_server.app.schemas.state.StateRead","title":"<code>StateRead</code>","text":"<p>             Bases: <code>_StateBase</code></p> <p>Class for <code>State</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> Source code in <code>fractal_server/app/schemas/state.py</code> <pre><code>class StateRead(_StateBase):\n    \"\"\"\n    Class for `State` read from database.\n\n    Attributes:\n        id:\n    \"\"\"\n\n    id: Optional[int]\n\n    _timestamp = validator(\"timestamp\", allow_reuse=True)(valutc(\"timestamp\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/state/#fractal_server.app.schemas.state._StateBase","title":"<code>_StateBase</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>State</code>.</p> <p>Attributes:</p> Name Type Description <code>id</code> <p>Primary key</p> <code>data</code> <code>dict[str, Any]</code> <p>Content of the state</p> <code>timestamp</code> <code>datetime</code> <p>Time stamp of the state</p> Source code in <code>fractal_server/app/schemas/state.py</code> <pre><code>class _StateBase(BaseModel):\n    \"\"\"\n    Base class for `State`.\n\n    Attributes:\n        id: Primary key\n        data: Content of the state\n        timestamp: Time stamp of the state\n    \"\"\"\n\n    data: dict[str, Any]\n    timestamp: datetime\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/","title":"user","text":""},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.UserCreate","title":"<code>UserCreate</code>","text":"<p>             Bases: <code>BaseUserCreate</code></p> <p>Schema for <code>User</code> creation.</p> <p>Attributes:</p> Name Type Description <code>slurm_user</code> <code>Optional[str]</code> <code>cache_dir</code> <code>Optional[str]</code> <code>username</code> <code>Optional[str]</code> <code>slurm_accounts</code> <code>list[StrictStr]</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserCreate(schemas.BaseUserCreate):\n    \"\"\"\n    Schema for `User` creation.\n\n    Attributes:\n        slurm_user:\n        cache_dir:\n        username:\n        slurm_accounts:\n    \"\"\"\n\n    slurm_user: Optional[str]\n    cache_dir: Optional[str]\n    username: Optional[str]\n    slurm_accounts: list[StrictStr] = Field(default_factory=list)\n\n    # Validators\n\n    @validator(\"slurm_accounts\")\n    def slurm_accounts_validator(cls, value):\n        for i, element in enumerate(value):\n            value[i] = valstr(attribute=f\"slurm_accounts[{i}]\")(element)\n        val_unique_list(\"slurm_accounts\")(value)\n        return value\n\n    _slurm_user = validator(\"slurm_user\", allow_reuse=True)(\n        valstr(\"slurm_user\")\n    )\n    _username = validator(\"username\", allow_reuse=True)(valstr(\"username\"))\n    _cache_dir = validator(\"cache_dir\", allow_reuse=True)(\n        val_absolute_path(\"cache_dir\")\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.UserRead","title":"<code>UserRead</code>","text":"<p>             Bases: <code>BaseUser[int]</code></p> <p>Schema for <code>User</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>slurm_user</code> <code>Optional[str]</code> <code>cache_dir</code> <code>Optional[str]</code> <code>username</code> <code>Optional[str]</code> <code>slurm_accounts</code> <code>list[str]</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserRead(schemas.BaseUser[int]):\n    \"\"\"\n    Schema for `User` read from database.\n\n    Attributes:\n        slurm_user:\n        cache_dir:\n        username:\n        slurm_accounts:\n    \"\"\"\n\n    slurm_user: Optional[str]\n    cache_dir: Optional[str]\n    username: Optional[str]\n    slurm_accounts: list[str]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.UserUpdate","title":"<code>UserUpdate</code>","text":"<p>             Bases: <code>BaseUserUpdate</code></p> <p>Schema for <code>User</code> update.</p> <p>Attributes:</p> Name Type Description <code>slurm_user</code> <code>Optional[str]</code> <code>cache_dir</code> <code>Optional[str]</code> <code>username</code> <code>Optional[str]</code> <code>slurm_accounts</code> <code>Optional[list[StrictStr]]</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdate(schemas.BaseUserUpdate):\n    \"\"\"\n    Schema for `User` update.\n\n    Attributes:\n        slurm_user:\n        cache_dir:\n        username:\n        slurm_accounts:\n    \"\"\"\n\n    slurm_user: Optional[str]\n    cache_dir: Optional[str]\n    username: Optional[str]\n    slurm_accounts: Optional[list[StrictStr]]\n\n    # Validators\n    _slurm_user = validator(\"slurm_user\", allow_reuse=True)(\n        valstr(\"slurm_user\")\n    )\n    _username = validator(\"username\", allow_reuse=True)(valstr(\"username\"))\n    _cache_dir = validator(\"cache_dir\", allow_reuse=True)(\n        val_absolute_path(\"cache_dir\")\n    )\n\n    _slurm_accounts = validator(\"slurm_accounts\", allow_reuse=True)(\n        val_unique_list(\"slurm_accounts\")\n    )\n\n    @validator(\n        \"is_active\",\n        \"is_verified\",\n        \"is_superuser\",\n        \"email\",\n        \"password\",\n        always=False,\n    )\n    def cant_set_none(cls, v, field):\n        if v is None:\n            raise ValueError(f\"Cannot set {field.name}=None\")\n        return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.UserUpdateStrict","title":"<code>UserUpdateStrict</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Schema for <code>User</code> self-editing.</p> <p>Attributes:</p> Name Type Description <code>cache_dir</code> <code>Optional[str]</code> <code>slurm_accounts</code> <code>Optional[list[StrictStr]]</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdateStrict(BaseModel, extra=Extra.forbid):\n    \"\"\"\n    Schema for `User` self-editing.\n\n    Attributes:\n        cache_dir:\n        slurm_accounts:\n    \"\"\"\n\n    cache_dir: Optional[str]\n    slurm_accounts: Optional[list[StrictStr]]\n\n    _slurm_accounts = validator(\"slurm_accounts\", allow_reuse=True)(\n        val_unique_list(\"slurm_accounts\")\n    )\n\n    _cache_dir = validator(\"cache_dir\", allow_reuse=True)(\n        val_absolute_path(\"cache_dir\")\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/","title":"v1","text":"<p>Schemas for API request/response bodies</p>"},{"location":"reference/fractal_server/app/schemas/v1/applyworkflow/","title":"applyworkflow","text":""},{"location":"reference/fractal_server/app/schemas/v1/applyworkflow/#fractal_server.app.schemas.v1.applyworkflow.ApplyWorkflowCreateV1","title":"<code>ApplyWorkflowCreateV1</code>","text":"<p>             Bases: <code>_ApplyWorkflowBaseV1</code></p> <p>Class for <code>ApplyWorkflow</code> creation.</p> <p>Attributes:</p> Name Type Description <code>first_task_index</code> <code>Optional[int]</code> <code>last_task_index</code> <code>Optional[int]</code> <code>slurm_account</code> <code>Optional[StrictStr]</code> Source code in <code>fractal_server/app/schemas/v1/applyworkflow.py</code> <pre><code>class ApplyWorkflowCreateV1(_ApplyWorkflowBaseV1):\n    \"\"\"\n    Class for `ApplyWorkflow` creation.\n\n    Attributes:\n        first_task_index:\n        last_task_index:\n        slurm_account:\n    \"\"\"\n\n    first_task_index: Optional[int] = None\n    last_task_index: Optional[int] = None\n    slurm_account: Optional[StrictStr] = None\n\n    # Validators\n    _worker_init = validator(\"worker_init\", allow_reuse=True)(\n        valstr(\"worker_init\")\n    )\n\n    @validator(\"first_task_index\", always=True)\n    def first_task_index_non_negative(cls, v, values):\n        \"\"\"\n        Check that `first_task_index` is non-negative.\n        \"\"\"\n        if v is not None and v &lt; 0:\n            raise ValueError(\n                f\"first_task_index cannot be negative (given: {v})\"\n            )\n        return v\n\n    @validator(\"last_task_index\", always=True)\n    def first_last_task_indices(cls, v, values):\n        \"\"\"\n        Check that `last_task_index` is non-negative, and that it is not\n        smaller than `first_task_index`.\n        \"\"\"\n        if v is not None and v &lt; 0:\n            raise ValueError(\n                f\"last_task_index cannot be negative (given: {v})\"\n            )\n\n        first_task_index = values.get(\"first_task_index\")\n        last_task_index = v\n        if first_task_index is not None and last_task_index is not None:\n            if first_task_index &gt; last_task_index:\n                raise ValueError(\n                    f\"{first_task_index=} cannot be larger than \"\n                    f\"{last_task_index=}\"\n                )\n        return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/applyworkflow/#fractal_server.app.schemas.v1.applyworkflow.ApplyWorkflowCreateV1.first_last_task_indices","title":"<code>first_last_task_indices(v, values)</code>","text":"<p>Check that <code>last_task_index</code> is non-negative, and that it is not smaller than <code>first_task_index</code>.</p> Source code in <code>fractal_server/app/schemas/v1/applyworkflow.py</code> <pre><code>@validator(\"last_task_index\", always=True)\ndef first_last_task_indices(cls, v, values):\n    \"\"\"\n    Check that `last_task_index` is non-negative, and that it is not\n    smaller than `first_task_index`.\n    \"\"\"\n    if v is not None and v &lt; 0:\n        raise ValueError(\n            f\"last_task_index cannot be negative (given: {v})\"\n        )\n\n    first_task_index = values.get(\"first_task_index\")\n    last_task_index = v\n    if first_task_index is not None and last_task_index is not None:\n        if first_task_index &gt; last_task_index:\n            raise ValueError(\n                f\"{first_task_index=} cannot be larger than \"\n                f\"{last_task_index=}\"\n            )\n    return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/applyworkflow/#fractal_server.app.schemas.v1.applyworkflow.ApplyWorkflowCreateV1.first_task_index_non_negative","title":"<code>first_task_index_non_negative(v, values)</code>","text":"<p>Check that <code>first_task_index</code> is non-negative.</p> Source code in <code>fractal_server/app/schemas/v1/applyworkflow.py</code> <pre><code>@validator(\"first_task_index\", always=True)\ndef first_task_index_non_negative(cls, v, values):\n    \"\"\"\n    Check that `first_task_index` is non-negative.\n    \"\"\"\n    if v is not None and v &lt; 0:\n        raise ValueError(\n            f\"first_task_index cannot be negative (given: {v})\"\n        )\n    return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/applyworkflow/#fractal_server.app.schemas.v1.applyworkflow.ApplyWorkflowReadV1","title":"<code>ApplyWorkflowReadV1</code>","text":"<p>             Bases: <code>_ApplyWorkflowBaseV1</code></p> <p>Class for <code>ApplyWorkflow</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <code>project_id</code> <code>Optional[int]</code> <code>project_dump</code> <code>ProjectDumpV1</code> <code>user_email</code> <code>str</code> <code>slurm_account</code> <code>Optional[str]</code> <code>workflow_id</code> <code>Optional[int]</code> <code>workflow_dump</code> <code>WorkflowDumpV1</code> <code>input_dataset_id</code> <code>Optional[int]</code> <code>input_dataset_dump</code> <code>DatasetDumpV1</code> <code>output_dataset_id</code> <code>Optional[int]</code> <code>output_dataset_dump</code> <code>DatasetDumpV1</code> <code>start_timestamp</code> <code>datetime</code> <code>end_timestamp</code> <code>Optional[datetime]</code> <code>status</code> <code>str</code> <code>log</code> <code>Optional[str]</code> <code>working_dir</code> <code>Optional[str]</code> <code>working_dir_user</code> <code>Optional[str]</code> <code>first_task_index</code> <code>Optional[int]</code> <code>last_task_index</code> <code>Optional[int]</code> Source code in <code>fractal_server/app/schemas/v1/applyworkflow.py</code> <pre><code>class ApplyWorkflowReadV1(_ApplyWorkflowBaseV1):\n    \"\"\"\n    Class for `ApplyWorkflow` read from database.\n\n    Attributes:\n        id:\n        project_id:\n        project_dump:\n        user_email:\n        slurm_account:\n        workflow_id:\n        workflow_dump:\n        input_dataset_id:\n        input_dataset_dump:\n        output_dataset_id:\n        output_dataset_dump:\n        start_timestamp:\n        end_timestamp:\n        status:\n        log:\n        working_dir:\n        working_dir_user:\n        first_task_index:\n        last_task_index:\n    \"\"\"\n\n    id: int\n    project_id: Optional[int]\n    project_dump: ProjectDumpV1\n    user_email: str\n    slurm_account: Optional[str]\n    workflow_id: Optional[int]\n    workflow_dump: WorkflowDumpV1\n    input_dataset_id: Optional[int]\n    input_dataset_dump: DatasetDumpV1\n    output_dataset_id: Optional[int]\n    output_dataset_dump: DatasetDumpV1\n    start_timestamp: datetime\n    end_timestamp: Optional[datetime]\n    status: str\n    log: Optional[str]\n    working_dir: Optional[str]\n    working_dir_user: Optional[str]\n    first_task_index: Optional[int]\n    last_task_index: Optional[int]\n\n    _start_timestamp = validator(\"start_timestamp\", allow_reuse=True)(\n        valutc(\"start_timestamp\")\n    )\n    _end_timestamp = validator(\"end_timestamp\", allow_reuse=True)(\n        valutc(\"end_timestamp\")\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/applyworkflow/#fractal_server.app.schemas.v1.applyworkflow.ApplyWorkflowUpdateV1","title":"<code>ApplyWorkflowUpdateV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class for updating a job status.</p> <p>Attributes:</p> Name Type Description <code>status</code> <code>JobStatusTypeV1</code> <p>New job status.</p> Source code in <code>fractal_server/app/schemas/v1/applyworkflow.py</code> <pre><code>class ApplyWorkflowUpdateV1(BaseModel):\n    \"\"\"\n    Class for updating a job status.\n\n    Attributes:\n        status: New job status.\n    \"\"\"\n\n    status: JobStatusTypeV1\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/applyworkflow/#fractal_server.app.schemas.v1.applyworkflow.JobStatusTypeV1","title":"<code>JobStatusTypeV1</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Define the available job statuses</p> <p>Attributes:</p> Name Type Description <code>SUBMITTED</code> <p>The job was created. This does not guarantee that it was also submitted to an executor (e.g. other errors could have prevented this), nor that it is actually running (e.g. SLURM jobs could be still in the queue).</p> <code>DONE</code> <p>The job successfully reached its end.</p> <code>FAILED</code> <p>The workflow terminated with an error.</p> Source code in <code>fractal_server/app/schemas/v1/applyworkflow.py</code> <pre><code>class JobStatusTypeV1(str, Enum):\n    \"\"\"\n    Define the available job statuses\n\n    Attributes:\n        SUBMITTED:\n            The job was created. This does not guarantee that it was also\n            submitted to an executor (e.g. other errors could have prevented\n            this), nor that it is actually running (e.g. SLURM jobs could be\n            still in the queue).\n        DONE:\n            The job successfully reached its end.\n        FAILED:\n            The workflow terminated with an error.\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/applyworkflow/#fractal_server.app.schemas.v1.applyworkflow._ApplyWorkflowBaseV1","title":"<code>_ApplyWorkflowBaseV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>ApplyWorkflow</code>.</p> <p>Attributes:</p> Name Type Description <code>worker_init</code> <code>Optional[str]</code> Source code in <code>fractal_server/app/schemas/v1/applyworkflow.py</code> <pre><code>class _ApplyWorkflowBaseV1(BaseModel):\n    \"\"\"\n    Base class for `ApplyWorkflow`.\n\n    Attributes:\n        worker_init:\n    \"\"\"\n\n    worker_init: Optional[str]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/","title":"dataset","text":""},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset.DatasetCreateV1","title":"<code>DatasetCreateV1</code>","text":"<p>             Bases: <code>_DatasetBaseV1</code></p> <p>Class for <code>Dataset</code> creation.</p> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class DatasetCreateV1(_DatasetBaseV1):\n    \"\"\"\n    Class for `Dataset` creation.\n    \"\"\"\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n    _type = validator(\"type\", allow_reuse=True)(valstr(\"type\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset.DatasetReadV1","title":"<code>DatasetReadV1</code>","text":"<p>             Bases: <code>_DatasetBaseV1</code></p> <p>Class for <code>Dataset</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <code>read_only</code> <code>bool</code> <code>resource_list</code> <code>list[ResourceReadV1]</code> <code>project_id</code> <code>int</code> <code>project</code> <code>ProjectReadV1</code> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class DatasetReadV1(_DatasetBaseV1):\n    \"\"\"\n    Class for `Dataset` read from database.\n\n    Attributes:\n        id:\n        read_only:\n        resource_list:\n        project_id:\n        project:\n    \"\"\"\n\n    id: int\n    resource_list: list[ResourceReadV1]\n    project_id: int\n    read_only: bool\n    project: ProjectReadV1\n    timestamp_created: datetime\n\n    _timestamp_created = validator(\"timestamp_created\", allow_reuse=True)(\n        valutc(\"timestamp_created\")\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset.DatasetStatusReadV1","title":"<code>DatasetStatusReadV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response type for the <code>/project/{project_id}/dataset/{dataset_id}/status/</code> endpoint</p> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class DatasetStatusReadV1(BaseModel):\n    \"\"\"\n    Response type for the\n    `/project/{project_id}/dataset/{dataset_id}/status/` endpoint\n    \"\"\"\n\n    status: Optional[\n        dict[\n            int,\n            WorkflowTaskStatusTypeV1,\n        ]\n    ] = None\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset.DatasetUpdateV1","title":"<code>DatasetUpdateV1</code>","text":"<p>             Bases: <code>_DatasetBaseV1</code></p> <p>Class for <code>Dataset</code> update.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[str]</code> <code>meta</code> <code>Optional[dict[str, Any]]</code> <code>history</code> <code>Optional[list[_DatasetHistoryItemV1]]</code> <code>read_only</code> <code>Optional[bool]</code> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class DatasetUpdateV1(_DatasetBaseV1):\n    \"\"\"\n    Class for `Dataset` update.\n\n    Attributes:\n        name:\n        meta:\n        history:\n        read_only:\n    \"\"\"\n\n    name: Optional[str]\n    meta: Optional[dict[str, Any]] = None\n    history: Optional[list[_DatasetHistoryItemV1]] = None\n    read_only: Optional[bool]\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n    _type = validator(\"type\", allow_reuse=True)(valstr(\"type\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset.ResourceCreateV1","title":"<code>ResourceCreateV1</code>","text":"<p>             Bases: <code>_ResourceBaseV1</code></p> <p>Class for <code>Resource</code> creation.</p> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class ResourceCreateV1(_ResourceBaseV1):\n    \"\"\"\n    Class for `Resource` creation.\n    \"\"\"\n\n    # Validators\n    _path = validator(\"path\", allow_reuse=True)(val_absolute_path(\"path\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset.ResourceReadV1","title":"<code>ResourceReadV1</code>","text":"<p>             Bases: <code>_ResourceBaseV1</code></p> <p>Class for <code>Resource</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <code>dataset_id</code> <code>int</code> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class ResourceReadV1(_ResourceBaseV1):\n    \"\"\"\n    Class for `Resource` read from database.\n\n    Attributes:\n        id:\n        dataset_id:\n    \"\"\"\n\n    id: int\n    dataset_id: int\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset.ResourceUpdateV1","title":"<code>ResourceUpdateV1</code>","text":"<p>             Bases: <code>_ResourceBaseV1</code></p> <p>Class for <code>Resource</code> update.</p> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class ResourceUpdateV1(_ResourceBaseV1):\n    \"\"\"\n    Class for `Resource` update.\n    \"\"\"\n\n    # Validators\n    _path = validator(\"path\", allow_reuse=True)(val_absolute_path(\"path\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset._DatasetBaseV1","title":"<code>_DatasetBaseV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>Dataset</code>.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <code>type</code> <code>Optional[str]</code> <code>meta</code> <code>dict[str, Any]</code> <code>history</code> <code>list[_DatasetHistoryItemV1]</code> <code>read_only</code> <code>bool</code> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class _DatasetBaseV1(BaseModel):\n    \"\"\"\n    Base class for `Dataset`.\n\n    Attributes:\n        name:\n        type:\n        meta:\n        history:\n        read_only:\n    \"\"\"\n\n    name: str\n    type: Optional[str]\n    meta: dict[str, Any] = Field(default={})\n    history: list[_DatasetHistoryItemV1] = Field(default=[])\n    read_only: bool = False\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset._DatasetHistoryItemV1","title":"<code>_DatasetHistoryItemV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class for an item of <code>Dataset.history</code>.</p> <p>Attributes:</p> Name Type Description <code>workflowtask</code> <code>WorkflowTaskDumpV1</code> <code>status</code> <code>WorkflowTaskStatusTypeV1</code> <code>parallelization</code> <code>Optional[dict]</code> <p>If provided, it includes keys <code>parallelization_level</code> and <code>component_list</code>.</p> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class _DatasetHistoryItemV1(BaseModel):\n    \"\"\"\n    Class for an item of `Dataset.history`.\n\n    Attributes:\n        workflowtask:\n        status:\n        parallelization: If provided, it includes keys `parallelization_level`\n            and `component_list`.\n    \"\"\"\n\n    workflowtask: WorkflowTaskDumpV1\n    status: WorkflowTaskStatusTypeV1\n    parallelization: Optional[dict]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dataset/#fractal_server.app.schemas.v1.dataset._ResourceBaseV1","title":"<code>_ResourceBaseV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>Resource</code>.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> Source code in <code>fractal_server/app/schemas/v1/dataset.py</code> <pre><code>class _ResourceBaseV1(BaseModel):\n    \"\"\"\n    Base class for `Resource`.\n\n    Attributes:\n        path:\n    \"\"\"\n\n    path: str\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/dumps/","title":"dumps","text":"<p>Dump models differ from their Read counterpart in that: * They are directly JSON-able, without any additional encoder. * They may only include a subset of the Read attributes.</p> <p>These models are used in at least two situations: 1. In the \"*_dump\" attributes of ApplyWorkflow models; 2. In the <code>_DatasetHistoryItem.workflowtask</code> model, to trim its size.</p>"},{"location":"reference/fractal_server/app/schemas/v1/manifest/","title":"manifest","text":""},{"location":"reference/fractal_server/app/schemas/v1/manifest/#fractal_server.app.schemas.v1.manifest.ManifestV1","title":"<code>ManifestV1</code>","text":"<p>             Bases: <code>_ManifestBaseV1</code></p> <p>Manifest schema version 1.</p> <p>Attributes:</p> Name Type Description <code>task_list</code> <code>list[TaskManifestV1]</code> Source code in <code>fractal_server/app/schemas/v1/manifest.py</code> <pre><code>class ManifestV1(_ManifestBaseV1):\n    \"\"\"\n    Manifest schema version 1.\n\n    Attributes:\n        task_list:\n    \"\"\"\n\n    task_list: list[TaskManifestV1]\n\n    @validator(\"manifest_version\")\n    def manifest_version_1(cls, value):\n        if value != \"1\":\n            raise ValueError(f\"Wrong manifest version (given {value})\")\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/manifest/#fractal_server.app.schemas.v1.manifest.TaskManifestV1","title":"<code>TaskManifestV1</code>","text":"<p>             Bases: <code>_TaskManifestBaseV1</code></p> <p>Task manifest schema version 1.</p> Source code in <code>fractal_server/app/schemas/v1/manifest.py</code> <pre><code>class TaskManifestV1(_TaskManifestBaseV1):\n    \"\"\"\n    Task manifest schema version 1.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/manifest/#fractal_server.app.schemas.v1.manifest._ManifestBaseV1","title":"<code>_ManifestBaseV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>ManifestV1</code>.</p> <p>Packages containing tasks are required to include a special file <code>__FRACTAL_MANIFEST__.json</code> in order to be discovered and used by Fractal.</p> <p>This model class and the model classes it depends on provide the base schema to read, write and validate manifests.</p> <p>Attributes:</p> Name Type Description <code>manifest_version</code> <code>str</code> <p>A version string that provides indication for compatibility between manifests as the schema evolves. This is for instance used by Fractal to determine which subclass of the present base class needs be used to read and validate the input.</p> <code>task_list</code> <p>list[TaskManifestType] The list of tasks, represented as specified by subclasses of the _TaskManifestBase (a.k.a. TaskManifestType)</p> <code>has_args_schemas</code> <code>bool</code> <p><code>True</code> if the manifest incldues JSON Schemas for the arguments of each task.</p> <code>args_schema_version</code> <code>Optional[str]</code> <p>Label of how <code>args_schema</code>s were generated (e.g. <code>pydantic_v1</code>).</p> Source code in <code>fractal_server/app/schemas/v1/manifest.py</code> <pre><code>class _ManifestBaseV1(BaseModel):\n    \"\"\"\n    Base class for `ManifestV1`.\n\n    Packages containing tasks are required to include a special file\n    `__FRACTAL_MANIFEST__.json` in order to be discovered and used by Fractal.\n\n    This model class and the model classes it depends on provide the base\n    schema to read, write and validate manifests.\n\n    Attributes:\n        manifest_version:\n            A version string that provides indication for compatibility between\n            manifests as the schema evolves. This is for instance used by\n            Fractal to determine which subclass of the present base class needs\n            be used to read and validate the input.\n        task_list : list[TaskManifestType]\n            The list of tasks, represented as specified by subclasses of the\n            _TaskManifestBase (a.k.a. TaskManifestType)\n        has_args_schemas:\n            `True` if the manifest incldues JSON Schemas for the arguments of\n            each task.\n        args_schema_version:\n            Label of how `args_schema`s were generated (e.g. `pydantic_v1`).\n    \"\"\"\n\n    manifest_version: str\n    task_list: list[TaskManifestType]\n    has_args_schemas: bool = False\n    args_schema_version: Optional[str]\n\n    @root_validator()\n    def _check_args_schemas_are_present(cls, values):\n        has_args_schemas = values[\"has_args_schemas\"]\n        task_list = values[\"task_list\"]\n        if has_args_schemas:\n            for task in task_list:\n                if task.args_schema is None:\n                    raise ValueError(\n                        f'has_args_schemas={has_args_schemas} but task \"'\n                        f'{task.name}\" has args_schema={task.args_schema}.'\n                    )\n        return values\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/manifest/#fractal_server.app.schemas.v1.manifest._TaskManifestBaseV1","title":"<code>_TaskManifestBaseV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>TaskManifestV1</code>.</p> <p>Represents a task within a manfest</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The task name</p> <code>executable</code> <code>str</code> <p>Path to the executable relative to the package root</p> <p>Note: by package root we mean \"as it will be installed\". If a package <code>Pkg</code> installs in the folder <code>pkg</code> the executable <code>pkg/executable.py</code>, this attribute must contain only <code>executable.py</code>.</p> <code>input_type</code> <code>str</code> <p>The input type accepted by the task</p> <code>output_type</code> <code>str</code> <p>The output type returned by the task</p> <code>meta</code> <code>Optional[dict[str, Any]]</code> <p>Additional information about the package, such as hash of the executable, specific runtime requirements (e.g., need_gpu=True), etc.</p> <code>args_schema</code> <code>Optional[dict[str, Any]]</code> <p>JSON Schema for task arguments</p> <code>docs_info</code> <code>Optional[str]</code> <p>Additional information about the Task, coming from the docstring.</p> <code>docs_link</code> <code>Optional[HttpUrl]</code> <p>Link to Task docs.</p> Source code in <code>fractal_server/app/schemas/v1/manifest.py</code> <pre><code>class _TaskManifestBaseV1(BaseModel):\n    \"\"\"\n    Base class for `TaskManifestV1`.\n\n    Represents a task within a manfest\n\n    Attributes:\n        name:\n            The task name\n        executable:\n            Path to the executable relative to the package root\n\n            Note: by package root we mean \"as it will be installed\". If a\n            package `Pkg` installs in the folder `pkg` the executable\n            `pkg/executable.py`, this attribute must contain only\n            `executable.py`.\n        input_type:\n            The input type accepted by the task\n        output_type:\n            The output type returned by the task\n        meta:\n            Additional information about the package, such as hash of the\n            executable, specific runtime requirements (e.g., need_gpu=True),\n            etc.\n        args_schema:\n            JSON Schema for task arguments\n        docs_info:\n            Additional information about the Task, coming from the docstring.\n        docs_link:\n            Link to Task docs.\n    \"\"\"\n\n    name: str\n    executable: str\n    input_type: str\n    output_type: str\n    meta: Optional[dict[str, Any]] = Field(default_factory=dict)\n    args_schema: Optional[dict[str, Any]]\n    docs_info: Optional[str]\n    docs_link: Optional[HttpUrl]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/project/","title":"project","text":""},{"location":"reference/fractal_server/app/schemas/v1/project/#fractal_server.app.schemas.v1.project.ProjectCreateV1","title":"<code>ProjectCreateV1</code>","text":"<p>             Bases: <code>_ProjectBaseV1</code></p> <p>Class for <code>Project</code> creation.</p> Source code in <code>fractal_server/app/schemas/v1/project.py</code> <pre><code>class ProjectCreateV1(_ProjectBaseV1):\n    \"\"\"\n    Class for `Project` creation.\n    \"\"\"\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/project/#fractal_server.app.schemas.v1.project.ProjectReadV1","title":"<code>ProjectReadV1</code>","text":"<p>             Bases: <code>_ProjectBaseV1</code></p> <p>Class for <code>Project</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <code>name</code> <code>int</code> <code>read_only</code> <code>int</code> Source code in <code>fractal_server/app/schemas/v1/project.py</code> <pre><code>class ProjectReadV1(_ProjectBaseV1):\n    \"\"\"\n    Class for `Project` read from database.\n\n    Attributes:\n        id:\n        name:\n        read_only:\n    \"\"\"\n\n    id: int\n    timestamp_created: datetime\n\n    _timestamp_created = validator(\"timestamp_created\", allow_reuse=True)(\n        valutc(\"timestamp_created\")\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/project/#fractal_server.app.schemas.v1.project.ProjectUpdateV1","title":"<code>ProjectUpdateV1</code>","text":"<p>             Bases: <code>_ProjectBaseV1</code></p> <p>Class for <code>Project</code> update.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[str]</code> <code>read_only</code> <code>Optional[bool]</code> Source code in <code>fractal_server/app/schemas/v1/project.py</code> <pre><code>class ProjectUpdateV1(_ProjectBaseV1):\n    \"\"\"\n    Class for `Project` update.\n\n    Attributes:\n        name:\n        read_only:\n    \"\"\"\n\n    name: Optional[str]\n    read_only: Optional[bool]\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/project/#fractal_server.app.schemas.v1.project._ProjectBaseV1","title":"<code>_ProjectBaseV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>Project</code>.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <code>read_only</code> <code>bool</code> Source code in <code>fractal_server/app/schemas/v1/project.py</code> <pre><code>class _ProjectBaseV1(BaseModel):\n    \"\"\"\n    Base class for `Project`.\n\n    Attributes:\n        name:\n        read_only:\n    \"\"\"\n\n    name: str\n    read_only: bool = False\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/task/","title":"task","text":""},{"location":"reference/fractal_server/app/schemas/v1/task/#fractal_server.app.schemas.v1.task.TaskCreateV1","title":"<code>TaskCreateV1</code>","text":"<p>             Bases: <code>_TaskBaseV1</code></p> <p>Class for <code>Task</code> creation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <code>command</code> <code>str</code> <code>input_type</code> <code>str</code> <code>output_type</code> <code>str</code> <code>meta</code> <code>Optional[dict[str, Any]]</code> <code>version</code> <code>Optional[str]</code> <code>args_schema</code> <code>Optional[dict[str, Any]]</code> <code>args_schema_version</code> <code>Optional[str]</code> <code>docs_info</code> <code>Optional[str]</code> <code>docs_link</code> <code>Optional[HttpUrl]</code> Source code in <code>fractal_server/app/schemas/v1/task.py</code> <pre><code>class TaskCreateV1(_TaskBaseV1):\n    \"\"\"\n    Class for `Task` creation.\n\n    Attributes:\n        name:\n        command:\n        input_type:\n        output_type:\n        meta:\n        version:\n        args_schema:\n        args_schema_version:\n        docs_info:\n        docs_link:\n    \"\"\"\n\n    name: str\n    command: str\n    input_type: str\n    output_type: str\n    meta: Optional[dict[str, Any]] = Field(default={})\n    version: Optional[str]\n    args_schema: Optional[dict[str, Any]]\n    args_schema_version: Optional[str]\n    docs_info: Optional[str]\n    docs_link: Optional[HttpUrl]\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n    _input_type = validator(\"input_type\", allow_reuse=True)(\n        valstr(\"input_type\")\n    )\n    _output_type = validator(\"output_type\", allow_reuse=True)(\n        valstr(\"output_type\")\n    )\n    _command = validator(\"command\", allow_reuse=True)(valstr(\"command\"))\n    _version = validator(\"version\", allow_reuse=True)(valstr(\"version\"))\n    _args_schema_version = validator(\"args_schema_version\", allow_reuse=True)(\n        valstr(\"args_schema_version\")\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/task/#fractal_server.app.schemas.v1.task.TaskExportV1","title":"<code>TaskExportV1</code>","text":"<p>             Bases: <code>_TaskBaseV1</code></p> <p>Class for <code>Task</code> export.</p> Source code in <code>fractal_server/app/schemas/v1/task.py</code> <pre><code>class TaskExportV1(_TaskBaseV1):\n    \"\"\"\n    Class for `Task` export.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/task/#fractal_server.app.schemas.v1.task.TaskImportV1","title":"<code>TaskImportV1</code>","text":"<p>             Bases: <code>_TaskBaseV1</code></p> <p>Class for <code>Task</code> import.</p> Source code in <code>fractal_server/app/schemas/v1/task.py</code> <pre><code>class TaskImportV1(_TaskBaseV1):\n    \"\"\"\n    Class for `Task` import.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/task/#fractal_server.app.schemas.v1.task.TaskReadV1","title":"<code>TaskReadV1</code>","text":"<p>             Bases: <code>_TaskBaseV1</code></p> <p>Class for <code>Task</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <code>name</code> <code>str</code> <code>command</code> <code>str</code> <code>input_type</code> <code>str</code> <code>output_type</code> <code>str</code> <code>meta</code> <code>Optional[dict[str, Any]]</code> <code>version</code> <code>Optional[str]</code> <code>args_schema</code> <code>Optional[dict[str, Any]]</code> <code>args_schema_version</code> <code>Optional[str]</code> <code>docs_info</code> <code>Optional[str]</code> <code>docs_link</code> <code>Optional[HttpUrl]</code> Source code in <code>fractal_server/app/schemas/v1/task.py</code> <pre><code>class TaskReadV1(_TaskBaseV1):\n    \"\"\"\n    Class for `Task` read from database.\n\n    Attributes:\n        id:\n        name:\n        command:\n        input_type:\n        output_type:\n        meta:\n        version:\n        args_schema:\n        args_schema_version:\n        docs_info:\n        docs_link:\n    \"\"\"\n\n    id: int\n    name: str\n    command: str\n    input_type: str\n    output_type: str\n    meta: Optional[dict[str, Any]] = Field(default={})\n    owner: Optional[str]\n    version: Optional[str]\n    args_schema: Optional[dict[str, Any]]\n    args_schema_version: Optional[str]\n    docs_info: Optional[str]\n    docs_link: Optional[HttpUrl]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/task/#fractal_server.app.schemas.v1.task.TaskUpdateV1","title":"<code>TaskUpdateV1</code>","text":"<p>             Bases: <code>_TaskBaseV1</code></p> <p>Class for <code>Task</code> update.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[str]</code> <code>input_type</code> <code>Optional[str]</code> <code>output_type</code> <code>Optional[str]</code> <code>command</code> <code>Optional[str]</code> <code>source</code> <code>Optional[str]</code> <code>meta</code> <code>Optional[dict[str, Any]]</code> <code>version</code> <code>Optional[str]</code> <code>args_schema</code> <code>Optional[dict[str, Any]]</code> <code>args_schema_version</code> <code>Optional[str]</code> <code>docs_info</code> <code>Optional[str]</code> <code>docs_link</code> <code>Optional[HttpUrl]</code> Source code in <code>fractal_server/app/schemas/v1/task.py</code> <pre><code>class TaskUpdateV1(_TaskBaseV1):\n    \"\"\"\n    Class for `Task` update.\n\n    Attributes:\n        name:\n        input_type:\n        output_type:\n        command:\n        source:\n        meta:\n        version:\n        args_schema:\n        args_schema_version:\n        docs_info:\n        docs_link:\n    \"\"\"\n\n    name: Optional[str]\n    input_type: Optional[str]\n    output_type: Optional[str]\n    command: Optional[str]\n    source: Optional[str]\n    meta: Optional[dict[str, Any]]\n    version: Optional[str]\n    args_schema: Optional[dict[str, Any]]\n    args_schema_version: Optional[str]\n    docs_info: Optional[str]\n    docs_link: Optional[HttpUrl]\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n    _input_type = validator(\"input_type\", allow_reuse=True)(\n        valstr(\"input_type\")\n    )\n    _output_type = validator(\"output_type\", allow_reuse=True)(\n        valstr(\"output_type\")\n    )\n    _command = validator(\"command\", allow_reuse=True)(valstr(\"command\"))\n    _version = validator(\"version\", allow_reuse=True)(\n        valstr(\"version\", accept_none=True)\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/task/#fractal_server.app.schemas.v1.task._TaskBaseV1","title":"<code>_TaskBaseV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>Task</code>.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>This is the information is used to match tasks across fractal installations when a workflow is imported.</p> Source code in <code>fractal_server/app/schemas/v1/task.py</code> <pre><code>class _TaskBaseV1(BaseModel):\n    \"\"\"\n\n    Base class for `Task`.\n\n    Attributes:\n        source:\n            This is the information is used to match tasks across fractal\n            installations when a workflow is imported.\n    \"\"\"\n\n    source: str\n    _source = validator(\"source\", allow_reuse=True)(valstr(\"source\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/task_collection/","title":"task_collection","text":""},{"location":"reference/fractal_server/app/schemas/v1/task_collection/#fractal_server.app.schemas.v1.task_collection.TaskCollectPipV1","title":"<code>TaskCollectPipV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>TaskCollectPip class</p> <p>This class only encodes the attributes required to trigger a task-collection operation. Other attributes (that are assigned during task collection) are defined as part of fractal-server.</p> <p>Two cases are supported:</p> <pre><code>1. `package` is the path of a local wheel file;\n2. `package` is the name of a package that can be installed via `pip`.\n</code></pre> <p>Attributes:</p> Name Type Description <code>package</code> <code>str</code> <p>The name of a <code>pip</code>-installable package, or the path to a local wheel file.</p> <code>package_version</code> <code>Optional[str]</code> <p>Version of the package</p> <code>package_extras</code> <code>Optional[str]</code> <p>Package extras to include in the <code>pip install</code> command</p> <code>python_version</code> <code>Optional[str]</code> <p>Python version to install and run the package tasks</p> <code>pinned_package_versions</code> <code>Optional[dict[str, str]]</code> <p>dictionary 'package':'version' used to pin versions for specific packages.</p> Source code in <code>fractal_server/app/schemas/v1/task_collection.py</code> <pre><code>class TaskCollectPipV1(BaseModel):\n    \"\"\"\n    TaskCollectPip class\n\n    This class only encodes the attributes required to trigger a\n    task-collection operation. Other attributes (that are assigned *during*\n    task collection) are defined as part of fractal-server.\n\n    Two cases are supported:\n\n        1. `package` is the path of a local wheel file;\n        2. `package` is the name of a package that can be installed via `pip`.\n\n\n    Attributes:\n        package:\n            The name of a `pip`-installable package, or the path to a local\n            wheel file.\n        package_version: Version of the package\n        package_extras: Package extras to include in the `pip install` command\n        python_version: Python version to install and run the package tasks\n        pinned_package_versions:\n            dictionary 'package':'version' used to pin versions for specific\n            packages.\n\n    \"\"\"\n\n    package: str\n    package_version: Optional[str] = None\n    package_extras: Optional[str] = None\n    python_version: Optional[str] = None\n    pinned_package_versions: Optional[dict[str, str]] = None\n\n    _package_extras = validator(\"package_extras\", allow_reuse=True)(\n        valstr(\"package_extras\")\n    )\n    _python_version = validator(\"python_version\", allow_reuse=True)(\n        valstr(\"python_version\")\n    )\n\n    @validator(\"package\")\n    def package_validator(cls, value):\n        if \"/\" in value:\n            if not value.endswith(\".whl\"):\n                raise ValueError(\n                    \"Local-package path must be a wheel file \"\n                    f\"(given {value}).\"\n                )\n            if not Path(value).is_absolute():\n                raise ValueError(\n                    f\"Local-package path must be absolute: (given {value}).\"\n                )\n        return value\n\n    @validator(\"package_version\")\n    def package_version_validator(cls, v, values):\n\n        valstr(\"package_version\")(v)\n\n        if values[\"package\"].endswith(\".whl\"):\n            raise ValueError(\n                \"Cannot provide version when package is a Wheel file.\"\n            )\n        return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/task_collection/#fractal_server.app.schemas.v1.task_collection.TaskCollectStatusV1","title":"<code>TaskCollectStatusV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>TaskCollectStatus class</p> <p>Attributes:</p> Name Type Description <code>status</code> <code>Literal['pending', 'installing', 'collecting', 'fail', 'OK']</code> <code>package</code> <code>str</code> <code>venv_path</code> <code>Path</code> <code>task_list</code> <code>Optional[list[TaskReadV1]]</code> <code>log</code> <code>Optional[str]</code> <code>info</code> <code>Optional[str]</code> Source code in <code>fractal_server/app/schemas/v1/task_collection.py</code> <pre><code>class TaskCollectStatusV1(BaseModel):\n    \"\"\"\n    TaskCollectStatus class\n\n    Attributes:\n        status:\n        package:\n        venv_path:\n        task_list:\n        log:\n        info:\n    \"\"\"\n\n    status: Literal[\"pending\", \"installing\", \"collecting\", \"fail\", \"OK\"]\n    package: str\n    venv_path: Path\n    task_list: Optional[list[TaskReadV1]] = Field(default=[])\n    log: Optional[str]\n    info: Optional[str]\n\n    def sanitised_dict(self):\n        \"\"\"\n        Return `self.dict()` after casting `self.venv_path` to a string\n        \"\"\"\n        d = self.dict()\n        d[\"venv_path\"] = str(self.venv_path)\n        return d\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/task_collection/#fractal_server.app.schemas.v1.task_collection.TaskCollectStatusV1.sanitised_dict","title":"<code>sanitised_dict()</code>","text":"<p>Return <code>self.dict()</code> after casting <code>self.venv_path</code> to a string</p> Source code in <code>fractal_server/app/schemas/v1/task_collection.py</code> <pre><code>def sanitised_dict(self):\n    \"\"\"\n    Return `self.dict()` after casting `self.venv_path` to a string\n    \"\"\"\n    d = self.dict()\n    d[\"venv_path\"] = str(self.venv_path)\n    return d\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/","title":"workflow","text":""},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowCreateV1","title":"<code>WorkflowCreateV1</code>","text":"<p>             Bases: <code>_WorkflowBaseV1</code></p> <p>Task for <code>Workflow</code> creation.</p> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowCreateV1(_WorkflowBaseV1):\n    \"\"\"\n    Task for `Workflow` creation.\n    \"\"\"\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowExportV1","title":"<code>WorkflowExportV1</code>","text":"<p>             Bases: <code>_WorkflowBaseV1</code></p> <p>Class for <code>Workflow</code> export.</p> <p>Attributes:</p> Name Type Description <code>task_list</code> <code>list[WorkflowTaskExportV1]</code> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowExportV1(_WorkflowBaseV1):\n    \"\"\"\n    Class for `Workflow` export.\n\n    Attributes:\n        task_list:\n    \"\"\"\n\n    task_list: list[WorkflowTaskExportV1]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowImportV1","title":"<code>WorkflowImportV1</code>","text":"<p>             Bases: <code>_WorkflowBaseV1</code></p> <p>Class for <code>Workflow</code> import.</p> <p>Attributes:</p> Name Type Description <code>task_list</code> <code>list[WorkflowTaskImportV1]</code> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowImportV1(_WorkflowBaseV1):\n    \"\"\"\n    Class for `Workflow` import.\n\n    Attributes:\n        task_list:\n    \"\"\"\n\n    task_list: list[WorkflowTaskImportV1]\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowReadV1","title":"<code>WorkflowReadV1</code>","text":"<p>             Bases: <code>_WorkflowBaseV1</code></p> <p>Task for <code>Workflow</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <code>project_id</code> <code>int</code> <code>task_list</code> <code>list[WorkflowTaskReadV1]</code> <code>project</code> <code>ProjectReadV1</code> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowReadV1(_WorkflowBaseV1):\n    \"\"\"\n    Task for `Workflow` read from database.\n\n    Attributes:\n        id:\n        project_id:\n        task_list:\n        project:\n    \"\"\"\n\n    id: int\n    project_id: int\n    task_list: list[WorkflowTaskReadV1]\n    project: ProjectReadV1\n    timestamp_created: datetime\n\n    _timestamp_created = validator(\"timestamp_created\", allow_reuse=True)(\n        valutc(\"timestamp_created\")\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowTaskCreateV1","title":"<code>WorkflowTaskCreateV1</code>","text":"<p>             Bases: <code>_WorkflowTaskBaseV1</code></p> <p>Class for <code>WorkflowTask</code> creation.</p> <p>Attributes:</p> Name Type Description <code>order</code> <code>Optional[int]</code> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowTaskCreateV1(_WorkflowTaskBaseV1):\n    \"\"\"\n    Class for `WorkflowTask` creation.\n\n    Attributes:\n        order:\n    \"\"\"\n\n    order: Optional[int]\n    # Validators\n    _order = validator(\"order\", allow_reuse=True)(valint(\"order\", min_val=0))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowTaskExportV1","title":"<code>WorkflowTaskExportV1</code>","text":"<p>             Bases: <code>_WorkflowTaskBaseV1</code></p> <p>Class for <code>WorkflowTask</code> export.</p> <p>Attributes:</p> Name Type Description <code>task</code> <code>TaskExportV1</code> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowTaskExportV1(_WorkflowTaskBaseV1):\n    \"\"\"\n    Class for `WorkflowTask` export.\n\n    Attributes:\n        task:\n    \"\"\"\n\n    task: TaskExportV1\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowTaskImportV1","title":"<code>WorkflowTaskImportV1</code>","text":"<p>             Bases: <code>_WorkflowTaskBaseV1</code></p> <p>Class for <code>WorkflowTask</code> import.</p> <p>Attributes:</p> Name Type Description <code>task</code> <code>TaskImportV1</code> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowTaskImportV1(_WorkflowTaskBaseV1):\n    \"\"\"\n    Class for `WorkflowTask` import.\n\n    Attributes:\n        task:\n    \"\"\"\n\n    task: TaskImportV1\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowTaskReadV1","title":"<code>WorkflowTaskReadV1</code>","text":"<p>             Bases: <code>_WorkflowTaskBaseV1</code></p> <p>Class for <code>WorkflowTask</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <code>order</code> <code>Optional[int]</code> <code>workflow_id</code> <code>int</code> <code>task_id</code> <code>int</code> <code>task</code> <code>TaskReadV1</code> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowTaskReadV1(_WorkflowTaskBaseV1):\n    \"\"\"\n    Class for `WorkflowTask` read from database.\n\n    Attributes:\n        id:\n        order:\n        workflow_id:\n        task_id:\n        task:\n    \"\"\"\n\n    id: int\n    order: Optional[int]\n    workflow_id: int\n    task_id: int\n    task: TaskReadV1\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowTaskStatusTypeV1","title":"<code>WorkflowTaskStatusTypeV1</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Define the available values for the status of a <code>WorkflowTask</code>.</p> <p>This model is used within the <code>Dataset.history</code> attribute, which is constructed in the runner and then used in the API (e.g. in the <code>api/v1/project/{project_id}/dataset/{dataset_id}/status</code> endpoint).</p> <p>Attributes:</p> Name Type Description <code>SUBMITTED</code> <p>The <code>WorkflowTask</code> is part of a running job.</p> <code>DONE</code> <p>The most-recent execution of this <code>WorkflowTask</code> was successful.</p> <code>FAILED</code> <p>The most-recent execution of this <code>WorkflowTask</code> failed.</p> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowTaskStatusTypeV1(str, Enum):\n    \"\"\"\n    Define the available values for the status of a `WorkflowTask`.\n\n    This model is used within the `Dataset.history` attribute, which is\n    constructed in the runner and then used in the API (e.g. in the\n    `api/v1/project/{project_id}/dataset/{dataset_id}/status` endpoint).\n\n    Attributes:\n        SUBMITTED: The `WorkflowTask` is part of a running job.\n        DONE: The most-recent execution of this `WorkflowTask` was successful.\n        FAILED: The most-recent execution of this `WorkflowTask` failed.\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowTaskUpdateV1","title":"<code>WorkflowTaskUpdateV1</code>","text":"<p>             Bases: <code>_WorkflowTaskBaseV1</code></p> <p>Class for <code>WorkflowTask</code> update.</p> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowTaskUpdateV1(_WorkflowTaskBaseV1):\n    \"\"\"\n    Class for `WorkflowTask` update.\n    \"\"\"\n\n    # Validators\n    @validator(\"meta\")\n    def check_no_parallelisation_level(cls, m):\n        if \"parallelization_level\" in m:\n            raise ValueError(\n                \"Overriding task parallelization level currently not allowed\"\n            )\n        return m\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow.WorkflowUpdateV1","title":"<code>WorkflowUpdateV1</code>","text":"<p>             Bases: <code>_WorkflowBaseV1</code></p> <p>Task for <code>Workflow</code> update.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[str]</code> <code>reordered_workflowtask_ids</code> <code>Optional[list[int]]</code> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class WorkflowUpdateV1(_WorkflowBaseV1):\n    \"\"\"\n    Task for `Workflow` update.\n\n    Attributes:\n        name:\n        reordered_workflowtask_ids:\n    \"\"\"\n\n    name: Optional[str]\n    reordered_workflowtask_ids: Optional[list[int]]\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n\n    @validator(\"reordered_workflowtask_ids\")\n    def check_positive_and_unique(cls, value):\n        if any(i &lt; 0 for i in value):\n            raise ValueError(\"Negative `id` in `reordered_workflowtask_ids`\")\n        if len(value) != len(set(value)):\n            raise ValueError(\"`reordered_workflowtask_ids` has repetitions\")\n        return value\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow._WorkflowBaseV1","title":"<code>_WorkflowBaseV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>Workflow</code>.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Workflow name.</p> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class _WorkflowBaseV1(BaseModel):\n    \"\"\"\n    Base class for `Workflow`.\n\n    Attributes:\n        name: Workflow name.\n    \"\"\"\n\n    name: str\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v1/workflow/#fractal_server.app.schemas.v1.workflow._WorkflowTaskBaseV1","title":"<code>_WorkflowTaskBaseV1</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for <code>WorkflowTask</code>.</p> Source code in <code>fractal_server/app/schemas/v1/workflow.py</code> <pre><code>class _WorkflowTaskBaseV1(BaseModel):\n    \"\"\"\n    Base class for `WorkflowTask`.\n    \"\"\"\n\n    meta: Optional[dict[str, Any]] = None\n    args: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/","title":"v2","text":""},{"location":"reference/fractal_server/app/schemas/v2/dataset/","title":"dataset","text":""},{"location":"reference/fractal_server/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset.DatasetExportV2","title":"<code>DatasetExportV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class for <code>Dataset</code> export.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <code>zarr_dir</code> <code>str</code> <code>images</code> <code>list[SingleImage]</code> <code>filters</code> <code>Filters</code> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class DatasetExportV2(BaseModel):\n    \"\"\"\n    Class for `Dataset` export.\n\n    Attributes:\n        name:\n        zarr_dir:\n        images:\n        filters:\n    \"\"\"\n\n    name: str\n    zarr_dir: str\n    images: list[SingleImage]\n    filters: Filters\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset.DatasetImportV2","title":"<code>DatasetImportV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class for <code>Dataset</code> import.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <code>zarr_dir</code> <code>str</code> <code>images</code> <code>list[SingleImage]</code> <code>filters</code> <code>Filters</code> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class DatasetImportV2(BaseModel):\n    \"\"\"\n    Class for `Dataset` import.\n\n    Attributes:\n        name:\n        zarr_dir:\n        images:\n        filters:\n    \"\"\"\n\n    class Config:\n        extra = \"forbid\"\n\n    name: str\n    zarr_dir: str\n    images: list[SingleImage] = Field(default_factory=[])\n    filters: Filters = Field(default_factory=Filters)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset._DatasetHistoryItemV2","title":"<code>_DatasetHistoryItemV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class for an item of <code>Dataset.history</code>.</p> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class _DatasetHistoryItemV2(BaseModel):\n    \"\"\"\n    Class for an item of `Dataset.history`.\n    \"\"\"\n\n    workflowtask: WorkflowTaskDumpV2\n    status: WorkflowTaskStatusTypeV2\n    parallelization: Optional[dict]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/dumps/","title":"dumps","text":"<p>Dump models differ from their Read counterpart in that: * They are directly JSON-able, without any additional encoder. * They may only include a subset of the Read attributes.</p> <p>These models are used in at least two situations: 1. In the \"*_dump\" attributes of Job models; 2. In the <code>_DatasetHistoryItem.workflowtask</code> model, to trim its size.</p>"},{"location":"reference/fractal_server/app/schemas/v2/job/","title":"job","text":""},{"location":"reference/fractal_server/app/schemas/v2/job/#fractal_server.app.schemas.v2.job.JobCreateV2","title":"<code>JobCreateV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>fractal_server/app/schemas/v2/job.py</code> <pre><code>class JobCreateV2(BaseModel, extra=Extra.forbid):\n\n    first_task_index: Optional[int] = None\n    last_task_index: Optional[int] = None\n    slurm_account: Optional[StrictStr] = None\n    worker_init: Optional[str]\n\n    # Validators\n    _worker_init = validator(\"worker_init\", allow_reuse=True)(\n        valstr(\"worker_init\")\n    )\n\n    @validator(\"first_task_index\", always=True)\n    def first_task_index_non_negative(cls, v, values):\n        \"\"\"\n        Check that `first_task_index` is non-negative.\n        \"\"\"\n        if v is not None and v &lt; 0:\n            raise ValueError(\n                f\"first_task_index cannot be negative (given: {v})\"\n            )\n        return v\n\n    @validator(\"last_task_index\", always=True)\n    def first_last_task_indices(cls, v, values):\n        \"\"\"\n        Check that `last_task_index` is non-negative, and that it is not\n        smaller than `first_task_index`.\n        \"\"\"\n        if v is not None and v &lt; 0:\n            raise ValueError(\n                f\"last_task_index cannot be negative (given: {v})\"\n            )\n\n        first_task_index = values.get(\"first_task_index\")\n        last_task_index = v\n        if first_task_index is not None and last_task_index is not None:\n            if first_task_index &gt; last_task_index:\n                raise ValueError(\n                    f\"{first_task_index=} cannot be larger than \"\n                    f\"{last_task_index=}\"\n                )\n        return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/job/#fractal_server.app.schemas.v2.job.JobCreateV2.first_last_task_indices","title":"<code>first_last_task_indices(v, values)</code>","text":"<p>Check that <code>last_task_index</code> is non-negative, and that it is not smaller than <code>first_task_index</code>.</p> Source code in <code>fractal_server/app/schemas/v2/job.py</code> <pre><code>@validator(\"last_task_index\", always=True)\ndef first_last_task_indices(cls, v, values):\n    \"\"\"\n    Check that `last_task_index` is non-negative, and that it is not\n    smaller than `first_task_index`.\n    \"\"\"\n    if v is not None and v &lt; 0:\n        raise ValueError(\n            f\"last_task_index cannot be negative (given: {v})\"\n        )\n\n    first_task_index = values.get(\"first_task_index\")\n    last_task_index = v\n    if first_task_index is not None and last_task_index is not None:\n        if first_task_index &gt; last_task_index:\n            raise ValueError(\n                f\"{first_task_index=} cannot be larger than \"\n                f\"{last_task_index=}\"\n            )\n    return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/job/#fractal_server.app.schemas.v2.job.JobCreateV2.first_task_index_non_negative","title":"<code>first_task_index_non_negative(v, values)</code>","text":"<p>Check that <code>first_task_index</code> is non-negative.</p> Source code in <code>fractal_server/app/schemas/v2/job.py</code> <pre><code>@validator(\"first_task_index\", always=True)\ndef first_task_index_non_negative(cls, v, values):\n    \"\"\"\n    Check that `first_task_index` is non-negative.\n    \"\"\"\n    if v is not None and v &lt; 0:\n        raise ValueError(\n            f\"first_task_index cannot be negative (given: {v})\"\n        )\n    return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/job/#fractal_server.app.schemas.v2.job.JobStatusTypeV2","title":"<code>JobStatusTypeV2</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Define the available job statuses</p> <p>Attributes:</p> Name Type Description <code>SUBMITTED</code> <p>The job was created. This does not guarantee that it was also submitted to an executor (e.g. other errors could have prevented this), nor that it is actually running (e.g. SLURM jobs could be still in the queue).</p> <code>DONE</code> <p>The job successfully reached its end.</p> <code>FAILED</code> <p>The workflow terminated with an error.</p> Source code in <code>fractal_server/app/schemas/v2/job.py</code> <pre><code>class JobStatusTypeV2(str, Enum):\n    \"\"\"\n    Define the available job statuses\n\n    Attributes:\n        SUBMITTED:\n            The job was created. This does not guarantee that it was also\n            submitted to an executor (e.g. other errors could have prevented\n            this), nor that it is actually running (e.g. SLURM jobs could be\n            still in the queue).\n        DONE:\n            The job successfully reached its end.\n        FAILED:\n            The workflow terminated with an error.\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/manifest/","title":"manifest","text":""},{"location":"reference/fractal_server/app/schemas/v2/manifest/#fractal_server.app.schemas.v2.manifest.ManifestV2","title":"<code>ManifestV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Packages containing tasks are required to include a special file <code>__FRACTAL_MANIFEST__.json</code> in order to be discovered and used by Fractal.</p> <p>This model class and the model classes it depends on provide the base schema to read, write and validate manifests.</p> <p>Attributes:</p> Name Type Description <code>manifest_version</code> <code>str</code> <p>A version string that provides indication for compatibility between manifests as the schema evolves. This is for instance used by Fractal to determine which subclass of the present base class needs be used to read and validate the input.</p> <code>task_list</code> <p>list[TaskManifestType] The list of tasks, represented as specified by subclasses of the _TaskManifestBase (a.k.a. TaskManifestType)</p> <code>has_args_schemas</code> <code>bool</code> <p><code>True</code> if the manifest incldues JSON Schemas for the arguments of each task.</p> <code>args_schema_version</code> <code>Optional[str]</code> <p>Label of how <code>args_schema</code>s were generated (e.g. <code>pydantic_v1</code>).</p> Source code in <code>fractal_server/app/schemas/v2/manifest.py</code> <pre><code>class ManifestV2(BaseModel):\n    \"\"\"\n    Packages containing tasks are required to include a special file\n    `__FRACTAL_MANIFEST__.json` in order to be discovered and used by Fractal.\n\n    This model class and the model classes it depends on provide the base\n    schema to read, write and validate manifests.\n\n    Attributes:\n        manifest_version:\n            A version string that provides indication for compatibility between\n            manifests as the schema evolves. This is for instance used by\n            Fractal to determine which subclass of the present base class needs\n            be used to read and validate the input.\n        task_list : list[TaskManifestType]\n            The list of tasks, represented as specified by subclasses of the\n            _TaskManifestBase (a.k.a. TaskManifestType)\n        has_args_schemas:\n            `True` if the manifest incldues JSON Schemas for the arguments of\n            each task.\n        args_schema_version:\n            Label of how `args_schema`s were generated (e.g. `pydantic_v1`).\n    \"\"\"\n\n    manifest_version: str\n    task_list: list[TaskManifestV2]\n    has_args_schemas: bool = False\n    args_schema_version: Optional[str]\n\n    @root_validator()\n    def _check_args_schemas_are_present(cls, values):\n        has_args_schemas = values[\"has_args_schemas\"]\n        task_list = values[\"task_list\"]\n        if has_args_schemas is True:\n            for task in task_list:\n                if task.executable_parallel is not None:\n                    if task.args_schema_parallel is None:\n                        raise ValueError(\n                            f\"Manifest has {has_args_schemas=}, but \"\n                            f\"task '{task.name}' has \"\n                            f\"{task.args_schema_parallel=}.\"\n                        )\n                if task.executable_non_parallel is not None:\n                    if task.args_schema_non_parallel is None:\n                        raise ValueError(\n                            f\"Manifest has {has_args_schemas=}, but \"\n                            f\"task '{task.name}' has \"\n                            f\"{task.args_schema_non_parallel=}.\"\n                        )\n        return values\n\n    @validator(\"manifest_version\")\n    def manifest_version_2(cls, value):\n        if value != \"2\":\n            raise ValueError(f\"Wrong manifest version (given {value})\")\n        return value\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/manifest/#fractal_server.app.schemas.v2.manifest.TaskManifestV2","title":"<code>TaskManifestV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents a task within a V2 manifest.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The task name</p> <code>executable</code> <code>str</code> <p>Path to the executable relative to the package root</p> <p>Note: by package root we mean \"as it will be installed\". If a package <code>Pkg</code> installs in the folder <code>pkg</code> the executable <code>pkg/executable.py</code>, this attribute must contain only <code>executable.py</code>.</p> <code>input_type</code> <code>str</code> <p>The input type accepted by the task</p> <code>output_type</code> <code>str</code> <p>The output type returned by the task</p> <code>meta</code> <code>str</code> <p>Additional information about the package, such as hash of the executable, specific runtime requirements (e.g., need_gpu=True), etc.</p> <code>args_schema</code> <code>str</code> <p>JSON Schema for task arguments</p> <code>docs_info</code> <code>Optional[str]</code> <p>Additional information about the Task, coming from the docstring.</p> <code>docs_link</code> <code>Optional[HttpUrl]</code> <p>Link to Task docs.</p> Source code in <code>fractal_server/app/schemas/v2/manifest.py</code> <pre><code>class TaskManifestV2(BaseModel):\n    \"\"\"\n    Represents a task within a V2 manifest.\n\n    Attributes:\n        name:\n            The task name\n        executable:\n            Path to the executable relative to the package root\n\n            Note: by package root we mean \"as it will be installed\". If a\n            package `Pkg` installs in the folder `pkg` the executable\n            `pkg/executable.py`, this attribute must contain only\n            `executable.py`.\n        input_type:\n            The input type accepted by the task\n        output_type:\n            The output type returned by the task\n        meta:\n            Additional information about the package, such as hash of the\n            executable, specific runtime requirements (e.g., need_gpu=True),\n            etc.\n        args_schema:\n            JSON Schema for task arguments\n        docs_info:\n            Additional information about the Task, coming from the docstring.\n        docs_link:\n            Link to Task docs.\n    \"\"\"\n\n    name: str\n    executable_non_parallel: Optional[str] = None\n    executable_parallel: Optional[str] = None\n    input_types: dict[str, bool] = Field(default_factory=dict)\n    output_types: dict[str, bool] = Field(default_factory=dict)\n    meta_non_parallel: dict[str, Any] = Field(default_factory=dict)\n    meta_parallel: dict[str, Any] = Field(default_factory=dict)\n    args_schema_non_parallel: Optional[dict[str, Any]] = None\n    args_schema_parallel: Optional[dict[str, Any]] = None\n    docs_info: Optional[str] = None\n    docs_link: Optional[HttpUrl] = None\n\n    @root_validator\n    def validate_executable_args_meta(cls, values):\n\n        executable_non_parallel = values.get(\"executable_non_parallel\")\n        executable_parallel = values.get(\"executable_parallel\")\n        if (executable_non_parallel is None) and (executable_parallel is None):\n\n            raise ValueError(\n                \"`TaskManifestV2.executable_non_parallel` and \"\n                \"`TaskManifestV2.executable_parallel` cannot be both None.\"\n            )\n\n        elif executable_non_parallel is None:\n\n            meta_non_parallel = values.get(\"meta_non_parallel\")\n            if meta_non_parallel != {}:\n                raise ValueError(\n                    \"`TaskManifestV2.meta_non_parallel` must be an empty dict \"\n                    \"if `TaskManifestV2.executable_non_parallel` is None. \"\n                    f\"Given: {meta_non_parallel}.\"\n                )\n\n            args_schema_non_parallel = values.get(\"args_schema_non_parallel\")\n            if args_schema_non_parallel is not None:\n                raise ValueError(\n                    \"`TaskManifestV2.args_schema_non_parallel` must be None \"\n                    \"if `TaskManifestV2.executable_non_parallel` is None. \"\n                    f\"Given: {args_schema_non_parallel}.\"\n                )\n\n        elif executable_parallel is None:\n\n            meta_parallel = values.get(\"meta_parallel\")\n            if meta_parallel != {}:\n                raise ValueError(\n                    \"`TaskManifestV2.meta_parallel` must be an empty dict if \"\n                    \"`TaskManifestV2.executable_parallel` is None. \"\n                    f\"Given: {meta_parallel}.\"\n                )\n\n            args_schema_parallel = values.get(\"args_schema_parallel\")\n            if args_schema_parallel is not None:\n                raise ValueError(\n                    \"`TaskManifestV2.args_schema_parallel` must be None if \"\n                    \"`TaskManifestV2.executable_parallel` is None. \"\n                    f\"Given: {args_schema_parallel}.\"\n                )\n\n        return values\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/project/","title":"project","text":""},{"location":"reference/fractal_server/app/schemas/v2/status/","title":"status","text":""},{"location":"reference/fractal_server/app/schemas/v2/status/#fractal_server.app.schemas.v2.status.StatusReadV2","title":"<code>StatusReadV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response type for the <code>/project/{project_id}/status/</code> endpoint</p> Source code in <code>fractal_server/app/schemas/v2/status.py</code> <pre><code>class StatusReadV2(BaseModel):\n    \"\"\"\n    Response type for the\n    `/project/{project_id}/status/` endpoint\n    \"\"\"\n\n    status: dict[\n        str,\n        WorkflowTaskStatusTypeV2,\n    ] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/task/","title":"task","text":""},{"location":"reference/fractal_server/app/schemas/v2/task_collection/","title":"task_collection","text":""},{"location":"reference/fractal_server/app/schemas/v2/task_collection/#fractal_server.app.schemas.v2.task_collection.TaskCollectPipV2","title":"<code>TaskCollectPipV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>TaskCollectPipV2 class</p> <p>This class only encodes the attributes required to trigger a task-collection operation. Other attributes (that are assigned during task collection) are defined as part of fractal-server.</p> <p>Two cases are supported:</p> <pre><code>1. `package` is the path of a local wheel file;\n2. `package` is the name of a package that can be installed via `pip`.\n</code></pre> <p>Attributes:</p> Name Type Description <code>package</code> <code>str</code> <p>The name of a <code>pip</code>-installable package, or the path to a local wheel file.</p> <code>package_version</code> <code>Optional[str]</code> <p>Version of the package</p> <code>package_extras</code> <code>Optional[str]</code> <p>Package extras to include in the <code>pip install</code> command</p> <code>python_version</code> <code>Optional[str]</code> <p>Python version to install and run the package tasks</p> <code>pinned_package_versions</code> <code>Optional[dict[str, str]]</code> <p>dictionary 'package':'version' used to pin versions for specific packages.</p> Source code in <code>fractal_server/app/schemas/v2/task_collection.py</code> <pre><code>class TaskCollectPipV2(BaseModel):\n    \"\"\"\n    TaskCollectPipV2 class\n\n    This class only encodes the attributes required to trigger a\n    task-collection operation. Other attributes (that are assigned *during*\n    task collection) are defined as part of fractal-server.\n\n    Two cases are supported:\n\n        1. `package` is the path of a local wheel file;\n        2. `package` is the name of a package that can be installed via `pip`.\n\n\n    Attributes:\n        package:\n            The name of a `pip`-installable package, or the path to a local\n            wheel file.\n        package_version: Version of the package\n        package_extras: Package extras to include in the `pip install` command\n        python_version: Python version to install and run the package tasks\n        pinned_package_versions:\n            dictionary 'package':'version' used to pin versions for specific\n            packages.\n\n    \"\"\"\n\n    package: str\n    package_version: Optional[str] = None\n    package_extras: Optional[str] = None\n    python_version: Optional[str] = None\n    pinned_package_versions: Optional[dict[str, str]] = None\n\n    _pinned_package_versions = validator(\n        \"pinned_package_versions\", allow_reuse=True\n    )(valdictkeys(\"pinned_package_versions\"))\n    _package_extras = validator(\"package_extras\", allow_reuse=True)(\n        valstr(\"package_extras\")\n    )\n    _python_version = validator(\"python_version\", allow_reuse=True)(\n        valstr(\"python_version\")\n    )\n\n    @validator(\"package\")\n    def package_validator(cls, value):\n        if \"/\" in value:\n            if not value.endswith(\".whl\"):\n                raise ValueError(\n                    \"Local-package path must be a wheel file \"\n                    f\"(given {value}).\"\n                )\n            if not Path(value).is_absolute():\n                raise ValueError(\n                    f\"Local-package path must be absolute: (given {value}).\"\n                )\n        return value\n\n    @validator(\"package_version\")\n    def package_version_validator(cls, v, values):\n\n        valstr(\"package_version\")(v)\n\n        if values[\"package\"].endswith(\".whl\"):\n            raise ValueError(\n                \"Cannot provide version when package is a Wheel file.\"\n            )\n        return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/task_collection/#fractal_server.app.schemas.v2.task_collection.TaskCollectStatusV2","title":"<code>TaskCollectStatusV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>TaskCollectStatus class</p> <p>Attributes:</p> Name Type Description <code>status</code> <code>Literal['pending', 'installing', 'collecting', 'fail', 'OK']</code> <code>package</code> <code>str</code> <code>venv_path</code> <code>Path</code> <code>task_list</code> <code>Optional[list[TaskReadV2]]</code> <code>log</code> <code>Optional[str]</code> <code>info</code> <code>Optional[str]</code> Source code in <code>fractal_server/app/schemas/v2/task_collection.py</code> <pre><code>class TaskCollectStatusV2(BaseModel):\n    \"\"\"\n    TaskCollectStatus class\n\n    Attributes:\n        status:\n        package:\n        venv_path:\n        task_list:\n        log:\n        info:\n    \"\"\"\n\n    status: Literal[\"pending\", \"installing\", \"collecting\", \"fail\", \"OK\"]\n    package: str\n    venv_path: Path\n    task_list: Optional[list[TaskReadV2]] = Field(default=[])\n    log: Optional[str]\n    info: Optional[str]\n\n    def sanitised_dict(self):\n        \"\"\"\n        Return `self.dict()` after casting `self.venv_path` to a string\n        \"\"\"\n        d = self.dict()\n        d[\"venv_path\"] = str(self.venv_path)\n        return d\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/task_collection/#fractal_server.app.schemas.v2.task_collection.TaskCollectStatusV2.sanitised_dict","title":"<code>sanitised_dict()</code>","text":"<p>Return <code>self.dict()</code> after casting <code>self.venv_path</code> to a string</p> Source code in <code>fractal_server/app/schemas/v2/task_collection.py</code> <pre><code>def sanitised_dict(self):\n    \"\"\"\n    Return `self.dict()` after casting `self.venv_path` to a string\n    \"\"\"\n    d = self.dict()\n    d[\"venv_path\"] = str(self.venv_path)\n    return d\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/workflow/","title":"workflow","text":""},{"location":"reference/fractal_server/app/schemas/v2/workflow/#fractal_server.app.schemas.v2.workflow.WorkflowExportV2","title":"<code>WorkflowExportV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class for <code>Workflow</code> export.</p> <p>Attributes:</p> Name Type Description <code>task_list</code> <code>list[WorkflowTaskExportV2]</code> Source code in <code>fractal_server/app/schemas/v2/workflow.py</code> <pre><code>class WorkflowExportV2(BaseModel):\n    \"\"\"\n    Class for `Workflow` export.\n\n    Attributes:\n        task_list:\n    \"\"\"\n\n    name: str\n    task_list: list[WorkflowTaskExportV2]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/workflow/#fractal_server.app.schemas.v2.workflow.WorkflowImportV2","title":"<code>WorkflowImportV2</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class for <code>Workflow</code> import.</p> <p>Attributes:</p> Name Type Description <code>task_list</code> <code>list[WorkflowTaskImportV2]</code> Source code in <code>fractal_server/app/schemas/v2/workflow.py</code> <pre><code>class WorkflowImportV2(BaseModel):\n    \"\"\"\n    Class for `Workflow` import.\n\n    Attributes:\n        task_list:\n    \"\"\"\n\n    name: str\n    task_list: list[WorkflowTaskImportV2]\n\n    # Validators\n    _name = validator(\"name\", allow_reuse=True)(valstr(\"name\"))\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/workflowtask/","title":"workflowtask","text":""},{"location":"reference/fractal_server/app/schemas/v2/workflowtask/#fractal_server.app.schemas.v2.workflowtask.WorkflowTaskStatusTypeV2","title":"<code>WorkflowTaskStatusTypeV2</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Define the available values for the status of a <code>WorkflowTask</code>.</p> <p>This model is used within the <code>Dataset.history</code> attribute, which is constructed in the runner and then used in the API (e.g. in the <code>api/v2/project/{project_id}/dataset/{dataset_id}/status</code> endpoint).</p> <p>Attributes:</p> Name Type Description <code>SUBMITTED</code> <p>The <code>WorkflowTask</code> is part of a running job.</p> <code>DONE</code> <p>The most-recent execution of this <code>WorkflowTask</code> was successful.</p> <code>FAILED</code> <p>The most-recent execution of this <code>WorkflowTask</code> failed.</p> Source code in <code>fractal_server/app/schemas/v2/workflowtask.py</code> <pre><code>class WorkflowTaskStatusTypeV2(str, Enum):\n    \"\"\"\n    Define the available values for the status of a `WorkflowTask`.\n\n    This model is used within the `Dataset.history` attribute, which is\n    constructed in the runner and then used in the API (e.g. in the\n    `api/v2/project/{project_id}/dataset/{dataset_id}/status` endpoint).\n\n    Attributes:\n        SUBMITTED: The `WorkflowTask` is part of a running job.\n        DONE: The most-recent execution of this `WorkflowTask` was successful.\n        FAILED: The most-recent execution of this `WorkflowTask` failed.\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/fractal_server/app/security/","title":"security","text":"<p>Auth subsystem</p> <p>This module implements the authorisation/authentication subsystem of the Fractal Server. It is based on the FastAPI Users library with support for the SQLModel database adapter.</p> <p>In particular, this module links the appropriate database models, sets up FastAPIUsers with Barer Token and cookie transports and register local routes. Then, for each OAuth client defined in the Fractal Settings configuration, it registers the client and the relative routes.</p> <p>All routes are registerd under the <code>auth/</code> prefix.</p>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync","title":"<code>SQLModelUserDatabaseAsync</code>","text":"<p>             Bases: <code>Generic[UP, ID]</code>, <code>BaseUserDatabase[UP, ID]</code></p> <p>This class is from fastapi_users_db_sqlmodel Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence</p> <p>Database adapter for SQLModel working purely asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>user_model</code> <code>Type[UP]</code> <p>SQLModel model of a DB representation of a user.</p> required <code>session</code> <code>AsyncSession</code> <p>SQLAlchemy async session.</p> required Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>class SQLModelUserDatabaseAsync(Generic[UP, ID], BaseUserDatabase[UP, ID]):\n    \"\"\"\n    This class is from fastapi_users_db_sqlmodel\n    Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence\n\n    Database adapter for SQLModel working purely asynchronously.\n\n    Args:\n        user_model: SQLModel model of a DB representation of a user.\n        session: SQLAlchemy async session.\n    \"\"\"\n\n    session: AsyncSession\n    user_model: Type[UP]\n    oauth_account_model: Optional[Type[OAuthAccount]]\n\n    def __init__(\n        self,\n        session: AsyncSession,\n        user_model: Type[UP],\n        oauth_account_model: Optional[Type[OAuthAccount]] = None,\n    ):\n        self.session = session\n        self.user_model = user_model\n        self.oauth_account_model = oauth_account_model\n\n    async def get(self, id: ID) -&gt; Optional[UP]:\n        \"\"\"Get a single user by id.\"\"\"\n        return await self.session.get(self.user_model, id)\n\n    async def get_by_email(self, email: str) -&gt; Optional[UP]:\n        \"\"\"Get a single user by email.\"\"\"\n        statement = select(self.user_model).where(\n            func.lower(self.user_model.email) == func.lower(email)\n        )\n        results = await self.session.execute(statement)\n        object = results.first()\n        if object is None:\n            return None\n        return object[0]\n\n    async def get_by_oauth_account(\n        self, oauth: str, account_id: str\n    ) -&gt; Optional[UP]:  # noqa\n        \"\"\"Get a single user by OAuth account id.\"\"\"\n        if self.oauth_account_model is None:\n            raise NotImplementedError()\n        statement = (\n            select(self.oauth_account_model)\n            .where(self.oauth_account_model.oauth_name == oauth)\n            .where(self.oauth_account_model.account_id == account_id)\n            .options(selectinload(self.oauth_account_model.user))  # type: ignore  # noqa\n        )\n        results = await self.session.execute(statement)\n        oauth_account = results.first()\n        if oauth_account:\n            user = oauth_account[0].user  # type: ignore\n            return user\n        return None\n\n    async def create(self, create_dict: Dict[str, Any]) -&gt; UP:\n        \"\"\"Create a user.\"\"\"\n        user = self.user_model(**create_dict)\n        self.session.add(user)\n        await self.session.commit()\n        await self.session.refresh(user)\n        return user\n\n    async def update(self, user: UP, update_dict: Dict[str, Any]) -&gt; UP:\n        for key, value in update_dict.items():\n            setattr(user, key, value)\n        self.session.add(user)\n        await self.session.commit()\n        await self.session.refresh(user)\n        return user\n\n    async def delete(self, user: UP) -&gt; None:\n        await self.session.delete(user)\n        await self.session.commit()\n\n    async def add_oauth_account(\n        self, user: UP, create_dict: Dict[str, Any]\n    ) -&gt; UP:  # noqa\n        if self.oauth_account_model is None:\n            raise NotImplementedError()\n\n        oauth_account = self.oauth_account_model(**create_dict)\n        user.oauth_accounts.append(oauth_account)  # type: ignore\n        self.session.add(user)\n\n        await self.session.commit()\n\n        return user\n\n    async def update_oauth_account(\n        self, user: UP, oauth_account: OAP, update_dict: Dict[str, Any]\n    ) -&gt; UP:\n        if self.oauth_account_model is None:\n            raise NotImplementedError()\n\n        for key, value in update_dict.items():\n            setattr(oauth_account, key, value)\n        self.session.add(oauth_account)\n        await self.session.commit()\n\n        return user\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.create","title":"<code>create(create_dict)</code>  <code>async</code>","text":"<p>Create a user.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def create(self, create_dict: Dict[str, Any]) -&gt; UP:\n    \"\"\"Create a user.\"\"\"\n    user = self.user_model(**create_dict)\n    self.session.add(user)\n    await self.session.commit()\n    await self.session.refresh(user)\n    return user\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.get","title":"<code>get(id)</code>  <code>async</code>","text":"<p>Get a single user by id.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def get(self, id: ID) -&gt; Optional[UP]:\n    \"\"\"Get a single user by id.\"\"\"\n    return await self.session.get(self.user_model, id)\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.get_by_email","title":"<code>get_by_email(email)</code>  <code>async</code>","text":"<p>Get a single user by email.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def get_by_email(self, email: str) -&gt; Optional[UP]:\n    \"\"\"Get a single user by email.\"\"\"\n    statement = select(self.user_model).where(\n        func.lower(self.user_model.email) == func.lower(email)\n    )\n    results = await self.session.execute(statement)\n    object = results.first()\n    if object is None:\n        return None\n    return object[0]\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.get_by_oauth_account","title":"<code>get_by_oauth_account(oauth, account_id)</code>  <code>async</code>","text":"<p>Get a single user by OAuth account id.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def get_by_oauth_account(\n    self, oauth: str, account_id: str\n) -&gt; Optional[UP]:  # noqa\n    \"\"\"Get a single user by OAuth account id.\"\"\"\n    if self.oauth_account_model is None:\n        raise NotImplementedError()\n    statement = (\n        select(self.oauth_account_model)\n        .where(self.oauth_account_model.oauth_name == oauth)\n        .where(self.oauth_account_model.account_id == account_id)\n        .options(selectinload(self.oauth_account_model.user))  # type: ignore  # noqa\n    )\n    results = await self.session.execute(statement)\n    oauth_account = results.first()\n    if oauth_account:\n        user = oauth_account[0].user  # type: ignore\n        return user\n    return None\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security._create_first_user","title":"<code>_create_first_user(email, password, is_superuser=False, is_verified=False, username=None)</code>  <code>async</code>","text":"<p>Private method to create the first fractal-server user</p> <p>Create a user with the given default arguments and return a message with the relevant informations. If the user alredy exists, for example after a restart, it returns a message to inform that user already exists.</p> <p>WARNING: This function is only meant to create the first user, and then it catches and ignores <code>IntegrityError</code>s (when multiple workers may be trying to concurrently create the first user). This is not the expected behavior for regular user creation, which must rather happen via the /auth/register endpoint.</p> <p>See fastapi_users docs</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>New user's email</p> required <code>password</code> <code>str</code> <p>New user's password</p> required <code>is_superuser</code> <code>bool</code> <p><code>True</code> if the new user is a superuser</p> <code>False</code> <code>is_verified</code> <code>bool</code> <p><code>True</code> if the new user is verifie</p> <code>False</code> <code>username</code> <code>Optional[str]</code> <code>None</code> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def _create_first_user(\n    email: str,\n    password: str,\n    is_superuser: bool = False,\n    is_verified: bool = False,\n    username: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Private method to create the first fractal-server user\n\n    Create a user with the given default arguments and return a message with\n    the relevant informations. If the user alredy exists, for example after a\n    restart, it returns a message to inform that user already exists.\n\n    **WARNING**: This function is only meant to create the first user, and then\n    it catches and ignores `IntegrityError`s (when multiple workers may be\n    trying to concurrently create the first user). This is not the expected\n    behavior for regular user creation, which must rather happen via the\n    /auth/register endpoint.\n\n    See [fastapi_users docs](https://fastapi-users.github.io/fastapi-users/\n    12.1/cookbook/create-user-programmatically)\n\n    Arguments:\n        email: New user's email\n        password: New user's password\n        is_superuser: `True` if the new user is a superuser\n        is_verified: `True` if the new user is verifie\n        username:\n    \"\"\"\n    try:\n        async with get_async_session_context() as session:\n\n            if is_superuser is True:\n                # If a superuser already exists, exit\n                stm = select(UserOAuth).where(\n                    UserOAuth.is_superuser == True  # noqa: E712\n                )\n                res = await session.execute(stm)\n                existing_superuser = res.scalars().first()\n                if existing_superuser is not None:\n                    logger.info(\n                        f\"{existing_superuser.email} superuser already exists,\"\n                        f\" skip creation of {email}\"\n                    )\n                    return None\n\n            async with get_user_db_context(session) as user_db:\n                async with get_user_manager_context(user_db) as user_manager:\n                    kwargs = dict(\n                        email=email,\n                        password=password,\n                        is_superuser=is_superuser,\n                        is_verified=is_verified,\n                    )\n                    if username is not None:\n                        kwargs[\"username\"] = username\n                    user = await user_manager.create(UserCreate(**kwargs))\n                    logger.info(f\"User {user.email} created\")\n\n    except IntegrityError:\n        logger.warning(\n            f\"Creation of user {email} failed with IntegrityError \"\n            \"(likely due to concurrent attempts from different workers).\"\n        )\n\n    except UserAlreadyExists:\n        logger.warning(f\"User {email} already exists\")\n</code></pre>"},{"location":"reference/fractal_server/images/","title":"images","text":""},{"location":"reference/fractal_server/images/models/","title":"models","text":""},{"location":"reference/fractal_server/images/models/#fractal_server.images.models.SingleImage","title":"<code>SingleImage</code>","text":"<p>             Bases: <code>SingleImageBase</code></p> <p><code>SingleImageBase</code>, with scalar <code>attributes</code> values (<code>None</code> excluded).</p> Source code in <code>fractal_server/images/models.py</code> <pre><code>class SingleImage(SingleImageBase):\n    \"\"\"\n    `SingleImageBase`, with scalar `attributes` values (`None` excluded).\n    \"\"\"\n\n    @validator(\"attributes\")\n    def validate_attributes(\n        cls, v: dict[str, Any]\n    ) -&gt; dict[str, Union[int, float, str, bool]]:\n        for key, value in v.items():\n            if not isinstance(value, (int, float, str, bool)):\n                raise ValueError(\n                    f\"SingleImage.attributes[{key}] must be a scalar \"\n                    f\"(int, float, str or bool). Given {value} ({type(value)})\"\n                )\n        return v\n</code></pre>"},{"location":"reference/fractal_server/images/models/#fractal_server.images.models.SingleImageBase","title":"<code>SingleImageBase</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base for SingleImage and SingleImageTaskOutput.</p> <p>Attributes:</p> Name Type Description <code>zarr_url</code> <code>str</code> <code>origin</code> <code>Optional[str]</code> <code>attributes</code> <code>dict[str, Any]</code> <code>types</code> <code>dict[str, bool]</code> Source code in <code>fractal_server/images/models.py</code> <pre><code>class SingleImageBase(BaseModel):\n    \"\"\"\n    Base for SingleImage and SingleImageTaskOutput.\n\n    Attributes:\n        zarr_url:\n        origin:\n        attributes:\n        types:\n    \"\"\"\n\n    zarr_url: str\n    origin: Optional[str] = None\n\n    attributes: dict[str, Any] = Field(default_factory=dict)\n    types: dict[str, bool] = Field(default_factory=dict)\n\n    # Validators\n    _attributes = validator(\"attributes\", allow_reuse=True)(\n        valdictkeys(\"attributes\")\n    )\n    _types = validator(\"types\", allow_reuse=True)(valdictkeys(\"types\"))\n\n    @validator(\"zarr_url\")\n    def normalize_zarr_url(cls, v: str) -&gt; str:\n        return normalize_url(v)\n\n    @validator(\"origin\")\n    def normalize_orig(cls, v: Optional[str]) -&gt; Optional[str]:\n        if v is not None:\n            return normalize_url(v)\n</code></pre>"},{"location":"reference/fractal_server/images/models/#fractal_server.images.models.SingleImageTaskOutput","title":"<code>SingleImageTaskOutput</code>","text":"<p>             Bases: <code>SingleImageBase</code></p> <p><code>SingleImageBase</code>, with scalar <code>attributes</code> values (<code>None</code> included).</p> Source code in <code>fractal_server/images/models.py</code> <pre><code>class SingleImageTaskOutput(SingleImageBase):\n    \"\"\"\n    `SingleImageBase`, with scalar `attributes` values (`None` included).\n    \"\"\"\n\n    @validator(\"attributes\")\n    def validate_attributes(\n        cls, v: dict[str, Any]\n    ) -&gt; dict[str, Union[int, float, str, bool, None]]:\n        for key, value in v.items():\n            if not isinstance(value, (int, float, str, bool, type(None))):\n                raise ValueError(\n                    f\"SingleImageTaskOutput.attributes[{key}] must be a \"\n                    \"scalar (int, float, str or bool). \"\n                    f\"Given {value} ({type(value)})\"\n                )\n        return v\n</code></pre>"},{"location":"reference/fractal_server/images/tools/","title":"tools","text":""},{"location":"reference/fractal_server/images/tools/#fractal_server.images.tools.filter_image_list","title":"<code>filter_image_list(images, filters)</code>","text":"<p>Compute a sublist with images that match a filter set.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[dict[str, Any]]</code> <p>A list of images.</p> required <code>filters</code> <code>Filters</code> <p>A set of filters.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of the <code>images</code> elements which match the filter set.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def filter_image_list(\n    images: list[dict[str, Any]],\n    filters: Filters,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Compute a sublist with images that match a filter set.\n\n    Arguments:\n        images: A list of images.\n        filters: A set of filters.\n\n    Returns:\n        List of the `images` elements which match the filter set.\n    \"\"\"\n\n    # When no filter is provided, return all images\n    if filters.attributes == {} and filters.types == {}:\n        return images\n\n    filtered_images = [\n        copy(this_image)\n        for this_image in images\n        if match_filter(this_image, filters=filters)\n    ]\n    return filtered_images\n</code></pre>"},{"location":"reference/fractal_server/images/tools/#fractal_server.images.tools.find_image_by_zarr_url","title":"<code>find_image_by_zarr_url(*, images, zarr_url)</code>","text":"<p>Return a copy of the image with a given zarr_url, and its positional index.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[dict[str, Any]]</code> <p>List of images.</p> required <code>zarr_url</code> <code>str</code> <p>Path that the returned image must have.</p> required <p>Returns:</p> Type Description <code>Optional[ImageSearch]</code> <p>The first image from <code>images</code> which has zarr_url equal to <code>zarr_url</code>.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def find_image_by_zarr_url(\n    *,\n    images: list[dict[str, Any]],\n    zarr_url: str,\n) -&gt; Optional[ImageSearch]:\n    \"\"\"\n    Return a copy of the image with a given zarr_url, and its positional index.\n\n    Arguments:\n        images: List of images.\n        zarr_url: Path that the returned image must have.\n\n    Returns:\n        The first image from `images` which has zarr_url equal to `zarr_url`.\n    \"\"\"\n    image_urls = [img[\"zarr_url\"] for img in images]\n    try:\n        ind = image_urls.index(zarr_url)\n    except ValueError:\n        return None\n    return dict(image=copy(images[ind]), index=ind)\n</code></pre>"},{"location":"reference/fractal_server/images/tools/#fractal_server.images.tools.match_filter","title":"<code>match_filter(image, filters)</code>","text":"<p>Find whether an image matches a filter set.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>dict[str, Any]</code> <p>A single image.</p> required <code>filters</code> <code>Filters</code> <p>A set of filters.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the image matches the filter set.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def match_filter(image: dict[str, Any], filters: Filters) -&gt; bool:\n    \"\"\"\n    Find whether an image matches a filter set.\n\n    Arguments:\n        image: A single image.\n        filters: A set of filters.\n\n    Returns:\n        Whether the image matches the filter set.\n    \"\"\"\n    # Verify match with types (using a False default)\n    for key, value in filters.types.items():\n        if image[\"types\"].get(key, False) != value:\n            return False\n    # Verify match with attributes (only for non-None filters)\n    for key, value in filters.attributes.items():\n        if value is None:\n            continue\n        if image[\"attributes\"].get(key) != value:\n            return False\n    return True\n</code></pre>"},{"location":"reference/fractal_server/tasks/","title":"tasks","text":"<p><code>tasks</code> module</p>"},{"location":"reference/fractal_server/tasks/endpoint_operations/","title":"endpoint_operations","text":""},{"location":"reference/fractal_server/tasks/endpoint_operations/#fractal_server.tasks.endpoint_operations.create_package_dir_pip","title":"<code>create_package_dir_pip(*, task_pkg, create=True)</code>","text":"<p>Create venv folder for a task package and return corresponding Path object</p> Source code in <code>fractal_server/tasks/endpoint_operations.py</code> <pre><code>def create_package_dir_pip(\n    *,\n    task_pkg: Union[_TaskCollectPipV1, _TaskCollectPipV2],\n    create: bool = True,\n) -&gt; Path:\n    \"\"\"\n    Create venv folder for a task package and return corresponding Path object\n    \"\"\"\n    settings = Inject(get_settings)\n    user = FRACTAL_PUBLIC_TASK_SUBDIR\n    if task_pkg.package_version is None:\n        raise ValueError(\n            f\"Cannot create venv folder for package `{task_pkg.package}` \"\n            \"with `version=None`.\"\n        )\n    normalized_package = _normalize_package_name(task_pkg.package)\n    package_dir = f\"{normalized_package}{task_pkg.package_version}\"\n    venv_path = settings.FRACTAL_TASKS_DIR / user / package_dir\n    if create:\n        venv_path.mkdir(exist_ok=False, parents=True)\n    return venv_path\n</code></pre>"},{"location":"reference/fractal_server/tasks/endpoint_operations/#fractal_server.tasks.endpoint_operations.download_package","title":"<code>download_package(*, task_pkg, dest)</code>  <code>async</code>","text":"<p>Download package to destination</p> Source code in <code>fractal_server/tasks/endpoint_operations.py</code> <pre><code>async def download_package(\n    *,\n    task_pkg: Union[_TaskCollectPipV1, _TaskCollectPipV2],\n    dest: Union[str, Path],\n) -&gt; Path:\n    \"\"\"\n    Download package to destination\n    \"\"\"\n    interpreter = get_python_interpreter(version=task_pkg.python_version)\n    pip = f\"{interpreter} -m pip\"\n    version = (\n        f\"=={task_pkg.package_version}\" if task_pkg.package_version else \"\"\n    )\n    package_and_version = f\"{task_pkg.package}{version}\"\n    cmd = f\"{pip} download --no-deps {package_and_version} -d {dest}\"\n    stdout = await execute_command(command=cmd, cwd=Path(\".\"))\n    pkg_file = next(\n        line.split()[-1] for line in stdout.split(\"\\n\") if \"Saved\" in line\n    )\n    return Path(pkg_file)\n</code></pre>"},{"location":"reference/fractal_server/tasks/endpoint_operations/#fractal_server.tasks.endpoint_operations.inspect_package","title":"<code>inspect_package(path, logger_name=None)</code>","text":"<p>Inspect task package to extract version, name and manifest</p> <p>Note that this only works with wheel files, which have a well-defined dist-info section. If we need to generalize to to tar.gz archives, we would need to go and look for <code>PKG-INFO</code>.</p> <p>Note: package name is normalized via <code>_normalize_package_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path the path in which the package is saved</p> required <p>Returns:</p> Name Type Description <code>version_manifest</code> <code>dict</code> <p>A dictionary containing <code>version</code>, the version of the</p> <code>dict</code> <p>pacakge, and <code>manifest</code>, the Fractal manifest object relative to the</p> <code>dict</code> <p>tasks.</p> Source code in <code>fractal_server/tasks/endpoint_operations.py</code> <pre><code>def inspect_package(path: Path, logger_name: Optional[str] = None) -&gt; dict:\n    \"\"\"\n    Inspect task package to extract version, name and manifest\n\n    Note that this only works with wheel files, which have a well-defined\n    dist-info section. If we need to generalize to to tar.gz archives, we would\n    need to go and look for `PKG-INFO`.\n\n    Note: package name is normalized via `_normalize_package_name`.\n\n    Args:\n        path: Path\n            the path in which the package is saved\n\n    Returns:\n        version_manifest: A dictionary containing `version`, the version of the\n        pacakge, and `manifest`, the Fractal manifest object relative to the\n        tasks.\n    \"\"\"\n\n    logger = get_logger(logger_name)\n\n    if not path.as_posix().endswith(\".whl\"):\n        raise ValueError(\n            f\"Only wheel packages are supported, given {path.as_posix()}.\"\n        )\n\n    with ZipFile(path) as wheel:\n        namelist = wheel.namelist()\n\n        # Read and validate task manifest\n        logger.debug(f\"Now reading manifest for {path.as_posix()}\")\n        pkg_manifest = _load_manifest_from_wheel(\n            path, wheel, logger_name=logger_name\n        )\n        logger.debug(\"Manifest read correctly.\")\n\n        # Read package name and version from *.dist-info/METADATA\n        logger.debug(\n            f\"Now reading package name and version for {path.as_posix()}\"\n        )\n        metadata = next(\n            name for name in namelist if \"dist-info/METADATA\" in name\n        )\n        with wheel.open(metadata) as metadata_fd:\n            meta = metadata_fd.read().decode(\"utf-8\")\n            pkg_name = next(\n                line.split()[-1]\n                for line in meta.splitlines()\n                if line.startswith(\"Name\")\n            )\n            pkg_version = next(\n                line.split()[-1]\n                for line in meta.splitlines()\n                if line.startswith(\"Version\")\n            )\n        logger.debug(\"Package name and version read correctly.\")\n\n    # Normalize package name:\n    pkg_name = _normalize_package_name(pkg_name)\n\n    info = dict(\n        pkg_name=pkg_name,\n        pkg_version=pkg_version,\n        pkg_manifest=pkg_manifest,\n    )\n    return info\n</code></pre>"},{"location":"reference/fractal_server/tasks/utils/","title":"utils","text":""},{"location":"reference/fractal_server/tasks/utils/#fractal_server.tasks.utils._init_venv","title":"<code>_init_venv(*, path, python_version=None, logger_name)</code>  <code>async</code>","text":"<p>Set a virtual environment at <code>path/venv</code></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path path to directory in which to set up the virtual environment</p> required <code>python_version</code> <p>default=None Python version the virtual environment will be based upon</p> <code>None</code> <p>Returns:</p> Name Type Description <code>python_bin</code> <code>Path</code> <p>Path path to python interpreter</p> Source code in <code>fractal_server/tasks/utils.py</code> <pre><code>async def _init_venv(\n    *,\n    path: Path,\n    python_version: Optional[str] = None,\n    logger_name: str,\n) -&gt; Path:\n    \"\"\"\n    Set a virtual environment at `path/venv`\n\n    Args:\n        path : Path\n            path to directory in which to set up the virtual environment\n        python_version : default=None\n            Python version the virtual environment will be based upon\n\n    Returns:\n        python_bin : Path\n            path to python interpreter\n    \"\"\"\n    logger = get_logger(logger_name)\n    logger.debug(f\"[_init_venv] {path=}\")\n    interpreter = get_python_interpreter(version=python_version)\n    logger.debug(f\"[_init_venv] {interpreter=}\")\n    await execute_command(\n        cwd=path,\n        command=f\"{interpreter} -m venv venv\",\n        logger_name=logger_name,\n    )\n    python_bin = path / \"venv/bin/python\"\n    logger.debug(f\"[_init_venv] {python_bin=}\")\n    return python_bin\n</code></pre>"},{"location":"reference/fractal_server/tasks/utils/#fractal_server.tasks.utils._normalize_package_name","title":"<code>_normalize_package_name(name)</code>","text":"<p>Implement PyPa specifications for package-name normalization</p> <p>The name should be lowercased with all runs of the characters <code>.</code>, <code>-</code>, or <code>_</code> replaced with a single <code>-</code> character. This can be implemented in Python with the re module. (https://packaging.python.org/en/latest/specifications/name-normalization)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The non-normalized package name.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized package name.</p> Source code in <code>fractal_server/tasks/utils.py</code> <pre><code>def _normalize_package_name(name: str) -&gt; str:\n    \"\"\"\n    Implement PyPa specifications for package-name normalization\n\n    The name should be lowercased with all runs of the characters `.`, `-`,\n    or `_` replaced with a single `-` character. This can be implemented in\n    Python with the re module.\n    (https://packaging.python.org/en/latest/specifications/name-normalization)\n\n    Args:\n        name: The non-normalized package name.\n\n    Returns:\n        The normalized package name.\n    \"\"\"\n    return re.sub(r\"[-_.]+\", \"-\", name).lower()\n</code></pre>"},{"location":"reference/fractal_server/tasks/utils/#fractal_server.tasks.utils.get_absolute_venv_path","title":"<code>get_absolute_venv_path(venv_path)</code>","text":"<p>If a path is not absolute, make it a relative path of FRACTAL_TASKS_DIR.</p> Source code in <code>fractal_server/tasks/utils.py</code> <pre><code>def get_absolute_venv_path(venv_path: Path) -&gt; Path:\n    \"\"\"\n    If a path is not absolute, make it a relative path of FRACTAL_TASKS_DIR.\n    \"\"\"\n    if venv_path.is_absolute():\n        package_path = venv_path\n    else:\n        settings = Inject(get_settings)\n        package_path = settings.FRACTAL_TASKS_DIR / venv_path\n    return package_path\n</code></pre>"},{"location":"reference/fractal_server/tasks/utils/#fractal_server.tasks.utils.get_python_interpreter","title":"<code>get_python_interpreter(version=None)</code>","text":"<p>Return the path to the python interpreter</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>Optional[str]</code> <p>Python version</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the python version requested is not available on the         host.</p> <p>Returns:</p> Name Type Description <code>interpreter</code> <code>str</code> <p>string representing the python executable or its path</p> Source code in <code>fractal_server/tasks/utils.py</code> <pre><code>def get_python_interpreter(version: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Return the path to the python interpreter\n\n    Args:\n        version: Python version\n\n    Raises:\n        ValueError: If the python version requested is not available on the\n                    host.\n\n    Returns:\n        interpreter: string representing the python executable or its path\n    \"\"\"\n    if version:\n        interpreter = shutil.which(f\"python{version}\")\n        if not interpreter:\n            raise ValueError(\n                f\"Python version {version} not available on host.\"\n            )\n    else:\n        interpreter = sys.executable\n\n    return interpreter\n</code></pre>"},{"location":"reference/fractal_server/tasks/v1/","title":"v1","text":""},{"location":"reference/fractal_server/tasks/v1/_TaskCollectPip/","title":"_TaskCollectPip","text":""},{"location":"reference/fractal_server/tasks/v1/_TaskCollectPip/#fractal_server.tasks.v1._TaskCollectPip._TaskCollectPip","title":"<code>_TaskCollectPip</code>","text":"<p>             Bases: <code>TaskCollectPipV1</code></p> <p>Internal TaskCollectPip schema</p> <p>Differences with its parent class (<code>TaskCollectPip</code>):</p> <pre><code>1. We check if the package corresponds to a path in the filesystem, and\n   whether it exists (via new validator `check_local_package`, new\n   method `is_local_package` and new attribute `package_path`).\n2. We include an additional `package_manifest` attribute.\n3. We expose an additional attribute `package_name`, which is filled\n   during task collection.\n</code></pre> Source code in <code>fractal_server/tasks/v1/_TaskCollectPip.py</code> <pre><code>class _TaskCollectPip(TaskCollectPipV1):\n    \"\"\"\n    Internal TaskCollectPip schema\n\n    Differences with its parent class (`TaskCollectPip`):\n\n        1. We check if the package corresponds to a path in the filesystem, and\n           whether it exists (via new validator `check_local_package`, new\n           method `is_local_package` and new attribute `package_path`).\n        2. We include an additional `package_manifest` attribute.\n        3. We expose an additional attribute `package_name`, which is filled\n           during task collection.\n    \"\"\"\n\n    package_name: Optional[str] = None\n    package_path: Optional[Path] = None\n    package_manifest: Optional[ManifestV1] = None\n\n    @property\n    def is_local_package(self) -&gt; bool:\n        return bool(self.package_path)\n\n    @root_validator(pre=True)\n    def check_local_package(cls, values):\n        \"\"\"\n        Checks if package corresponds to an existing path on the filesystem\n\n        In this case, the user is providing directly a package file, rather\n        than a remote one from PyPI. We set the `package_path` attribute and\n        get the actual package name and version from the package file name.\n        \"\"\"\n        if \"/\" in values[\"package\"]:\n            package_path = Path(values[\"package\"])\n            if not package_path.is_absolute():\n                raise ValueError(\"Package path must be absolute\")\n            if package_path.exists():\n                values[\"package_path\"] = package_path\n                (\n                    values[\"package\"],\n                    values[\"version\"],\n                    *_,\n                ) = package_path.name.split(\"-\")\n            else:\n                raise ValueError(f\"Package {package_path} does not exist.\")\n        return values\n\n    @property\n    def package_source(self) -&gt; str:\n        \"\"\"\n        NOTE: As of PR #1188 in `fractal-server`, the attribute\n        `self.package_name` is normalized; this means e.g. that `_` is\n        replaced by `-`. To guarantee backwards compatibility with\n        `Task.source` attributes created before this change, we still replace\n        `-` with `_` upon generation of the `source` attribute, in this\n        method.\n        \"\"\"\n        if not self.package_name or not self.package_version:\n            raise ValueError(\n                \"Cannot construct `package_source` property with \"\n                f\"{self.package_name=} and {self.package_version=}.\"\n            )\n        if self.is_local_package:\n            collection_type = \"pip_local\"\n        else:\n            collection_type = \"pip_remote\"\n\n        package_extras = self.package_extras or \"\"\n        if self.python_version:\n            python_version = f\"py{self.python_version}\"\n        else:\n            python_version = \"\"  # FIXME: can we allow this?\n\n        source = \":\".join(\n            (\n                collection_type,\n                self.package_name.replace(\"-\", \"_\"),  # see method docstring\n                self.package_version,\n                package_extras,\n                python_version,\n            )\n        )\n        return source\n\n    def check(self):\n        \"\"\"\n        Verify that the package has all attributes that are needed to continue\n        with task collection\n        \"\"\"\n        if not self.package_name:\n            raise ValueError(\"`package_name` attribute is not set\")\n        if not self.package_version:\n            raise ValueError(\"`package_version` attribute is not set\")\n        if not self.package_manifest:\n            raise ValueError(\"`package_manifest` attribute is not set\")\n</code></pre>"},{"location":"reference/fractal_server/tasks/v1/_TaskCollectPip/#fractal_server.tasks.v1._TaskCollectPip._TaskCollectPip.package_source","title":"<code>package_source: str</code>  <code>property</code>","text":"<p>NOTE: As of PR #1188 in <code>fractal-server</code>, the attribute <code>self.package_name</code> is normalized; this means e.g. that <code>_</code> is replaced by <code>-</code>. To guarantee backwards compatibility with <code>Task.source</code> attributes created before this change, we still replace <code>-</code> with <code>_</code> upon generation of the <code>source</code> attribute, in this method.</p>"},{"location":"reference/fractal_server/tasks/v1/_TaskCollectPip/#fractal_server.tasks.v1._TaskCollectPip._TaskCollectPip.check","title":"<code>check()</code>","text":"<p>Verify that the package has all attributes that are needed to continue with task collection</p> Source code in <code>fractal_server/tasks/v1/_TaskCollectPip.py</code> <pre><code>def check(self):\n    \"\"\"\n    Verify that the package has all attributes that are needed to continue\n    with task collection\n    \"\"\"\n    if not self.package_name:\n        raise ValueError(\"`package_name` attribute is not set\")\n    if not self.package_version:\n        raise ValueError(\"`package_version` attribute is not set\")\n    if not self.package_manifest:\n        raise ValueError(\"`package_manifest` attribute is not set\")\n</code></pre>"},{"location":"reference/fractal_server/tasks/v1/_TaskCollectPip/#fractal_server.tasks.v1._TaskCollectPip._TaskCollectPip.check_local_package","title":"<code>check_local_package(values)</code>","text":"<p>Checks if package corresponds to an existing path on the filesystem</p> <p>In this case, the user is providing directly a package file, rather than a remote one from PyPI. We set the <code>package_path</code> attribute and get the actual package name and version from the package file name.</p> Source code in <code>fractal_server/tasks/v1/_TaskCollectPip.py</code> <pre><code>@root_validator(pre=True)\ndef check_local_package(cls, values):\n    \"\"\"\n    Checks if package corresponds to an existing path on the filesystem\n\n    In this case, the user is providing directly a package file, rather\n    than a remote one from PyPI. We set the `package_path` attribute and\n    get the actual package name and version from the package file name.\n    \"\"\"\n    if \"/\" in values[\"package\"]:\n        package_path = Path(values[\"package\"])\n        if not package_path.is_absolute():\n            raise ValueError(\"Package path must be absolute\")\n        if package_path.exists():\n            values[\"package_path\"] = package_path\n            (\n                values[\"package\"],\n                values[\"version\"],\n                *_,\n            ) = package_path.name.split(\"-\")\n        else:\n            raise ValueError(f\"Package {package_path} does not exist.\")\n    return values\n</code></pre>"},{"location":"reference/fractal_server/tasks/v1/background_operations/","title":"background_operations","text":"<p>The main function exported from this module is <code>background_collect_pip</code>, which is used as a background task for the task-collection endpoint.</p>"},{"location":"reference/fractal_server/tasks/v1/background_operations/#fractal_server.tasks.v1.background_operations._create_venv_install_package","title":"<code>_create_venv_install_package(*, task_pkg, path, logger_name)</code>  <code>async</code>","text":"<p>Create venv and install package</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>the directory in which to create the environment</p> required <code>task_pkg</code> <code>_TaskCollectPip</code> <p>object containing the different metadata required to install the package</p> required <p>Returns:</p> Name Type Description <code>python_bin</code> <code>Path</code> <p>path to venv's python interpreter</p> <code>package_root</code> <code>Path</code> <p>the location of the package manifest</p> Source code in <code>fractal_server/tasks/v1/background_operations.py</code> <pre><code>async def _create_venv_install_package(\n    *,\n    task_pkg: _TaskCollectPip,\n    path: Path,\n    logger_name: str,\n) -&gt; tuple[Path, Path]:\n    \"\"\"Create venv and install package\n\n    Args:\n        path: the directory in which to create the environment\n        task_pkg: object containing the different metadata required to install\n            the package\n\n    Returns:\n        python_bin: path to venv's python interpreter\n        package_root: the location of the package manifest\n    \"\"\"\n\n    # Normalize package name\n    task_pkg.package_name = _normalize_package_name(task_pkg.package_name)\n    task_pkg.package = _normalize_package_name(task_pkg.package)\n\n    python_bin = await _init_venv(\n        path=path,\n        python_version=task_pkg.python_version,\n        logger_name=logger_name,\n    )\n    package_root = await _pip_install(\n        venv_path=path, task_pkg=task_pkg, logger_name=logger_name\n    )\n    return python_bin, package_root\n</code></pre>"},{"location":"reference/fractal_server/tasks/v1/background_operations/#fractal_server.tasks.v1.background_operations._insert_tasks","title":"<code>_insert_tasks(task_list, db)</code>  <code>async</code>","text":"<p>Insert tasks into database</p> Source code in <code>fractal_server/tasks/v1/background_operations.py</code> <pre><code>async def _insert_tasks(\n    task_list: list[TaskCreateV1],\n    db: DBSyncSession,\n) -&gt; list[Task]:\n    \"\"\"\n    Insert tasks into database\n    \"\"\"\n    task_db_list = [Task(**t.dict()) for t in task_list]\n    db.add_all(task_db_list)\n    db.commit()\n    for t in task_db_list:\n        db.refresh(t)\n    db.close()\n    return task_db_list\n</code></pre>"},{"location":"reference/fractal_server/tasks/v1/background_operations/#fractal_server.tasks.v1.background_operations._pip_install","title":"<code>_pip_install(venv_path, task_pkg, logger_name)</code>  <code>async</code>","text":"<p>Install package in venv</p> <p>Parameters:</p> Name Type Description Default <code>venv_path</code> <code>Path</code> required <code>task_pkg</code> <code>_TaskCollectPip</code> required <code>logger_name</code> <code>str</code> required <p>Returns:</p> Type Description <code>Path</code> <p>The location of the package.</p> Source code in <code>fractal_server/tasks/v1/background_operations.py</code> <pre><code>async def _pip_install(\n    venv_path: Path,\n    task_pkg: _TaskCollectPip,\n    logger_name: str,\n) -&gt; Path:\n    \"\"\"\n    Install package in venv\n\n    Args:\n        venv_path:\n        task_pkg:\n        logger_name:\n\n    Returns:\n        The location of the package.\n    \"\"\"\n\n    logger = get_logger(logger_name)\n\n    pip = venv_path / \"venv/bin/pip\"\n\n    extras = f\"[{task_pkg.package_extras}]\" if task_pkg.package_extras else \"\"\n\n    if task_pkg.is_local_package:\n        pip_install_str = f\"{task_pkg.package_path.as_posix()}{extras}\"\n    else:\n        version_string = (\n            f\"=={task_pkg.package_version}\" if task_pkg.package_version else \"\"\n        )\n        pip_install_str = f\"{task_pkg.package}{extras}{version_string}\"\n\n    cmd_install = f\"{pip} install {pip_install_str}\"\n    cmd_inspect = f\"{pip} show {task_pkg.package}\"\n\n    await execute_command(\n        cwd=venv_path,\n        command=f\"{pip} install --upgrade pip\",\n        logger_name=logger_name,\n    )\n    await execute_command(\n        cwd=venv_path, command=cmd_install, logger_name=logger_name\n    )\n    if task_pkg.pinned_package_versions:\n        for (\n            pinned_pkg_name,\n            pinned_pkg_version,\n        ) in task_pkg.pinned_package_versions.items():\n\n            logger.debug(\n                \"Specific version required: \"\n                f\"{pinned_pkg_name}=={pinned_pkg_version}\"\n            )\n            logger.debug(\n                \"Preliminary check: verify that \"\n                f\"{pinned_pkg_version} is already installed\"\n            )\n            stdout_inspect = await execute_command(\n                cwd=venv_path,\n                command=f\"{pip} show {pinned_pkg_name}\",\n                logger_name=logger_name,\n            )\n            current_version = next(\n                line.split()[-1]\n                for line in stdout_inspect.split(\"\\n\")\n                if line.startswith(\"Version:\")\n            )\n            if current_version != pinned_pkg_version:\n                logger.debug(\n                    f\"Currently installed version of {pinned_pkg_name} \"\n                    f\"({current_version}) differs from pinned version \"\n                    f\"({pinned_pkg_version}); \"\n                    f\"install version {pinned_pkg_version}.\"\n                )\n                await execute_command(\n                    cwd=venv_path,\n                    command=(\n                        f\"{pip} install \"\n                        f\"{pinned_pkg_name}=={pinned_pkg_version}\"\n                    ),\n                    logger_name=logger_name,\n                )\n            else:\n                logger.debug(\n                    f\"Currently installed version of {pinned_pkg_name} \"\n                    f\"({current_version}) already matches the pinned version.\"\n                )\n\n    # Extract package installation path from `pip show`\n    stdout_inspect = await execute_command(\n        cwd=venv_path, command=cmd_inspect, logger_name=logger_name\n    )\n\n    location = Path(\n        next(\n            line.split()[-1]\n            for line in stdout_inspect.split(\"\\n\")\n            if line.startswith(\"Location:\")\n        )\n    )\n\n    # NOTE\n    # https://packaging.python.org/en/latest/specifications/recording-installed-packages/\n    # This directory is named as {name}-{version}.dist-info, with name and\n    # version fields corresponding to Core metadata specifications. Both\n    # fields must be normalized (see the name normalization specification and\n    # the version normalization specification), and replace dash (-)\n    # characters with underscore (_) characters, so the .dist-info directory\n    # always has exactly one dash (-) character in its stem, separating the\n    # name and version fields.\n    package_root = location / (task_pkg.package.replace(\"-\", \"_\"))\n    logger.debug(f\"[_pip install] {location=}\")\n    logger.debug(f\"[_pip install] {task_pkg.package=}\")\n    logger.debug(f\"[_pip install] {package_root=}\")\n    if not package_root.exists():\n        raise RuntimeError(\n            \"Could not determine package installation location.\"\n        )\n    return package_root\n</code></pre>"},{"location":"reference/fractal_server/tasks/v1/background_operations/#fractal_server.tasks.v1.background_operations.background_collect_pip","title":"<code>background_collect_pip(state_id, venv_path, task_pkg)</code>  <code>async</code>","text":"<p>Install package and collect tasks</p> <p>Install a python package and collect the tasks it provides according to the manifest.</p> <p>In case of error, copy the log into the state and delete the package directory.</p> Source code in <code>fractal_server/tasks/v1/background_operations.py</code> <pre><code>async def background_collect_pip(\n    state_id: int,\n    venv_path: Path,\n    task_pkg: _TaskCollectPip,\n) -&gt; None:\n    \"\"\"\n    Install package and collect tasks\n\n    Install a python package and collect the tasks it provides according to\n    the manifest.\n\n    In case of error, copy the log into the state and delete the package\n    directory.\n    \"\"\"\n    logger_name = task_pkg.package.replace(\"/\", \"_\")\n    logger = set_logger(\n        logger_name=logger_name,\n        log_file_path=get_log_path(venv_path),\n    )\n    logger.debug(\"Start background task collection\")\n    for key, value in task_pkg.dict(exclude={\"package_manifest\"}).items():\n        logger.debug(f\"{key}: {value}\")\n\n    with next(get_sync_db()) as db:\n        state: State = db.get(State, state_id)\n        data = TaskCollectStatusV1(**state.data)\n        data.info = None\n\n        try:\n            # install\n            logger.debug(\"Task-collection status: installing\")\n            data.status = \"installing\"\n\n            state.data = data.sanitised_dict()\n            db.merge(state)\n            db.commit()\n            task_list = await create_package_environment_pip(\n                venv_path=venv_path,\n                task_pkg=task_pkg,\n                logger_name=logger_name,\n            )\n\n            # collect\n            logger.debug(\"Task-collection status: collecting\")\n            data.status = \"collecting\"\n            state.data = data.sanitised_dict()\n            db.merge(state)\n            db.commit()\n            tasks = await _insert_tasks(task_list=task_list, db=db)\n\n            # finalise\n            logger.debug(\"Task-collection status: finalising\")\n            collection_path = get_collection_path(venv_path)\n            data.task_list = [\n                TaskReadV1(**task.model_dump()) for task in tasks\n            ]\n            with collection_path.open(\"w\") as f:\n                json.dump(data.sanitised_dict(), f, indent=2)\n\n            # Update DB\n            data.status = \"OK\"\n            data.log = get_collection_log(venv_path)\n            state.data = data.sanitised_dict()\n            db.add(state)\n            db.merge(state)\n            db.commit()\n\n            # Write last logs to file\n            logger.debug(\"Task-collection status: OK\")\n            logger.info(\"Background task collection completed successfully\")\n            close_logger(logger)\n            db.close()\n\n        except Exception as e:\n            # Write last logs to file\n            logger.debug(\"Task-collection status: fail\")\n            logger.info(f\"Background collection failed. Original error: {e}\")\n            close_logger(logger)\n\n            # Update db\n            data.status = \"fail\"\n            data.info = f\"Original error: {e}\"\n            data.log = get_collection_log(venv_path)\n            state.data = data.sanitised_dict()\n            db.merge(state)\n            db.commit()\n            db.close()\n\n            # Delete corrupted package dir\n            shell_rmtree(venv_path)\n</code></pre>"},{"location":"reference/fractal_server/tasks/v1/background_operations/#fractal_server.tasks.v1.background_operations.create_package_environment_pip","title":"<code>create_package_environment_pip(*, task_pkg, venv_path, logger_name)</code>  <code>async</code>","text":"<p>Create environment, install package, and prepare task list</p> Source code in <code>fractal_server/tasks/v1/background_operations.py</code> <pre><code>async def create_package_environment_pip(\n    *,\n    task_pkg: _TaskCollectPip,\n    venv_path: Path,\n    logger_name: str,\n) -&gt; list[TaskCreateV1]:\n    \"\"\"\n    Create environment, install package, and prepare task list\n    \"\"\"\n\n    logger = get_logger(logger_name)\n\n    # Normalize package name\n    task_pkg.package_name = _normalize_package_name(task_pkg.package_name)\n    task_pkg.package = _normalize_package_name(task_pkg.package)\n\n    # Only proceed if package, version and manifest attributes are set\n    task_pkg.check()\n\n    try:\n\n        logger.debug(\"Creating venv and installing package\")\n        python_bin, package_root = await _create_venv_install_package(\n            path=venv_path,\n            task_pkg=task_pkg,\n            logger_name=logger_name,\n        )\n        logger.debug(\"Venv creation and package installation ended correctly.\")\n\n        # Prepare task_list with appropriate metadata\n        logger.debug(\"Creating task list from manifest\")\n        task_list = []\n        for t in task_pkg.package_manifest.task_list:\n            # Fill in attributes for TaskCreate\n            task_executable = package_root / t.executable\n            cmd = f\"{python_bin.as_posix()} {task_executable.as_posix()}\"\n            task_name_slug = slugify_task_name(t.name)\n            task_source = f\"{task_pkg.package_source}:{task_name_slug}\"\n            if not task_executable.exists():\n                raise FileNotFoundError(\n                    f\"Cannot find executable `{task_executable}` \"\n                    f\"for task `{t.name}`\"\n                )\n            manifest = task_pkg.package_manifest\n            if manifest.has_args_schemas:\n                additional_attrs = dict(\n                    args_schema_version=manifest.args_schema_version\n                )\n            else:\n                additional_attrs = {}\n            this_task = TaskCreateV1(\n                **t.dict(),\n                command=cmd,\n                version=task_pkg.package_version,\n                **additional_attrs,\n                source=task_source,\n            )\n            task_list.append(this_task)\n        logger.debug(\"Task list created correctly\")\n    except Exception as e:\n        logger.error(\"Task manifest loading failed\")\n        raise e\n    return task_list\n</code></pre>"},{"location":"reference/fractal_server/tasks/v1/get_collection_data/","title":"get_collection_data","text":""},{"location":"reference/fractal_server/tasks/v2/","title":"v2","text":""},{"location":"reference/fractal_server/tasks/v2/_TaskCollectPip/","title":"_TaskCollectPip","text":""},{"location":"reference/fractal_server/tasks/v2/_TaskCollectPip/#fractal_server.tasks.v2._TaskCollectPip._TaskCollectPip","title":"<code>_TaskCollectPip</code>","text":"<p>             Bases: <code>TaskCollectPipV2</code></p> <p>Internal TaskCollectPip schema</p> <p>Differences with its parent class (<code>TaskCollectPip</code>):</p> <pre><code>1. We check if the package corresponds to a path in the filesystem, and\n   whether it exists (via new validator `check_local_package`, new\n   method `is_local_package` and new attribute `package_path`).\n2. We include an additional `package_manifest` attribute.\n3. We expose an additional attribute `package_name`, which is filled\n   during task collection.\n</code></pre> Source code in <code>fractal_server/tasks/v2/_TaskCollectPip.py</code> <pre><code>class _TaskCollectPip(TaskCollectPipV2):\n    \"\"\"\n    Internal TaskCollectPip schema\n\n    Differences with its parent class (`TaskCollectPip`):\n\n        1. We check if the package corresponds to a path in the filesystem, and\n           whether it exists (via new validator `check_local_package`, new\n           method `is_local_package` and new attribute `package_path`).\n        2. We include an additional `package_manifest` attribute.\n        3. We expose an additional attribute `package_name`, which is filled\n           during task collection.\n    \"\"\"\n\n    package_name: Optional[str] = None\n    package_path: Optional[Path] = None\n    package_manifest: Optional[ManifestV2] = None\n\n    @property\n    def is_local_package(self) -&gt; bool:\n        return bool(self.package_path)\n\n    @root_validator(pre=True)\n    def check_local_package(cls, values):\n        \"\"\"\n        Checks if package corresponds to an existing path on the filesystem\n\n        In this case, the user is providing directly a package file, rather\n        than a remote one from PyPI. We set the `package_path` attribute and\n        get the actual package name and version from the package file name.\n        \"\"\"\n        if \"/\" in values[\"package\"]:\n            package_path = Path(values[\"package\"])\n            if not package_path.is_absolute():\n                raise ValueError(\"Package path must be absolute\")\n            if package_path.exists():\n                values[\"package_path\"] = package_path\n                (\n                    values[\"package\"],\n                    values[\"version\"],\n                    *_,\n                ) = package_path.name.split(\"-\")\n            else:\n                raise ValueError(f\"Package {package_path} does not exist.\")\n        return values\n\n    @property\n    def package_source(self) -&gt; str:\n        \"\"\"\n        NOTE: As of PR #1188 in `fractal-server`, the attribute\n        `self.package_name` is normalized; this means e.g. that `_` is\n        replaced by `-`. To guarantee backwards compatibility with\n        `Task.source` attributes created before this change, we still replace\n        `-` with `_` upon generation of the `source` attribute, in this\n        method.\n        \"\"\"\n        if not self.package_name or not self.package_version:\n            raise ValueError(\n                \"Cannot construct `package_source` property with \"\n                f\"{self.package_name=} and {self.package_version=}.\"\n            )\n        if self.is_local_package:\n            collection_type = \"pip_local\"\n        else:\n            collection_type = \"pip_remote\"\n\n        package_extras = self.package_extras or \"\"\n        if self.python_version:\n            python_version = f\"py{self.python_version}\"\n        else:\n            python_version = \"\"  # FIXME: can we allow this?\n\n        source = \":\".join(\n            (\n                collection_type,\n                self.package_name.replace(\"-\", \"_\"),  # see method docstring\n                self.package_version,\n                package_extras,\n                python_version,\n            )\n        )\n        return source\n\n    def check(self):\n        \"\"\"\n        Verify that the package has all attributes that are needed to continue\n        with task collection\n        \"\"\"\n        if not self.package_name:\n            raise ValueError(\"`package_name` attribute is not set\")\n        if not self.package_version:\n            raise ValueError(\"`package_version` attribute is not set\")\n        if not self.package_manifest:\n            raise ValueError(\"`package_manifest` attribute is not set\")\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/_TaskCollectPip/#fractal_server.tasks.v2._TaskCollectPip._TaskCollectPip.package_source","title":"<code>package_source: str</code>  <code>property</code>","text":"<p>NOTE: As of PR #1188 in <code>fractal-server</code>, the attribute <code>self.package_name</code> is normalized; this means e.g. that <code>_</code> is replaced by <code>-</code>. To guarantee backwards compatibility with <code>Task.source</code> attributes created before this change, we still replace <code>-</code> with <code>_</code> upon generation of the <code>source</code> attribute, in this method.</p>"},{"location":"reference/fractal_server/tasks/v2/_TaskCollectPip/#fractal_server.tasks.v2._TaskCollectPip._TaskCollectPip.check","title":"<code>check()</code>","text":"<p>Verify that the package has all attributes that are needed to continue with task collection</p> Source code in <code>fractal_server/tasks/v2/_TaskCollectPip.py</code> <pre><code>def check(self):\n    \"\"\"\n    Verify that the package has all attributes that are needed to continue\n    with task collection\n    \"\"\"\n    if not self.package_name:\n        raise ValueError(\"`package_name` attribute is not set\")\n    if not self.package_version:\n        raise ValueError(\"`package_version` attribute is not set\")\n    if not self.package_manifest:\n        raise ValueError(\"`package_manifest` attribute is not set\")\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/_TaskCollectPip/#fractal_server.tasks.v2._TaskCollectPip._TaskCollectPip.check_local_package","title":"<code>check_local_package(values)</code>","text":"<p>Checks if package corresponds to an existing path on the filesystem</p> <p>In this case, the user is providing directly a package file, rather than a remote one from PyPI. We set the <code>package_path</code> attribute and get the actual package name and version from the package file name.</p> Source code in <code>fractal_server/tasks/v2/_TaskCollectPip.py</code> <pre><code>@root_validator(pre=True)\ndef check_local_package(cls, values):\n    \"\"\"\n    Checks if package corresponds to an existing path on the filesystem\n\n    In this case, the user is providing directly a package file, rather\n    than a remote one from PyPI. We set the `package_path` attribute and\n    get the actual package name and version from the package file name.\n    \"\"\"\n    if \"/\" in values[\"package\"]:\n        package_path = Path(values[\"package\"])\n        if not package_path.is_absolute():\n            raise ValueError(\"Package path must be absolute\")\n        if package_path.exists():\n            values[\"package_path\"] = package_path\n            (\n                values[\"package\"],\n                values[\"version\"],\n                *_,\n            ) = package_path.name.split(\"-\")\n        else:\n            raise ValueError(f\"Package {package_path} does not exist.\")\n    return values\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/background_operations/","title":"background_operations","text":"<p>The main function exported from this module is <code>background_collect_pip</code>, which is used as a background task for the task-collection endpoint.</p>"},{"location":"reference/fractal_server/tasks/v2/background_operations/#fractal_server.tasks.v2.background_operations._create_venv_install_package","title":"<code>_create_venv_install_package(*, task_pkg, path, logger_name)</code>  <code>async</code>","text":"<p>Create venv and install package</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>the directory in which to create the environment</p> required <code>task_pkg</code> <code>_TaskCollectPip</code> <p>object containing the different metadata required to install the package</p> required <p>Returns:</p> Name Type Description <code>python_bin</code> <code>Path</code> <p>path to venv's python interpreter</p> <code>package_root</code> <code>Path</code> <p>the location of the package manifest</p> Source code in <code>fractal_server/tasks/v2/background_operations.py</code> <pre><code>async def _create_venv_install_package(\n    *,\n    task_pkg: _TaskCollectPip,\n    path: Path,\n    logger_name: str,\n) -&gt; tuple[Path, Path]:\n    \"\"\"Create venv and install package\n\n    Args:\n        path: the directory in which to create the environment\n        task_pkg: object containing the different metadata required to install\n            the package\n\n    Returns:\n        python_bin: path to venv's python interpreter\n        package_root: the location of the package manifest\n    \"\"\"\n\n    # Normalize package name\n    task_pkg.package_name = _normalize_package_name(task_pkg.package_name)\n    task_pkg.package = _normalize_package_name(task_pkg.package)\n\n    python_bin = await _init_venv(\n        path=path,\n        python_version=task_pkg.python_version,\n        logger_name=logger_name,\n    )\n    package_root = await _pip_install(\n        venv_path=path, task_pkg=task_pkg, logger_name=logger_name\n    )\n    return python_bin, package_root\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/background_operations/#fractal_server.tasks.v2.background_operations._insert_tasks","title":"<code>_insert_tasks(task_list, db)</code>  <code>async</code>","text":"<p>Insert tasks into database</p> Source code in <code>fractal_server/tasks/v2/background_operations.py</code> <pre><code>async def _insert_tasks(\n    task_list: list[TaskCreateV2],\n    db: DBSyncSession,\n) -&gt; list[TaskV2]:\n    \"\"\"\n    Insert tasks into database\n    \"\"\"\n\n    task_db_list = [\n        TaskV2(**t.dict(), type=_get_task_type(t)) for t in task_list\n    ]\n    db.add_all(task_db_list)\n    db.commit()\n    for t in task_db_list:\n        db.refresh(t)\n    db.close()\n    return task_db_list\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/background_operations/#fractal_server.tasks.v2.background_operations._pip_install","title":"<code>_pip_install(venv_path, task_pkg, logger_name)</code>  <code>async</code>","text":"<p>Install package in venv</p> <p>Parameters:</p> Name Type Description Default <code>venv_path</code> <code>Path</code> required <code>task_pkg</code> <code>_TaskCollectPip</code> required <code>logger_name</code> <code>str</code> required <p>Returns:</p> Type Description <code>Path</code> <p>The location of the package.</p> Source code in <code>fractal_server/tasks/v2/background_operations.py</code> <pre><code>async def _pip_install(\n    venv_path: Path,\n    task_pkg: _TaskCollectPip,\n    logger_name: str,\n) -&gt; Path:\n    \"\"\"\n    Install package in venv\n\n    Args:\n        venv_path:\n        task_pkg:\n        logger_name:\n\n    Returns:\n        The location of the package.\n    \"\"\"\n\n    logger = get_logger(logger_name)\n\n    pip = venv_path / \"venv/bin/pip\"\n\n    extras = f\"[{task_pkg.package_extras}]\" if task_pkg.package_extras else \"\"\n\n    if task_pkg.is_local_package:\n        pip_install_str = f\"{task_pkg.package_path.as_posix()}{extras}\"\n    else:\n        version_string = (\n            f\"=={task_pkg.package_version}\" if task_pkg.package_version else \"\"\n        )\n        pip_install_str = f\"{task_pkg.package}{extras}{version_string}\"\n\n    cmd_install = f\"{pip} install {pip_install_str}\"\n    cmd_inspect = f\"{pip} show {task_pkg.package}\"\n\n    await execute_command(\n        cwd=venv_path,\n        command=f\"{pip} install --upgrade pip\",\n        logger_name=logger_name,\n    )\n    await execute_command(\n        cwd=venv_path, command=cmd_install, logger_name=logger_name\n    )\n    if task_pkg.pinned_package_versions:\n        for (\n            pinned_pkg_name,\n            pinned_pkg_version,\n        ) in task_pkg.pinned_package_versions.items():\n\n            logger.debug(\n                \"Specific version required: \"\n                f\"{pinned_pkg_name}=={pinned_pkg_version}\"\n            )\n            logger.debug(\n                \"Preliminary check: verify that \"\n                f\"{pinned_pkg_version} is already installed\"\n            )\n            stdout_inspect = await execute_command(\n                cwd=venv_path,\n                command=f\"{pip} show {pinned_pkg_name}\",\n                logger_name=logger_name,\n            )\n            current_version = next(\n                line.split()[-1]\n                for line in stdout_inspect.split(\"\\n\")\n                if line.startswith(\"Version:\")\n            )\n            if current_version != pinned_pkg_version:\n                logger.debug(\n                    f\"Currently installed version of {pinned_pkg_name} \"\n                    f\"({current_version}) differs from pinned version \"\n                    f\"({pinned_pkg_version}); \"\n                    f\"install version {pinned_pkg_version}.\"\n                )\n                await execute_command(\n                    cwd=venv_path,\n                    command=(\n                        f\"{pip} install \"\n                        f\"{pinned_pkg_name}=={pinned_pkg_version}\"\n                    ),\n                    logger_name=logger_name,\n                )\n            else:\n                logger.debug(\n                    f\"Currently installed version of {pinned_pkg_name} \"\n                    f\"({current_version}) already matches the pinned version.\"\n                )\n\n    # Extract package installation path from `pip show`\n    stdout_inspect = await execute_command(\n        cwd=venv_path, command=cmd_inspect, logger_name=logger_name\n    )\n\n    location = Path(\n        next(\n            line.split()[-1]\n            for line in stdout_inspect.split(\"\\n\")\n            if line.startswith(\"Location:\")\n        )\n    )\n\n    # NOTE\n    # https://packaging.python.org/en/latest/specifications/recording-installed-packages/\n    # This directory is named as {name}-{version}.dist-info, with name and\n    # version fields corresponding to Core metadata specifications. Both\n    # fields must be normalized (see the name normalization specification and\n    # the version normalization specification), and replace dash (-)\n    # characters with underscore (_) characters, so the .dist-info directory\n    # always has exactly one dash (-) character in its stem, separating the\n    # name and version fields.\n    package_root = location / (task_pkg.package.replace(\"-\", \"_\"))\n    logger.debug(f\"[_pip install] {location=}\")\n    logger.debug(f\"[_pip install] {task_pkg.package=}\")\n    logger.debug(f\"[_pip install] {package_root=}\")\n    if not package_root.exists():\n        raise RuntimeError(\n            \"Could not determine package installation location.\"\n        )\n    return package_root\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/background_operations/#fractal_server.tasks.v2.background_operations.background_collect_pip","title":"<code>background_collect_pip(state_id, venv_path, task_pkg)</code>  <code>async</code>","text":"<p>Install package and collect tasks</p> <p>Install a python package and collect the tasks it provides according to the manifest.</p> <p>In case of error, copy the log into the state and delete the package directory.</p> Source code in <code>fractal_server/tasks/v2/background_operations.py</code> <pre><code>async def background_collect_pip(\n    state_id: int,\n    venv_path: Path,\n    task_pkg: _TaskCollectPip,\n) -&gt; None:\n    \"\"\"\n    Install package and collect tasks\n\n    Install a python package and collect the tasks it provides according to\n    the manifest.\n\n    In case of error, copy the log into the state and delete the package\n    directory.\n    \"\"\"\n    logger_name = task_pkg.package.replace(\"/\", \"_\")\n    logger = set_logger(\n        logger_name=logger_name,\n        log_file_path=get_log_path(venv_path),\n    )\n    logger.debug(\"Start background task collection\")\n    for key, value in task_pkg.dict(exclude={\"package_manifest\"}).items():\n        logger.debug(f\"{key}: {value}\")\n\n    with next(get_sync_db()) as db:\n        state: CollectionStateV2 = db.get(CollectionStateV2, state_id)\n        data = TaskCollectStatusV2(**state.data)\n        data.info = None\n\n        try:\n            # install\n            logger.debug(\"Task-collection status: installing\")\n            data.status = \"installing\"\n\n            state.data = data.sanitised_dict()\n            db.merge(state)\n            db.commit()\n            task_list = await create_package_environment_pip(\n                venv_path=venv_path,\n                task_pkg=task_pkg,\n                logger_name=logger_name,\n            )\n\n            # collect\n            logger.debug(\"Task-collection status: collecting\")\n            data.status = \"collecting\"\n            state.data = data.sanitised_dict()\n            db.merge(state)\n            db.commit()\n            tasks = await _insert_tasks(task_list=task_list, db=db)\n\n            # finalise\n            logger.debug(\"Task-collection status: finalising\")\n            collection_path = get_collection_path(venv_path)\n            data.task_list = [\n                TaskReadV2(**task.model_dump()) for task in tasks\n            ]\n            with collection_path.open(\"w\") as f:\n                json.dump(data.sanitised_dict(), f, indent=2)\n\n            # Update DB\n            data.status = \"OK\"\n            data.log = get_collection_log(venv_path)\n            state.data = data.sanitised_dict()\n            db.merge(state)\n            db.commit()\n\n            # Write last logs to file\n            logger.debug(\"Task-collection status: OK\")\n            logger.info(\"Background task collection completed successfully\")\n            close_logger(logger)\n            db.close()\n\n        except Exception as e:\n            # Write last logs to file\n            logger.debug(\"Task-collection status: fail\")\n            logger.info(f\"Background collection failed. Original error: {e}\")\n            close_logger(logger)\n\n            # Update db\n            data.status = \"fail\"\n            data.info = f\"Original error: {e}\"\n            data.log = get_collection_log(venv_path)\n            state.data = data.sanitised_dict()\n            db.merge(state)\n            db.commit()\n            db.close()\n\n            # Delete corrupted package dir\n            logger.info(f\"Now deleting temporary folder {venv_path}\")\n            shell_rmtree(venv_path)\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/background_operations/#fractal_server.tasks.v2.background_operations.create_package_environment_pip","title":"<code>create_package_environment_pip(*, task_pkg, venv_path, logger_name)</code>  <code>async</code>","text":"<p>Create environment, install package, and prepare task list</p> Source code in <code>fractal_server/tasks/v2/background_operations.py</code> <pre><code>async def create_package_environment_pip(\n    *,\n    task_pkg: _TaskCollectPip,\n    venv_path: Path,\n    logger_name: str,\n) -&gt; list[TaskCreateV2]:\n    \"\"\"\n    Create environment, install package, and prepare task list\n    \"\"\"\n\n    logger = get_logger(logger_name)\n\n    # Normalize package name\n    task_pkg.package_name = _normalize_package_name(task_pkg.package_name)\n    task_pkg.package = _normalize_package_name(task_pkg.package)\n\n    # Only proceed if package, version and manifest attributes are set\n    task_pkg.check()\n\n    try:\n\n        logger.debug(\"Creating venv and installing package\")\n        python_bin, package_root = await _create_venv_install_package(\n            path=venv_path,\n            task_pkg=task_pkg,\n            logger_name=logger_name,\n        )\n        logger.debug(\"Venv creation and package installation ended correctly.\")\n\n        # Prepare task_list with appropriate metadata\n        logger.debug(\"Creating task list from manifest\")\n        task_list = []\n        for t in task_pkg.package_manifest.task_list:\n            # Fill in attributes for TaskCreate\n            task_attributes = {}\n            task_attributes[\"version\"] = task_pkg.package_version\n            task_name_slug = slugify_task_name(t.name)\n            task_attributes[\n                \"source\"\n            ] = f\"{task_pkg.package_source}:{task_name_slug}\"\n            # Executables\n            if t.executable_non_parallel is not None:\n                non_parallel_path = package_root / t.executable_non_parallel\n                if not non_parallel_path.exists():\n                    raise FileNotFoundError(\n                        f\"Cannot find executable `{non_parallel_path}` \"\n                        f\"for task `{t.name}`\"\n                    )\n                task_attributes[\n                    \"command_non_parallel\"\n                ] = f\"{python_bin.as_posix()} {non_parallel_path.as_posix()}\"\n            if t.executable_parallel is not None:\n                parallel_path = package_root / t.executable_parallel\n                if not parallel_path.exists():\n                    raise FileNotFoundError(\n                        f\"Cannot find executable `{parallel_path}` \"\n                        f\"for task `{t.name}`\"\n                    )\n                task_attributes[\n                    \"command_parallel\"\n                ] = f\"{python_bin.as_posix()} {parallel_path.as_posix()}\"\n\n            manifest = task_pkg.package_manifest\n            if manifest.has_args_schemas:\n                task_attributes[\n                    \"args_schema_version\"\n                ] = manifest.args_schema_version\n\n            this_task = TaskCreateV2(\n                **t.dict(\n                    exclude={\"executable_non_parallel\", \"executable_parallel\"}\n                ),\n                **task_attributes,\n            )\n            task_list.append(this_task)\n        logger.debug(\"Task list created correctly\")\n    except Exception as e:\n        logger.error(\"Task manifest loading failed\")\n        raise e\n    return task_list\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/get_collection_data/","title":"get_collection_data","text":""}]}